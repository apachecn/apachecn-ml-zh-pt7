    

# 三、客户分析

客户分析是一个过程，在这个过程中，我们使用客户行为数据，通过市场细分和预测分析得出最重要的商业决策。市场细分是根据用户的行为和其他类型的共同特征将用户群划分为子群的过程。这将有助于公司为每个用户群提供定制产品。这种分析的结果将引导公司以有效的方式发展他们的业务。公司也获得了更多的利润。有很多优点。我知道这只是一个关于市场细分的简短讨论，但请耐心听我说一会儿。在接下来的部分，我会给你所有必要的信息。

公司可以使用市场细分和预测模型生成的结果进行直接营销、选址、客户获取和客户关系管理。简而言之，在客户分析的帮助下，公司可以决定最佳和最有效的营销战略以及增长战略。公司可以用有限的营销支出取得巨大的成果。客户分析包括各种方法。您可以在下图中引用这些方法的名称:

![Customer Analytics](img/B08394_03_01.jpg)

图 3.1:客户分析的各种方法

在这一章中，我们不会涵盖上图中给出的所有方法，但我们将涵盖行业中最广泛使用的方法。我们将构建一个客户细分应用。在本章中，我们将讨论以下主题:

*   介绍客户细分:

    *   介绍问题陈述

*   了解数据集
*   构建客户细分的基线方法:

    *   实施基线方法
    *   了解测试矩阵
    *   测试基线方法的结果
    *   基线方法的问题
    *   优化基线方法

*   构建客户细分的修订方法:

    *   实施修订方法
    *   测试修订方法
    *   修订方法的问题
    *   了解如何改进修订方法

*   客户细分的最佳方法:

    *   实施最佳方法

*   测试最佳方法
*   不同领域的客户细分
*   摘要

我们将从客户细分开始。

# 介绍客户细分

在这一部分，我们将详细介绍客户细分。最初，我只是简单介绍了一下客户细分，这样你就能稍微理解这个术语。在这里，我们将更多地了解客户细分，这将在我们构建客户细分分析时进一步帮助我们。

如前所述，客户细分是一个过程，我们将公司的消费者群划分为子群。我们需要通过使用一些特定的特征来产生子群，以便公司用更少的营销支出销售更多的产品。在继续之前，我们需要了解一些基础知识，例如，我所说的客户群是什么意思？我说的细分是什么意思？我们如何生成消费者子群？在对消费者进行细分时，我们考虑的特征是什么？让我们逐一回答这些问题。

基本上，任何公司的消费者基础都由两种类型的消费者组成:

1.  现有消费者
2.  潜在消费者

一般来说，我们需要将我们的消费者群分成不同的小组。这些子群被称为段。我们需要以这样一种方式创建组，即每个客户子组都有一些共同的特征。为了解释如何生成子组，让我举一个例子。

假设一家公司正在销售婴儿用品。然后，它需要提出一个消费者细分(消费者子群)，其中包括想要购买婴儿产品的消费者。我们可以借助一个简单的标准来构建第一个细分(子组)。我们将包括家中有一个婴儿并且在上个月购买了婴儿用品的消费者。现在，该公司推出了一款过于昂贵的婴儿产品。在这种情况下，我们可以进一步将第一个分组划分为月收入和社会经济地位。基于这些新的标准，我们可以生成第二个消费者子群。该公司将针对第二小组的消费者购买昂贵和优质的产品，而对于普通产品，该公司将针对第一小组的消费者。

当我们有不同的细分市场时，我们可以设计定制的营销策略以及适合特定细分市场客户的定制产品。这种细分市场营销将有助于公司以更低的营销费用销售更多的产品。因此，公司将获得更多的利润。这是现在公司使用客户细分分析的主要原因。客户细分用于其他领域，如零售领域、金融领域以及基于客户关系管理(CRM)的产品。我已经提供了在细分过程中可以考虑的基本特征的列表。可以参考下面截图中的他们:

![Introducing customer segmentation](img/B08394_03_02.jpg)

图 3.2:客户细分中使用的基本特征列表

你可能想知道公司是如何根据客户细分分析制定营销策略的。答案是公司正在使用 STP 方法来使营销策略变得稳固。什么是 STP 方法？首先，STP 代表细分-目标-定位。在这种方法中，有三个阶段。我们在每个阶段处理的要点解释如下:

*   **细分**:在这一阶段，我们利用客户的档案特征，并考虑上图中提供的特征，对客户群进行细分。一旦细分确定，我们就进入下一阶段。
*   **瞄准**:在这个阶段，营销团队评估细分市场，并试图了解哪种产品适合哪个特定的细分市场。团队为每个细分市场执行这一练习，最后，团队设计定制产品，吸引一个或多个细分市场的客户。他们还将选择向哪个细分市场提供哪个产品。
*   **定位**:这是 STP 流程的最后一个阶段。在这个阶段，公司研究市场机会以及他们的产品能为顾客提供什么。营销团队应该提出独特的销售主张。在这里，团队还试图了解特定的细分市场如何看待产品、品牌或服务。这是公司确定如何最好地定位其产品的一种方式。公司的营销和产品团队创建一个价值主张，清楚地解释他们的产品如何优于任何其他竞争对手。最后，公司以这样一种方式开始他们代表这一价值主张的活动，即消费者群体会对他们得到的东西感到高兴。

我在下图中总结了前面所有的观点:

![Introducing customer segmentation](img/B08394_03_03.jpg)

图 3.3:STP 方法总结

我们已经介绍了客户细分的大部分基本部分。现在是时候进入问题陈述了。

## 介绍问题陈述

如你所知，客户细分有助于公司留住现有客户并获得新的潜在客户。基于细分，公司可以为特定的客户群定制产品，但迄今为止，我们还不知道如何生成细分。这是我们将在本章中重点讨论的一点。你需要学习如何创造客户细分。我们可以为许多领域建立客户细分，例如电子商务、旅游、金融、电信等等。这里，我们将只关注电子商务领域。

下面是我们将要构建的电子商务客户细分应用的问题陈述、输入和输出的详细说明:

*   **问题陈述**:我们的客户细分应用的目标是为给定的问题提供一个解决方案:

    *   我们能否根据客户的购买模式将他们归类到一个特定的细分市场？
    *   我们能否根据他们的细分预测他们将来会购买哪种商品？

*   **输入**:我们将使用包含 4，000 名客户在 1 年内购买清单的电子商务数据。
*   **输出**:第一个目标是，我们需要将我们的消费群分成适当的客户群。第二个目标是我们需要根据客户的第一次购买来预测今年和明年的购买量。

您可能想知道我们如何使用细分来实现对即将到来的购买的预测。好吧，让我告诉你细分是如何帮助我们的！所以，我们不知道新客户的购买模式，但我们知道客户的概况。我们还知道客户购买了哪种产品。因此，我们可以将该客户归入其中一个细分市场，在这个细分市场中，所有其他客户都购买了类似的商品，并分享了类似的个人资料。

让我给你举个例子。比如说，一个人买了一本哈利波特，他住在英国。顾客的年龄组是从 13 岁到 22 岁。如果我们已经生成了一个满足这些特征的客户群，那么我们将把这个新客户放在那个特定的子群中。我们将得出顾客将来可能购买的物品清单。我们也将提供类似的服务，该小组中的其他客户也有。

我们将用于开发电子商务领域客户细分的方法也可以用于其他领域，但每个领域的数据点(特征)会有所不同。在这一章的后面，我们将讨论什么样的数据点可以用于其他领域，比如旅游、金融等等。我将提供其他领域的数据点列表，帮助您从头开始构建客户细分应用。

现在是时候了解用于构建电子商务领域客户细分的数据集了。

# 了解数据集

在数据科学中，找到一个合适的数据集是一项具有挑战性的任务。有时，您找到一个数据集，但它的格式不合适。我们的问题陈述将决定我们需要什么类型的数据集和数据格式。这类活动是数据争论的一部分。

### 注意

数据争论被定义为将数据从一种数据形式转换和映射到另一种数据形式的过程。通过转换和映射，我们的目的应该是创建一个适当的、有价值的数据集，该数据集可以用于开发分析产品。数据争论也被称为数据管理，是任何数据科学应用的重要组成部分。

一般来说，电子商务数据集是专有数据集，你很少能获得真实用户的交易。幸运的是，*UCI 机器学习库*托管了一个名为*在线零售*的数据集。该数据集包含英国零售商的实际交易。

## 数据集的描述

该在线零售数据集包含 2010 年 12 月 1 日至 2011 年 12 月 9 日之间的实际交易。所有交易均取自注册的无店铺网络零售平台。这些在线零售平台大多位于英国。网上零售平台销售独特的适合所有场合的礼物。这些网络零售平台的消费者很多都是批发商。该数据集中有 532610 条记录。

## 下载数据集

您可以通过以下链接之一下载该数据集:

1.  [http://archive.ics.uci.edu/ml/datasets/online+retail](http://archive.ics.uci.edu/ml/datasets/online+retail)
2.  [https://www . ka ggle . com/fabiendaniel/customer-segmentation/data](https://www.kaggle.com/fabiendaniel/customer-segmentation/data)

## 数据集的属性

这些是这个数据集中的属性。我们将看一下对它们的简短描述:

1.  InvoiceNo:该数据属性表示发票号。它是一个六位数的整数。每笔交易的记录都是唯一分配的。如果发票号以字母“c”开头，则表示取消。
2.  StockCode:这个数据属性表示产品(项目)代码。它是一个五位数的整数。所有项目代码都被唯一地分配给每个不同的产品。
3.  描述:该数据属性包含关于项目的描述。
4.  数量:该数据属性包含每笔交易中每种产品的数量。数据是数字格式的。
5.  InvoiceDate:数据属性包含发票日期和时间。它指示每笔交易生成的日期和时间。
6.  单价:价格表示以英镑为单位的产品价格。
7.  CustomerID:该列包含客户标识号。它是唯一分配给每个客户的五位整数。
8.  国家:该列包含客户的地理信息。它记录了客户的国家名称。

可以参考下面截图给出的数据集样本:

![Attributes of the dataset](img/B08394_03_04.jpg)

图 3.4:来自数据集的样本记录

现在我们将开始构建客户细分应用。

# 建立基线方法

在本节中，我们将开始实施客户细分应用的基本模型。此外，我们将改进这种基线方法。在实现时，我们将涵盖必要的概念、技术方面以及执行该特定步骤的意义。您可以在以下 GitHub 链接找到客户细分应用的代码:[https://github.com/jalajthanaki/Customer_segmentation](https://github.com/jalajthanaki/Customer_segmentation)

与本章相关的代码在单个 iPython 笔记本中给出。你可以使用这个 GitHub 链接来访问笔记本:[https://GitHub . com/jalajthanaki/Customer _ segmentation/blob/master/Cust _ segmentation _ online _ retail . ipynb](https://github.com/jalajthanaki/Customer_segmentation/blob/master/Cust_segmentation_online_retail.ipynb)。

参考 GitHub 上给出的代码，因为它会帮助你更好地理解事情。现在让我们开始实施吧！

## 实施基线方法

为了实施客户细分模型，我们的实施将包括以下步骤:

1.  数据准备
2.  探索性数据分析
3.  生成客户类别
4.  客户分类

先从资料准备开始吧！

### 数据准备

当你试图构建任何分析应用时，这是一个基本的步骤。首先，我们需要确保数据的格式是适当的。如果不是，那么我们需要以这样一种方式准备我们的数据集，以便我们可以轻松地构建我们的应用。在这一步，我们将发现我们是否有一个高质量的数据集。我们还可以发现数据集的一些基本事实。

幸运的是，我们不需要改变我们的电子商务数据集的格式，但我们将以这样一种方式探索数据集，即我们可以发现数据集的质量。如果数据集的格式不合适，那么您需要以这样的方式确定数据集的格式，即可以使用数据集进行任何类型的分析。您可以将数据记录转换为 CSV 格式或 JSON 格式或 XML 格式。此外，我们可以推导出关于数据集的一般事实，例如我们的数据集是否有偏差，数据集是否包含任何空值，客户与`Customer_ID`的映射是否正确，他们的购买是否正确记录在数据集中，等等。

#### 加载数据集

为了加载数据集，我们将使用 pandas `read_csv` API。您可以在下面的屏幕截图中找到代码片段:

![Loading the dataset](img/B08394_03_05.jpg)

图 3.5:加载数据集的代码片段

如您所见，数据集的维度为(541909，8)。这意味着数据集中有 541，909 条记录和八个数据属性。我们已经讨论了这八个数据属性。

现在我们需要执行探索性数据分析(EDA)，这可以帮助我们预处理数据集。

### 探索性数据分析(EDA)

在这个部分，我们需要检查数据集的统计属性，并执行一些预处理步骤:

1.  删除空数据条目
2.  删除重复的数据条目
3.  各种数据属性的 EDA

#### 删除空数据条目

首先，我们需要来检查每个属性的数据类型，并找出哪一列有空值。可以参考下面截图中显示的代码片段:

![Removing null data entries](img/B08394_03_06.jpg)

图 3.6:探索数据集的代码片段

正如您在代码中看到的，我们已经为每个数据属性生成了 null 值的总数。我们还为每个数据属性生成了空值的百分比。我们可以观察到，对于`CustomerID`列，大约有 25%的数据条目为空。这意味着大约 25%的数据集没有可用的`CustomerID`值。这表明有许多条目不属于任何客户。这些是异常数据条目。我们无法将它们映射到现有的 CustomerIDs。因此，我们需要删除它们。您可以在下面的屏幕截图中找到从数据集中删除空数据条目的代码片段:

![Removing null data entries](img/B08394_03_07.jpg)

图 3.7:删除空数据条目

#### 删除重复的数据条目

在这一步之后，我们将检查数据集中是否存在任何重复的数据条目。为了让回答这个问题，我们将使用熊猫`duplicate()`功能。可以参考下面截图中显示的代码片段:

![Removing duplicate data entries](img/B08394_03_08.jpg)

图 3.8:删除重复的数据条目

如您所见，我们发现了 5，225 个重复的数据条目。因此，我们已将其删除。

现在我们来详细分析一下每个数据属性。

#### 各种数据属性的 EDA

EDA for 每个数据属性将帮助我们更深入地了解数据集。稍后，我们将使用这些事实来构建一个准确的客户细分应用。

我们将按以下顺序开始研究数据属性:

1.  国家
2.  客户和产品
3.  产品类别
4.  定义产品类别

##### **国家**

我们需要找出事实，比如我们的数据集中有多少个国家。为了回答这个问题，我们需要执行下面截图中显示的代码:

![Country](img/B08394_03_09.jpg)

图 3.9:生成数据集中出现的县数的代码片段

我们还需要找到我们收到最多订单的国家。我们可以通过使用熊猫`groupby()`和`count()`函数来找到答案。我们按降序对订单数量进行排序。可以参考下面截图中的代码片段:

![Country](img/B08394_03_10.jpg)

图 3.10:生成国家订单数量的代码片段

正如您在前面的代码片段中看到的，大部分订单来自英国的客户。现在我们需要探索客户和产品变量。

##### **客户和产品**

这里，我们有大约 400，000 个数据项。我们需要知道这些数据条目中存在的用户和产品的数量。我们将使用来自`pandas` 库中的`value_counts()`函数。看看下面截图中的代码片段:

![Customer and products](img/B08394_03_11.jpg)

图 3.11:探索客户和产品的代码

正如您在上面的屏幕截图中看到的，该数据集包含购买了 3684 种不同商品的 4372 个用户的记录

我们得出了一些有趣的事实。在给定的数据集中，有 4，372 名客户购买了 3，684 种不同的产品。交易总数为 22190 笔。

我们还应该找出每笔交易购买了多少产品。为此，我们将使用`InvoiceNo` 和`InvoiceDate` 数据属性，并计算每笔交易购买的产品数量。可以参考下面截图中显示的代码片段:

![Customer and products](img/B08394_03_12.jpg)

图 3.12:探索每笔交易的产品数量的代码片段

如前面代码片段中的所示，我们可以观察到以下情况:

*   有一些用户在电子商务平台上只进行了一次购买，买了一件物品。这类用户的一个例子是`customerID 12346`。
*   有些用户经常在每个订单中购买大量商品。这类用户的一个例子就是`customerID 12347`。
*   如果您查看 InvoiceNo 数据属性，那么您可以看到有一个发票的前缀`C`。这个`'C'`表示特定的交易已经被取消。

正如我们所知，在我们的数据集中可能存在几个被取消的订单，我们需要计算与被取消的订单相对应的交易数量。我们已经使用了一个使用 lambda 表达式的简单检查条件。现在我们将计算取消订单的百分比。可以参考下面截图中给出的代码片段:

![Customer and products](img/B08394_03_13.jpg)

图 3.13:生成取消订单百分比的代码片段

让我们列出一些取消的订单条目，以便我们可以找到如何处理它们。看一下下面的截图:

![Customer and products](img/B08394_03_14.jpg)

图 3.14:取消订单列表

基本上，为了处理取消的订单，我们需要采取以下步骤:

*   正如您所观察到的，如果订单被取消，那么除了数量和发票日期之外，还有另一个交易，该交易基本上是相同的。首先，我们需要检查是否所有条目都是如此。
*   我们可以通过使用简单的逻辑来执行这个检查操作。大多数情况下，被取消的订单具有负数量，因此我们将检查是否有订单指示相同的数量(但为正)，具有相同的描述值。
*   还有一些折扣条目，我们需要处理它们。我们将丢弃折扣条目。

这个可以参考代码，如下截图所示:

![Customer and products](img/B08394_03_15.jpg)

图 3.15:处理取消订单的代码

当我们运行前面的代码时，我们发现对于所有取消的事务，在我们的数据集中没有类似的条目。为了克服这种情况，我们将在 dataframe 中创建一个新的变量,它指示事务是否已经被取消。取消订单有三种可能性:

*   有一些交易被取消，但没有对应的交易。其中一些可能是因为购买订单是在 2010 年 12 月之前执行的。我们有从 2010 年 12 月到 2011 年 12 月的数据集。
*   有一些订单正好被一个对应方取消了。我们也会考虑的。
*   有些条目值得怀疑。我们将检查是否至少有一个数量完全相同的副本可用。如果可用，那么我们可以将这些条目标记为可疑。

可以参考下面截图中显示的代码:

![Customer and products](img/B08394_03_16.jpg)

图 3.16:为取消订单生成标志的代码片段

正如我们在前面的代码片段中看到的，有 7，521 个条目显示了取消的订单及其对应的订单。有 1226 个条目显示取消的订单，但没有对应的订单。为了简单起见，我们将删除所有与取消订单相关的条目。删除这些记录的代码在下面的屏幕截图中给出:

![Customer and products](img/B08394_03_17.jpg)

图 3.17:删除取消订单的代码片段

现在让我们分析基于股票代码的条目，因为我们知道在识别取消订单的过程中，我们发现了基于*股票代码 D* 的折扣项目。首先，我们将列出所有的股票代码及其含义。可以参考以下截图:

![Customer and products](img/B08394_03_18.jpg)

图 3.18:股票代码的代码片段

现在让我们把重点放在单个订单的定价上。在给定的数据集中，来自单个客户的订单被分割成几行。我说的几行是什么意思？为了理解这一点，请参考下面的截图:

![Customer and products](img/B08394_03_19.jpg)

图 3.19:理解订单的数据输入

我们数据集中的每一个条目都代表了一种产品的奖品。如果包含不同产品的订单是由一个客户下的，那么该特定订单有多个条目。数据条目的数量取决于订单中有多少不同的产品。正如您在上图中看到的，一个订单中包含三种不同的产品。我们需要获得每份订单的总价。为了实现这一点，我们将添加一个名为 *TotalPrice* 的列，它给出订单的总价值或单个订单的购物篮价格。得出*总价*的主要逻辑是我们将*单价*乘以净数量。我们通过从总数量中减去取消的数量来获得净数量。看看下面截图中的代码片段:

![Customer and products](img/B08394_03_20.jpg)

图 3.20:获取总价的代码

一旦我们获得总价格，我们将生成单个订单的总和，然后根据发票数据对我们的条目进行分组。我们将只列出那些购物篮价格大于 0 的数据条目。下面的屏幕截图给出了实现这一点的代码:

![Customer and products](img/B08394_03_21.jpg)

图 3.21:根据发票日期生成篮子价格的代码

现在是时候了解给定数据集订单数量的分布情况了。我说的订单金额分配是什么意思？嗯，我们应该知道数据集中所有订单的价格，并且我们需要根据所有订单的数量放入范围。这将帮助我们获得数据集中超过 200 的订单数。它还将帮助我们识别低于 100 的订单数量。这类信息有助于我们了解基于订单数量的数据分布。这就让我们对电商平台上的销售有了一个基本的了解。根据订单金额生成数据分布的代码片段显示在下面的屏幕截图中:

![Customer and products](img/B08394_03_22.jpg)

图 3.22:基于订单金额生成数据分布的代码片段

您可以看到如下数据分布的图示:

![Customer and products](img/B08394_03_23.jpg)

图 3.23:数据分布的图示

正如我们所看到的，大约 65%的订单在 200 以上。我们已经非常详细地研究了订单。现在让我们从产品类别的分析开始。

##### **产品类别**

在此部分，我们将对产品相关数据属性进行 EDA。我们将在本节中包括以下类型的分析:

*   分析产品描述
*   定义产品类别
*   表征集群的内容

###### **分析产品描述**

在本节中，我们将使用两个数据属性。我们将使用`StockCode` 数据属性，它包含每个产品的惟一 ID。我们还将使用`Description` 数据属性来将产品分组到不同的类别中。先说产品描述。

首先，我们将定义将数据帧作为输入的函数，然后我们将执行以下操作:

*   我们将从产品描述中提取名称(名词)。
*   然后，我们将生成提取名称的根形式。我们将存储名称的根作为键，所有相关的名称作为它的值。对于这一步，我们将使用 NLTK 库中的词干分析器。词干分析器基本上通过去除后缀和前缀来生成单词的词根形式。
*   我们将计算名字的词根出现的频率，这意味着我们将计算每个名字的词根形式出现的次数。
*   如果不同的名字有相同的词根，那么我们认为词根形式就是关键字标签。

您可以在下面的屏幕截图中看到该函数的代码:

![Analyzing the product description](img/B08394_03_24.jpg)

图 3.24:从产品描述中生成关键字的函数的代码片段

现在我们需要来调用这个函数并输入数据帧。您可以看看下面截图中给出的代码片段:

![Analyzing the product description](img/B08394_03_25.jpg)

图 3.25:实际生成关键字的代码片段

这里，我们返回三个变量:

*   `Keyword:`这是提取的名字列表
*   `Keywords_roots:` 这是一个字典，其中键是名称的根，值是与根名称相关的名称列表。
*   `Count_keywords:`这是一本记录每个名字出现频率的词典。计数表示特定名称在描述中出现的次数。稍后，我们将把字典转换成列表。

现在让我们绘制关键词和它们的频率图。代码在下面的截图中给出:

![Analyzing the product description](img/B08394_03_26.jpg)

图 3.26:生成频率图的代码片段

正如您在上图中看到的，单词(表示名词或名称)heart 在产品描述中出现的次数最多。你可能想知道生成这个词频的意义是什么。嗯，我们用这个来分类产品。现在是时候研究如何提出产品类别了。

###### **定义产品类别**

这里我们将获取产品类别。我们获得了 1400 多个关键词，最常见的名字出现在 200 多个产品中。现在我们需要删除不太重要的单词。我们可以观察一些无用的词，比如颜色的名称，然后丢弃它们。因此，我们将考虑在数据集中出现超过 13 次的单词。可以参考下面截图中显示的代码片段:

![Defining product categories](img/B08394_03_27.jpg)

图 3.27:保留重要单词的代码片段

现在我们需要对数据进行编码。这里，我们有文本数据，我们需要将它转换成数字格式。为此，我们将使用一次性编码。一键编码是一个简单的概念。为了理解它，参考给定的矩阵 x，看一下下面的截图:

![Defining product categories](img/B08394_03_28.jpg)

图 3.28:用于理解独热数据编码的表格

如果某个特定的词出现在产品描述中，则系数值为 1，如果该词不出现在产品描述中，则系数值为 0。你可以参考下面的截图:

![Defining product categories](img/B08394_03_29.jpg)

图 3.29:独热数据编码的直观示例

如您所见，这种数据编码是一种二进制矢量化，因为我们放置的不是 0 就是 1。我们将在编码后得到每个单词的稀疏向量。通俗地说，我们可以说，这种向量化表明了这个词在产品描述中的存在。

现在让我们根据价格范围为产品创建组或集群。为此，我们将使用我们生成的关键字列表，检查产品描述中是否有关键字中出现的单词，并取平均值`UnitPrice`。可以参考下面截图中给出的代码:

![Defining product categories](img/B08394_03_30.jpg)

图 3.30:基于价格范围生成产品组的代码片段

现在我们将创建产品集群。我们将使用 k-means 聚类算法。我们还将使用 scikit-learn 库来实现 K-means 聚类算法。scikit-learn 的算法使用欧几里德距离。在我们的情况下，这不是最佳选择。我们应该使用海明距离。最适合的库是`Kmods`，但是这个库并不适用于所有的操作系统，所以我们必须使用 scikit-learn 库。我们需要定义能够完美代表数据的聚类数。我们将得出理想的聚类数，然后我们将使用剪影分数。

你可以利用这本书的链接来看看 k-means 聚类算法是如何工作的:[https://www . packtpub . com/big-data-and-business-intelligence/python-natural-language-processing](https://www.packtpub.com/big-data-and-business-intelligence/python-natural-language-processing)，参考 K-means 聚类表章节[第八章](ch08.xhtml "Chapter 8. Developing Chatbots")，*NLP 问题的机器学习*。

让我们退一步，先了解一下剪影评分。轮廓系数是用两种方法计算的。第一个是平均聚类内距离(a ),第二个是数据集中每个样本的平均最近聚类距离(b)。因此，等式如下:

*(b-a) / max (a，b)*

*b* 表示样本和样本不属于的最近聚类之间的距离。如果标签数量为*2<= n _ labels<= n _ samples–1*，则此分数有效。该分数的最佳值为 1，最差值为–1。值 0 表示我们有重叠的簇。负值表示样本被分配到了错误的聚类。参考下面截图中显示的代码片段:

![Defining product categories](img/B08394_03_31.jpg)

图 3.31:使用剪影得分选择理想聚类数的代码片段

这里，我们使用 scikit-learn API 实现了代码。正如我们所看到的，在五个集群之外，一个集群可能包含很少的元素，所以我们选择将产品分为五个集群。我们将尝试增加剪影分数的值。为此，我们将遍历数据集。可以参考下面截图中显示的代码:

![Defining product categories](img/B08394_03_32.jpg)

图 3.32:即兴创作剪影配乐的代码片段

现在让我们继续描述集群部分的内容，这可以帮助我们了解产品在特定集群中的分类情况。

##### **表征集群的内容**

在本节中，我们将分析产品集群的属性。这里将有三个小节:

*   剪影组内得分分析
*   使用词云进行分析
*   主成分分析

在我们开始分析之前，我们需要检查每个集群中的产品数量。为此，我们将使用下面截图中给出的代码片段:

![Characterizing the content of clusters](img/B08394_03_33.jpg)

图 3.33:计算每个集群产品数量的代码片段

正如您在输出中看到的，有 1009 种产品属于第 3 类，而只有 470 种产品属于第 4 类。我们将开始深入分析这五个集群及其要素。首先，我们将从剪影组内得分分析开始。

###### **剪影类内得分分析**

基本上，在这个部分，我们将检查每个元素的集群内得分。我们将对剪影组内得分进行排序。排序后我们会画一个图，其中 *x* *轴*表示剪影系数值， *y* *轴*表示聚类标签。我们为所有样本生成轮廓类内得分。我们构建此图是因为我们希望根据轮廓类内得分为`n_clusters`选择一个最佳值。

由于我们之前已经生成了轮廓类内得分，我们知道`n_clusters = 5`是我们的理想选择，因此我们将以图形方式表示这些类。可以参考下面截图中生成图形的函数:

![Silhouette intra-cluster score analysis](img/B08394_03_34.jpg)

图 3.34:剪影聚类内得分分析函数的代码片段

在执行并调用该函数后，我们可以得到如下截图所示的图形:

![Silhouette intra-cluster score analysis](img/B08394_03_35.jpg)

图 3.35:剪影类内分析的代码片段和图表

### 注意

注意，在这里，我们获得了最优`n_cluster`值的图形。在我们的例子中，这个值是 5。

##### **分析使用一个词云**

在这个部分，我们将根据关键字分析集群。我们将检查每个簇有什么单词。对于这个分析，我们将使用单词云库。你一定想知道为什么我们使用这种类型的分析。在我们的集群中，我们期望相似种类的产品属于一个集群。作为人类，我们懂得语言。当我们看到整个集群的单词时，我们可以很容易地断定我们的集群是否有相似种类的产品。我们将生成足够直观的图表，以便我们判断聚类的准确性。

可以参考下面截图中给出的代码片段:

![Analysis using a word cloud](img/B08394_03_36.jpg)

图 3.36:生成单词云的代码片段

可以参考下面截图中给出的代码片段:

![Analysis using a word cloud](img/B08394_03_37.jpg)

图 3.37:生成单词云图表的代码片段

你可以参考下面截图中的图表:

![Analysis using a word cloud](img/B08394_03_38.jpg)

图 3.38:所有五个聚类的词云图

从前面的图表中，我们可以总结出以下几点:

*   簇号 2 包含所有与礼物相关的单词，例如圣诞节、包装、礼物、卡片等等。
*   聚类数 4 包含与奢侈品和珠宝相关的所有单词。因此，诸如项链、银、蕾丝等关键字出现在该集群中。
*   有一些单词存在于每个簇中，因此很难清楚地区分它们。

现在让我们跳到下一部分，我们将执行主成分分析。

##### 主成分分析

为了让检查是否所有的集群都有真正不同的值，我们需要关注它们的组成。众所周知，关键字的一键编码矩阵具有大量的维数或大量的变量。可能会出现这样的情况，由于变量数量太多，我们的聚类算法可能会过度适应数据集。首先，我们需要减少变量的数量，但不能随意减少。我们需要选择能够代表数据集大部分特征的最重要的变量。逻辑上减少变量数量的过程称为降维。

为了实现这一点，我们将使用 PCA，这是一种统计技术，我们将执行正交变换，以便将一组高度相关的数据样本转换为一组线性不相关变量的值，这些变量称为主成分。因此，基本上，我们将使用主成分分析，因为我们希望减少变量的数量，我们已经考虑到目前为止。主成分分析是一种著名的降维技术。通过使用主成分分析，我们可以避免过拟合问题。

现在，您可能想知道可以使用 PCA 的情况，如下所示:

*   如果我们想要减少变量的数量(特征的数量或维度的数量),但是我们不能确定哪些变量可以考虑，哪些不可以
*   如果我们想确保我们的变量是相互独立的
*   如果我们愿意降低自变量的可解释性

在我们的例子中，我们需要减少变量的数量。为此，我们将实现下面截图中给出的代码:

![Principal component analysis (PCA)](img/B08394_03_39.jpg)

图 3.39:实现 PCA 的代码片段

正如您在前面的代码中看到的，我们正在检查每个组件解释的差异量。我们需要考虑 100 多个组件来解释我们数据集 90%的方差。

这里，我将考虑有限数量的组件，因为执行这种分解只是为了可视化数据。可以参考下面截图中显示的代码:

![Principal component analysis (PCA)](img/B08394_03_40.jpg)

图 3.40:生成 PCA 分解图的代码片段

正如您所看到的，我们已经使用`PCA(n_components=50)`使用了 PCA 组件，并且我们已经将值存储在数据帧`mat`中，以便将来使用。

前面代码的输出是图形的形式。所以，你可以参考下面的截图:

![Principal component analysis (PCA)](img/B08394_03_41.jpg)

图 3.41:每个聚类的主成分分析图

在这里，我们使用了`tight_layout`，这也是图形缩小了一点的原因。

到目前为止，我们已经执行了足够多的 EDA 来帮助我们生成对数据集的基本洞察。现在，我们将进入下一部分，开始构建客户类别或客户细分。我们将考虑我们迄今为止实施的所有调查结果。

### 生成客户类别

如你所知，我们的首要目标是发展客户细分。从这一部分开始，我们将主要关注如何进行客户细分。到目前为止，我们已经对订单、产品、价格等进行了分析。在这里，我们的主要焦点是基于我们在 EDA 过程中获得的洞察力来生成客户类别。

为了开发客户类别，我们将遵循以下步骤:

*   格式化数据:

    *   分组产品
    *   拆分数据集
    *   分组订单

*   创建客户类别:

    *   数据编码
    *   生成客户类别

现在让我们来看看在每一步中我们要做什么。

#### 格式化数据

正如前面提到的，我们将使用我们在 EDA 过程中得到的发现。在上一节中，我们为产品生成了五个集群。为了执行剩下的分析，我们将使用这个已经生成的关键字、矩阵和集群列表。通过使用它们，我们将生成一个新的分类变量`categ_product`。此变量表示每个产品的分类。可以参考下面截图中显示的代码片段:

![Formatting data](img/B08394_03_42.jpg)

图 3.42:生成新分类变量 categ_product 的代码片段

正如您所看到的，新变量表示每个数据条目的集群编号。现在让我们将产品分组。

##### 产品分组

你可能会想，如果我们已经开发了产品的类别，为什么还要在这里执行分组步骤。这里，我们将以这样一种方式进行分组，即我们可以知道在每个产品类别中花费了多少钱。为此，我们将添加五个新变量，例如，categ_0、categ_1、categ_2、categ_3 和 categ_4。可以参考下面截图中显示的代码片段:

![Grouping products](img/B08394_03_43.jpg)

图 3.43:生成每个产品类别花费金额的代码片段

订单被分割成多个条目，所以我们需要使用购物篮价格。这一次，我们将合并篮子价格以及它在五个产品类别中的分配方式。我们将把所有这些信息放入新的数据框架中。参考以下截图:

![Grouping products](img/B08394_03_44.jpg)

图 3.44:获取五个分类的篮子价格分布的代码片段

最后，我们有每个订单的篮子价格，我们还知道五个集群的价格分布。新的数据帧是`basket_price`。现在让我们进入下一部分。

##### 分割数据集

在本节中，我们将使用 dataframe `basket_price`，它包含过去 12 个月的数据条目。这个应用的第二个目标是根据客户的第一次现场访问或购买来预测他们的购买行为。因此，为了立即实现这一目标，我们将拆分数据集。我们将使用 10 个月的数据集进行训练，2 个月的数据集进行测试。我在这里包括这一步骤，因为稍后，我们可以使用这些训练和测试数据集，并且您可以很容易地使用新的数据框架。可以参考下面截图中给出的代码:

![Splitting the dataset](img/B08394_03_45.jpg)

图 3.45:使用时间分割数据集的代码片段

现在，我们将对客户及其订单以及购物篮价格分布进行分组。

##### 订单分组

在这里，我们将合并客户和他们的订单，以便我们可以了解哪个客户下了多少订单。我们还将生成最小订单金额、最大订单金额和平均订单金额。参考下面截图中给出的代码:

![Grouping orders](img/B08394_03_46.jpg)

图 3.46:为每个客户生成订单统计数据的代码片段

我们还将生成两个变量，指示自上次购买和首次购买以来经过的天数。这些变量的名字是`FirstPurchase` 和`LastPurchase`。参考下面截图中给出的代码片段:

![Grouping orders](img/B08394_03_47.jpg)

图 3.47:为最后一次和第一次购买生成经过天数的代码片段

我们感兴趣的客户类别是那些只下一份订单的客户。我们的主要目标之一是以这样一种方式锁定这些客户，以便我们能够留住他们。我们需要获得属于这个类别的客户数量的数据。为此，请参考下面截图中给出的代码:

![Grouping orders](img/B08394_03_48.jpg)

图 3.48:生成一次购买的客户数量的代码片段

从前面的代码中，我们可以发现 40%的客户只下了一个订单，我们需要留住他们。

现在让我们建立客户类别。

#### 创建客户类别

基本上，我们将在这里进行客户细分。因此，我们将致力于实现本章的第一个目标。我们将根据客户的购买模式进行客户细分。这里有两个步骤:

*   数据编码
*   生成客户类别或客户细分

我们将从数据编码开始。

##### 数据编码

我们将生成数据帧，其中包含迄今为止我们执行的所有操作的摘要。该数据帧的每个记录都与单个客户端相关联。我们可以利用这些信息来描述不同类型的客户。

我们生成的数据框架有不同的变量。所有这些变量都有不同的范围和变化。因此，我们需要生成一个矩阵，将这些数据条目标准化。可以参考下面截图中给出的代码:

![Data encoding](img/B08394_03_49.jpg)

图 3.49:为每个客户端生成摘要数据条目的代码片段

在创建客户细分之前，我们需要创建基础。这个基数应该包括重要的变量。我们需要包括少量的重要变量。为了选择重要的变量，我们将使用主成分分析。因此，我们可以准确地描述分割。我们将使用 PCA 来完成这项任务。下面的屏幕截图中给出了代码片段:

![Data encoding](img/B08394_03_50.jpg)

图 3.50:生成客户细分的 PCA 代码片段

在这里，我们可以看到有八个主成分。现在让我们继续下一部分，我们将生成客户细分。

##### 生成客户类别

我们将使用 k-means 聚类算法来生成分割。聚类的数量将通过使用轮廓分数来导出。我们之前已经使用了剪影分数，通过使用相同的方法，我们可以导出聚类的数量。这里，我们基于轮廓分数获得 11 个聚类。可以参考以下截图:

![Generating customer categories](img/B08394_03_51.jpg)

图 3.51:生成客户细分的代码片段

正如您所看到的，分割的大小有很大的差异，因此我们需要使用 PCA 来分析聚类的组成部分。

##### 主成分分析

这里我们将使用六个组件。下面的屏幕截图给出了 11 个集群的 PCA 的代码片段和图形表示:

![PCA analysis](img/B08394_03_52.jpg)

图 3.52:实现 PCA 和生成图形的代码片段

作为输出，生成了以下图表:

![PCA analysis](img/B08394_03_53.jpg)

图 3.53:客户细分的主成分分析图

我在这里只展示了三张图。在代码中，有九个图形。当您运行代码时，您可以看到它们。请注意，第一个组件将最小的集群与其他集群分开。对于该数据集，我们可以说总会有一个表示，其中两个线段看起来是不同的。现在让我们获得轮廓分数。

##### 使用剪影分数分析聚类

在此部分，我们将为每个聚类生成轮廓分数。这将表明数据样本分离的质量。可以参考下面截图中显示的代码片段和图形:

![Analyzing the cluster using silhouette scores](img/B08394_03_54.jpg)

图 3.54:生成轮廓分数图表的代码片段

从前面的图中，我们可以确保所有的集群都是分离的。现在，我们需要更多地了解每个集群的客户习惯。为此，我们将添加定义每个客户所属的集群的变量。

为此，我们将生成一个新的数据帧`selected_customers`。生成新的数据帧后，我们将对数据帧的内容进行平均。这将为我们提供平均购物篮价格，总访问量，等等。可以参考下面截图中显示的代码:

![Analyzing the cluster using silhouette scores](img/B08394_03_55.jpg)

图 3.55:存储客户习惯的代码片段

现在我们需要重组数据帧的内容。这里我们将考虑两点:

1.  我们需要根据每个产品类别的花费来重新组织数据
2.  之后，我们将根据花费的总额重新组织内容

您可以看看下面截图中显示的实现:

![Analyzing the cluster using silhouette scores](img/B08394_03_56.jpg)

图 3.56:重组数据集的代码片段

正如您所见，我们已经获得了每个细分市场的客户行为。现在我们可以根据这些特点来推荐商品了。我们可以根据产生的事实设计营销活动。

特定的营销策略可以应用于属于聚类 4 和聚类 8 的客户。我们应该向集群 1 客户推荐优质产品。

到目前为止，我们已经实现了第一个目标。现在是时候瞄准第二个目标了。所以让我们开始吧！

### 客户分类

在我们开始之前，让我们回顾一下我们的目标是什么。这有助于你以更清晰的方式理解事物。目标是我们将构建一个分类器，将客户分类到上一节中建立的不同客户群中。我们还需要多一个特性。当客户第一次访问平台时，我们的分类器应该生成这个分类结果。为了实现这种功能，我们将使用各种监督机器学习算法。我们将使用 scikit-learn API。

为了让开发基线分类器，我们需要执行以下步骤:

*   定义助手功能
*   将数据分为训练和测试
*   实现机器学习(ML)算法

#### 定义助手功能

基本上，我们定义了一个名为`class_fit`的类，然后我们定义了在训练 ML 模型时可以帮助我们的各种函数。这些是我们将使用的助手功能:

1.  `train`功能帮助我们训练模型
2.  `predict`函数帮助我们预测测试数据集或新数据样本的结果
3.  `grid_search`功能帮助我们找到合适的超参数和交叉验证(CV)折叠值
4.  `grid_fit`函数帮助我们使用交叉验证训练模型，并生成最佳超参数。
5.  `grid_predict`功能帮助我们生成预测以及准确度分数。

可以参考下面截图中显示的代码片段:

![Defining helper functions](img/B08394_03_57.jpg)

图 3.57:助手函数的代码片段

现在让我们进入下一部分。

#### 将数据分为训练和测试

我们将使用存储在`selected_customers`数据帧中的数据。您可以看到我们将应用 ML 算法的数据集的一些条目。看一下下面的截图:

![Splitting the data into training and testing](img/B08394_03_58.jpg)

图 3.58:数据集中的样本条目

如您所见，我们将预测新客户的分类数，因此我们将那个值存储为`Y`，并且像`mean, categ_0 to categ_4`这样的列被用作 ML 模型的输入特征，因此我们将它们存储在`X`变量中。现在，我们需要将这些数据分为训练和测试。为此，我们使用 sklearn API `train_test_split()`。我们将 80%的数据用于训练，20%的数据用于测试。看一下下面的截图:

![Splitting the data into training and testing](img/B08394_03_59.jpg)

图 3.59:将数据集分为训练和测试的代码片段

我们有训练和测试数据集。现在，我们需要开始实现 ML 算法。

#### 实现机器学习(ML)算法

对于基线方法，我们将实现支持向量机(SVM)分类器。我们将使用之前定义的帮助函数。在这里，我将创建该类的一个实例，并调用我们之前声明的方法。看看下面截图中的代码片段:

![Implementing the Machine Learning (ML) algorithm](img/B08394_03_60.jpg)

图 3.60:使用 SVM 分类器训练模型的代码片段

正如您在代码片段中看到的，`svc`是类实例。我们使用线性 SVM。我们使用`grid_search`搜索最佳超参数，并获得 CV 折叠的数量。之后，我们调用了`grid_fit`方法，该方法用于使用我们的训练数据集来训练 ML 模型。

这是我们实施基线方法的方式。现在让我们测试结果。

## 了解测试矩阵

我们将使用混淆矩阵和学习曲线来评估 ML 模型。所以在开始测试之前，我们需要理解混淆矩阵和学习曲线是什么。我们将逐一介绍这些概念。

### 混淆矩阵

当我们实现多类分类器时，自然地，我们有多个类，并且属于所有类的数据条目的数量是不同的，所以在测试期间，我们需要知道分类器是否对所有类都表现得一样好，或者它是否偏向某些类。这种分析可以使用混淆矩阵来完成。它将计算有多少数据条目被正确分类，有多少被错误分类。

我们举个例子。比方说，总共有 10 个数据条目属于一个类，该类的标签是 1。现在，当我们从 ML 模型生成预测时，我们将检查 10 个条目中有多少个条目得到预测的类标签 1。假设六个数据条目被正确分类，并得到类标签 1。在这种情况下，对于六个条目，*预测标签*和*真实标签*是相同的，因此准确率为 60%，而对于剩余的数据条目，ML 模型将它们错误分类。ML 模型预测除 1 以外的类别标签。

从前面的例子中，您可以看到混淆矩阵让我们知道有多少数据条目被正确分类，有多少被错误分类。我们可以探索分类器的分类准确性。看一下下面的截图:

![Confusion matrix](img/B08394_03_61.jpg)

图 3.61:混淆矩阵示例

现在我们来看一下的学习曲线。

### 学习曲线

我们在这里画了两条线。一行表示训练分数，另一行表示测试分数。这里，训练和测试分数确定了不同训练数据集大小的交叉验证的训练和测试分数。通过使用该学习曲线，我们可以监控 ML 模型是否正确收敛。CV 分数和训练分数都将帮助我们确定训练是否在正确的方向上进行，或者 ML 模型是否过拟合或欠拟合。随着数据集大小的增加，如果 CV 分数和训练分数达到较低的分数，则意味着训练没有以适当的方式进行。然而，如果 CV 分数和训练分数随着数据集大小的增加而增加，那么这意味着训练正在向正确的方向发展。参考以下截图:

![Learning curve](img/B08394_03_62.jpg)

图 3.62:学习曲线的好例子和坏例子

现在我们已经理解了测试矩阵背后的基本直觉，我们可以开始测试我们的基线方法了。

## 测试基线方法的结果

在此部分，我们将使用以下方法测试基线模型:

*   生成分类器的准确度分数
*   为分类器生成混淆矩阵
*   生成分类器的学习曲线

### 生成分类器的准确度分数

首先，我们将使用`grid_predict`来生成测试数据集的准确度分数。我们将检查 SVM 算法的准确性。为此，下面的屏幕截图中给出了代码片段:

![Generating the accuracy score for classifier](img/B08394_03_63.jpg)

图 3.63:生成准确度分数的代码片段

基线方法的精确度为 79.50%。现在让我们用混淆矩阵来看看预测的质量。

### 为分类器生成混淆矩阵

现在我们将生成混淆矩阵，这将给我们一个公平的想法，哪个类被正确分类，哪个类在大多数时间对数据进行了错误分类。要生成混淆矩阵，请参考下面截图中给出的代码:

![Generating the confusion matrix for the classifier](img/B08394_03_64.jpg)

图 3.64:生成混淆矩阵的代码片段

我们已经为 sklearn 使用了`confusion_matrix` API。为了绘制这个图，我们将定义一个名为`plot_confusion_matrix`的方法。在前面代码的帮助下，我们生成了下面截图中给出的混淆矩阵:

![Generating the confusion matrix for the classifier](img/B08394_03_65.jpg)

图 3.65:基线方法的混淆矩阵

正如您所看到的，分类器能够准确地将数据分类为类标签 0、2、4、6 和 10，而对于类标签 1、5、7 和 8，分类器的表现并不太好。

让我们画出基线方法的学习曲线。

### 生成分类器的学习曲线

学习曲线指示分类器是否面临过拟合或欠拟合问题。`plot_learning_curve`方法用于绘制分类器的学习曲线。可以参考下面截图中的代码片段:

![Generating the learning curve for the classifier](img/B08394_03_66.jpg)

图 3.66:为基线方法生成学习曲线的代码片段

学习曲线如下图所示:

![Generating the learning curve for the classifier](img/B08394_03_67.jpg)

图 3.67:基线方法的学习曲线

如你所见，当我们增加样本量时，CV 曲线收敛于相同的极限。这意味着我们有很低的方差，我们没有遭受过度拟合。方差是指示如果我们将提供不同的训练数据集，我们的目标函数将改变多少的值。理想地，目标函数的值通过机器学习算法从训练数据集导出，然而，如果我们使用另一个训练数据集，估计函数的值不应该改变太多。估计函数的微小变化(微小方差)是可以预期的。这里，准确度分数具有较低的偏差，这意味着模型也没有面临拟合不足的问题。

## 基线方法的问题

在本节中，我们将讨论基线方法面临的问题，以便优化当前方法。这些问题如下:

*   精度分数低。还有改进的余地。
*   我们需要尝试其他的最大似然算法，以便比较结果。稍后，如果有需要，我们可以建立投票机制。

基本上，在修订的方法中，我们需要尝试各种 ML 算法，这样我们才能确定哪些算法可以使用，哪些不应该使用。

## 优化基线方法

在这一节，我们将考虑所有的问题，并讨论通过的方法，我们将增加我们的分类器的准确性。正如上一节所讨论的，我们需要实现其他的 ML 算法。这是我们将使用修订后的方法实施的六种算法:

*   逻辑回归
*   k-最近邻
*   决策图表
*   随机森林
*   Adaboost 分类器
*   梯度推进分类器

基于前面所有算法的精度得分，我们将决定哪个算法可以使用，哪个不能使用。

不浪费时间，让我们开始实施修改后的方法。

# 构建修订的方法

在这个部分，我们将实现各种 ML 算法，检查它们的精度分数，并监控它们的学习曲线。总共有六种 ML 算法将用于确定哪一种最适合我们的应用。

## 实施修订后的方法

在本节中，我们将实现逻辑回归、K-最近邻、决策树、随机森林、Adaboost 和梯度下降。为了实现这一点，我们将使用我们之前构建的 helper 类。您可以看看下面截图中给出的代码片段:

![Implementing the revised approach](img/B08394_03_68.jpg)

图 3.68:使用各种 ML 分类器执行训练的代码片段

我们已经为所有分类器生成了精度分数。我们可以看到随机森林和梯度推进分类器具有很高的精度。然而，我们仍然没有检查他们的学习曲线。首先，我们将检查他们的学习曲线，然后得出任何分类器是否面临过拟合或欠拟合的问题。

## 测试修订后的方法

在这个部分，我们将检查所有分类器的学习曲线。可以参考下面截图中的学习曲线:

![Testing the revised approach](img/B08394_03_69.jpg)

图 3.69:各种 ML 分类器的学习曲线

你可以看到所有的分类器都经过了适当的训练。不存在装配不足或装配过度的问题。随着数据量的增加，分数也在提高。

## 修订方法的问题

这种方法的主要问题是我们需要决定我们需要使用哪种算法，以及我们应该停止使用哪种算法。我们将丢弃 Adaboost 分类器，因为它的精度分数太低。

这里我需要强调另一个问题。没有一个单一的分类器能很好地适用于所有的类别标签。可能有一个分类器适用于类别标签 0，而另一个可能适用于类别标签 8。我认为，我们不应该抛弃任何其他分类器。我们需要想出一个投票机制。用更专业的术语来说，我们需要开发一个集合模型，这样我们的预测质量才能更好、更准确。

现在，我们将看看我们的方法是什么，以建立一个投票分类器，可以给我们最好的准确性。

### 了解如何改进修订后的方法

如前所述，为了改进修订后的方法，我们将使用投票机制。为此，我们将使用 scikit-learn 投票分类器 API。首先，我们将使用网格搜索来为每个分类器生成适当的超参数。之后，我们将使用 scikit 的投票分类器 API 来学习和训练模型。方法很简单，所以让我们开始实现它。

# 最好的方法

我们将用这种方法生成的分类器模型应该会给我们提供尽可能好的准确性。我们已经讨论过这种方法。如果你是 ML 模型集合的新手，那么让我给你一个基本的直观概念。通俗地说，ensemble ML 模型基本上使用了各种 ML 算法的组合。将各种 ML 模型结合在一起有什么好处？我们知道没有一个单一的分类器可以完美地分类所有的样本，所以如果我们结合一个以上的分类器，那么我们可以获得更高的准确性，因为一个分类器的问题可以被另一个分类器克服。由于这个原因，我们将使用投票分类器，它是一种集成分类器。

## 实施最佳方法

如你所知，我们使用网格搜索和投票分类器 API 来实现最佳方法。正如所讨论的，首先，我们将使用网格搜索来获得最佳可能的超参数，然后使用投票分类器 API。下面的截图给出了一步一步的实现:

![Implementing the best approach](img/B08394_03_70.jpg)

图 3.70:最佳方法的代码片段

如你所见，我们用这种方法得到了 90%的精确度。这一次，我们需要在我们两个月的坚持语料库上测试方法,以便我们可以发现投票分类器在看不见的数据集上表现如何。

在下一节中，我们将测试这种方法。

## 测试最佳方法

我们在 20%的数据集上测试我们的 ML 模型，我们甚至在开始训练之前就把它放在一边。这个数据集对我们来说是一个开发数据集。对于训练，我们考虑了 10 个月的数据集。现在是时候在 hold out 语料库上测试这个模型了。在这里，我们的 hold-out 语料库包含 2 个月的数据条目。这些是我们需要实施的步骤:

*   以训练数据集的形式转换拒绝语料库
*   将转换后的数据集转换为矩阵形式
*   生成预测

所以让我们从第一步开始。

### 以训练数据集的形式转换拒绝的语料库

首先，我们需要将驻留在`set_test` 数据帧中的数据转换成训练数据集的形式。为此，我们将用名称`basket_price`将副本存储在新的数据帧中。

现在，我们将借助对基线方法执行的相同操作来生成用户特征数据。别担心。当您看到代码时，您会记得我们之前执行的步骤。在转换数据集之后，我们将把它存储在数据帧`transactions_per_user`中。可以参考下面截图中显示的代码片段:

![Transforming the hold-out corpus in the form of the training dataset](img/B08394_03_71.jpg)

图 3.71:将测试数据集转换成相同形式的训练数据集的代码片段

现在让我们将数据集转换成矩阵形式。

### 将转换后的数据集转换成矩阵形式

我们的分类器将矩阵作为输入，因此我们需要将转换后的数据集转换成矩阵格式。为此，我们将使用下面截图中显示的代码片段:

![Converting the transformed dataset into a matrix form](img/B08394_03_72.jpg)

图 3.72:将测试数据集转换成矩阵格式的代码片段

我们在这里使用一个基本的类型转换。

### 生成预测

在本节中，我们将使用投票分类器生成精度分数。因此，为了为测试数据集生成预测，我们需要使用下面截图中给出的代码片段:

![Generating the predictions](img/B08394_03_73.jpg)

图 3.73:为测试数据集生成精度分数的代码片段

正如你所看到的，我们将达到 76%的准确率。这很好，因为我们只用了 10 个月的数据来建立这个模型。通过使用 10 个月的数据集，我们实现了该领域的最佳精度。如果我们考虑更多的数据记录，那么我们仍然可以改善结果。这对你们来说是一个考虑更多数据集和随机应变结果的练习。

# 不同领域的客户细分

请注意，我们在这里考虑的是电子商务数据，但是您也可以考虑各种领域的其他数据集。您可以为提供旅游服务、金融服务等的公司建立客户细分。数据点会因域而异。

对于旅游服务，您可以考虑用户使用旅游平台预订航班或房间的频率。人口统计和专业信息帮助很大，比如说，用户使用促销优惠的次数。用户活动的数据也很重要。

如果您正在为金融领域构建细分应用，那么您可以考虑以下数据点:帐户持有人的交易历史，例如，使用借记卡或信用卡的频率、每月收入、每月支出、客户在银行帐户中的平均余额、用户拥有的帐户类型、客户的专业信息等等。对于这两个领域，还有其他一些你可以考虑的公共数据点，比如花在网站或移动应用上的时间。

现在，我将把自己限制在这两个领域，但是您可以对电信领域、营销领域、教育领域、娱乐领域等等进行客户细分。

# 总结

到目前为止，我们开发的所有给定分析模型对于成功运营企业至关重要。在这一章中，我们根据客户的行为进行了客户细分。为了做到这一点，我们使用了各种算法，如 SVM，线性回归，决策树，随机森林，梯度推进，基于投票的模型，等等。通过使用基于投票的模型，我们获得了最好的准确性。客户细分分析对于中小型组织非常重要，因为这些分析有助于他们优化营销策略，并显著降低客户获取成本。我为客户流失分析开发了代码，可在:[https://github.com/jalajthanaki/Customer_churn_analysis](https://github.com/jalajthanaki/Customer_churn_analysis)获得，为客户终身价值分析开发了代码，可在:[https://github . com/jalajthanaki/Customer _ lifetime _ value _ analysis](https://github.com/jalajthanaki/Customer_lifetime_value_analysis)获得。你可以参考它们来了解更多关于客户分析的知识。你可以在[https://github.com/Acrotrend/Awesome-Customer-Analytics](https://github.com/Acrotrend/Awesome-Customer-Analytics)了解客户分析。

在接下来的章节中，我们将构建一个特定于电子商务产品的推荐系统。我们将构建一个推荐应用，根据用户在平台上的浏览和购买活动向用户推荐书籍。我们将实现各种技术来构建最好的推荐引擎。所以继续读吧！