

# 十一、深度学习

到目前为止，我们已经介绍了一些监督、半监督、非监督和强化学习技术和算法。在这一章中，我们将涵盖神经网络及其与深度学习实践的关系。传统的学习方法是编写告诉计算机做什么的程序，但神经网络是利用形成主要输入来源的观察数据来学习和寻找解决方案。这项技术的成功取决于神经网络的训练方式(即观察数据的质量)。深度学习指的是学习前面提到的神经网络的方法。

技术的进步将这些技术带到了新的高度，这些技术展示了卓越的性能，并用于解决计算机视觉、语音识别和 **自然语言处理** ( **NLP** )中的一些重要需求。像脸书和谷歌这样的大公司已经在很大程度上采用了深度学习实践。

本章的主要目的是从概念上加强对神经网络和相关深度学习技术的掌握。借助于一个复杂的模式识别问题，本章涵盖了开发一个典型的神经网络的程序，你将能够用它来解决一个类似的复杂问题。以下图示显示了本书涵盖的所有学习方法，突出了本章学习的主要主题— *深度学习*。

![Deep learning](img/B03980_11_01.jpg)

本章深入讨论了以下主题:

*   快速回顾机器学习的目的、学习的类型以及深度学习的背景，并详细介绍它解决的特定问题。
*   神经网络概述:

    *   人脑是神经网络的主要灵感来源
    *   神经网络架构的类型和神经元的一些基本模型
    *   一个简单的学习示例(数字识别)
    *   感知器概述、第一代神经网络以及它们能够做什么和不能做什么

*   线性和逻辑输出神经元概述。介绍反向传播算法，并应用反向传播算法的衍生物来解决一些现实世界的问题
*   认知科学的概念、softmax 输出函数和处理多输出场景
*   卷积网络的应用与物体或数字识别问题
*   **循环神经网络**(**【RNN】**)和梯度下降法
*   作为部件分析和自编码器原理的信号处理:自编码器的类型有深编码器和浅编码器
*   使用 Apache Mahout、R、Julia、Python (scikit-learn)和 Apache Spark 的实践练习

# 背景

让我们首先回顾机器学习的前提，并强化学习方法的目的和背景。正如我们所知，机器学习是通过使用观察数据建立模型来训练机器，而不是直接编写特定的指令来定义数据模型，以解决特定的分类或预测问题。*车型*这个词在这个语境里无非就是一个*系统*。

程序或系统是使用数据构建的，因此看起来与手写的非常不同。如果数据发生变化，程序也会根据新数据进行下一级的训练。因此，它需要的是大规模处理的能力，而不是让一个熟练的程序员为所有可能仍然被证明是严重错误的情况编写代码。

我们有一个机器学习系统的例子，叫做垃圾邮件检测器。该系统的主要目的是识别哪些邮件是垃圾邮件，哪些不是。在这种情况下，垃圾邮件检测器没有被编码为处理每种类型的邮件；相反，它从数据中学习。因此，这些模型的精确度总是依赖于观测数据的好坏。换句话说，从原始数据中提取的特征通常应该覆盖数据的所有状态，以使模型准确。构建特征提取器是为了从分类器或预测器使用的给定数据样本中提取标准特征。

更多的例子包括识别模式，如语音识别、对象识别、人脸检测等等。

深度学习是一种机器学习，它试图从给定的数据中学习突出的特征，从而试图减少为每一类数据(例如，图像、语音等)构建特征提取器的任务。).对于人脸检测需求，深度学习算法会记录或学习鼻子的长度、两眼之间的距离、眼球的颜色等特征。该数据用于解决分类或预测问题，并且显然与传统的 **浅层学习算法**非常不同。

## 人类的大脑

众所周知，人脑是人体中最难以置信的器官之一。大脑本质上是让我们人类变得聪明的东西。它负责根据我们对触觉、嗅觉、视觉、视觉和听觉的体验来建立我们的感知。这些经历被收集并储存为记忆和情感。从本质上来说，大脑使我们变得聪明，没有它，我们可能只是这个世界上的原始生物。

新生婴儿的大脑能够解决任何复杂而强大的机器都无法解决的问题。事实上，就在出生后的几天内，婴儿就开始认识他/她的父母的面部和声音，并开始表现出当他们不在身边时渴望见到他们的表情。经过一段时间后，他们开始将声音与物体联系起来，甚至可以认出一个有视觉的物体。现在，他们是怎么做到的呢？如果他们遇到一只狗，他们如何认出它是一只狗；还有，他们会把狗吠声和它联系起来并模仿同样的声音吗？

很简单。每当婴儿遇到一只狗，他/她的父母就把它说成是一只狗，这加强了孩子的模型。如果他们认为孩子是错的，孩子的模型会包含这些信息。所以，狗有长耳朵，长鼻子，四条腿，一条长尾巴，可以是不同的颜色，如黑色，白色或棕色，发出吠叫声。这些特征是通过婴儿大脑记录的视觉和听觉来识别的。由此收集的观测数据推动了今后对任何新天体的认识。

现在，假设婴儿第一次看到一只狼；他/她会通过观察狼的特征的相似性来识别狼是狗。现在，如果父母在第一次看到狼时输入明确的差异，例如，它发出的声音的差异，那么这就成为一种新的体验，并存储在存储器中，用于下一次看到。随着越来越多这样的例子的同化，孩子的模型变得越来越准确；这个过程是很潜意识的。

几年来，我们一直致力于建造像人类一样具有智能的机器。我们正在谈论的机器人可以像人类一样行动，并且可以以与人类相似的效率完成特定的工作，例如开车、打扫房间等等。现在，把机器建成机器人需要什么？我们可能需要建立一些超级复杂的计算系统来解决我们的大脑可以立即解决的问题。致力于构建人工智能系统的这个领域被称为深度学习。

以下是深度学习的一些正式定义:

根据维基百科，深度学习是一套机器学习的算法，试图通过使用由多个非线性变换组成的模型架构来对数据中的高级抽象进行建模。

根据[http://deeplearning.net/](http://deeplearning.net/)的说法，深度学习是机器学习研究的新领域，其目的是让机器学习更接近其最初的目标之一——人工智能。

这个主题已经发展了好几年；下表列出了多年来的研究领域:

| 

研究领域

 | 

年

 |
| --- | --- |
| 神经网络 | 1960 |
| 多层感知器 | 1985 |
| 受限玻尔兹曼机 | 1986 |
| 支持向量机 | 1995 |
| 韩丁介绍了 **深度信仰网** ( **DBN** )深度学习和 RBM 的新兴趣最先进的 MNIST | 2005 |
| 深度循环神经网络 | 2009 |
| 卷积 DBN | 2010 |
| 最大池 CDBN | 2011 |

在许多其他人当中，这一领域的一些主要贡献者是杰弗里·辛顿、扬·勒村、洪拉克·李、安德鲁·y·Ng 和约舒阿·本吉奥。

以下概念模型涵盖了深度学习的不同领域以及本章涵盖的主题范围:

![The human brain](img/B03980_11_02.jpg)

让我们来看看手头的一个简单问题；要求是从这里给出的手写笔迹中识别数字:

![The human brain](img/B03980_11_03.jpg)

对于人脑来说，这非常简单，因为我们可以识别数字 287635。我们大脑解读数字的简单性被认为破坏了这个过程的复杂性。由于视觉皮层的存在，我们的大脑被训练成逐步拦截不同的视觉，每个皮层包含超过 1.4 亿个神经元，它们之间有数十亿个连接。简而言之，我们的大脑不亚于一台已经进化了几百万年的超级计算机，众所周知它能很好地适应视觉世界。

如果计算机程序必须破解数字的识别，那么识别和区分一个数字和另一个数字的规则应该是什么？

神经网络就是这样一个研究了几年的领域，众所周知它解决了多层学习的需要。总体思路是馈入大量手写数字；下图显示了此数据(训练)的一个示例，可以从这些示例中学习。这意味着规则是从提供的训练数据中自动推断出来的。因此，训练数据集越大，预测就越准确。如果我们遇到一个问题，要区分数字 1 和数字 7，或者数字 6 和数字 0，我们需要学习一些细微的区别。对于零点，起点和终点之间的距离很小或者为零。

![The human brain](img/B03980_11_04.jpg)

区别基本上是因为这些学习方法的目标是模仿人脑。让我们看看是什么让这个问题变得难以解决。

总之，深度学习是机器学习的一个子集，我们知道这涉及到输入示例的技术和一个模型，该模型可以评估模式，以便在它出错的情况下进行进化。因此，在一段时间内，这个模型将尽可能精确地解决问题。

如果这需要用数学来表示，让我们定义我们的模型为一个函数 *f(x，θ)* 。

这里， *x* 是作为值的向量提供的输入，而 *θ* 是模型用来预测或分类 *x* 的参考向量。因此，为了提高准确性，我们需要接触最大数量的示例。

我们来举个例子；如果我们基于两个因素来预测一个去餐馆的游客是否会回来——一个是账单的金额(*x[1]，另一个是他/她的年龄(*x[2]T18)。当我们收集特定持续时间的数据并分析它的输出值时，输出值可以是 1(如果访问者回来了)或-1(如果访问者没有回来)。绘制时，数据可以采用任何形式，从线性关系到任何其他复杂形式，如下所示:**

![The human brain](img/B03980_11_05.jpg)

类似线性关系的东西看起来很简单，而更复杂的关系会使模型的动态变得复杂。参数 *θ* 到底能不能有一个最优值？我们可能需要应用优化技术，在接下来的章节中，我们将讨论这些技术，比如感知器和梯度下降法等等。如果我们想开发一个程序来做到这一点，我们需要知道我们的大脑是如何识别这些数字的，即使我们知道，这些程序在本质上可能非常复杂。

## 神经网络

神经计算一直是这项研究的主要兴趣，旨在了解并行计算如何在神经元中工作(灵活连接的概念)，并像人脑一样解决实际问题。现在让我们来看看人类大脑的核心基本单元，神经元:

![Neural networks](img/B03980_11_06.jpg)

### 神经元

人类大脑是关于神经元和连接的。神经元是大脑中最小的部分，如果我们取一小米粒大小的大脑，已知它至少包含 10000 个神经元。每个神经元平均有大约 6000 个与其他神经元的连接。如果我们观察神经元的一般结构，它看起来如下。

我们人类经历的每一种感觉，无论是思想还是情感，都是因为我们大脑中数百万个被称为神经元的细胞。由于这些神经元通过传递信息相互交流，人类感觉、行动并形成感知。这里的图表描述了生物神经结构及其组成部分:

![Neuron](img/B03980_11_07.jpg)

每个神经元都有一个中央胞体；正如任何细胞一样，一般来说，它有一个轴突和一个树突树，分别负责与其他神经元发送和接收信息。轴突连接到树突树的地方称为突触。突触本身有一个有趣的结构。它们包含引发传递的传递分子，传递性质可以是阳性或阴性。

神经元的输入被聚集，当它们超过阈值时，电尖峰被传输到下一个神经元。

### 突触

下面的图描绘了突触的模型，描述了信息从轴突到树突的流动。突触的工作不仅仅是传递信息，事实上，它使自己适应信号的流动，并具有从过去的活动中学习的能力。

![Synapses](img/B03980_11_08.jpg)

作为机器学习的领域中的一个类比，传入连接的强度是根据它被使用的频率来确定的，因此它对神经元输出的影响被确定。人类潜意识里就是这样学习新概念的。

此外，诸如药物或身体化学物质等外部因素也可能影响这一学习过程。

现在，我们将借助以下列表，总结大脑内部的学习过程:

*   神经元与其他神经元或有时与受体进行交流。皮质神经元使用尖峰信号进行交流。
*   神经元之间的连接强度会发生变化。它们可以通过建立和移除神经元之间的连接，或者通过基于一个神经元对另一个神经元的影响来加强连接，来获取正值或负值。一个叫做**长时程增强** ( **LTP** )的过程发生，导致这种长期影响。
*   大约有 1011 个神经元的权重使得人脑可以比工作站更有效地进行计算。
*   最后，大脑是模块化的；大脑皮层的不同部分负责做不同的事情。一些任务在一些区域注入比其他区域更多的血液，从而确保不同的结果。

在将神经元模型系统化为**人工神经网络** ( **安**)之前，让我们先看看不同类型、类别或方面的神经元，并在中具体说明人工神经元或感知器，即生物神经元的深度学习等价物。众所周知，这种方法在我们在前一节列出的一些用例中产生了非常有效的结果。ann 又叫前馈神经网络，**多层感知器** ( **MLP** )，最近又叫，深度网络或学习。一个重要的特征是需要特征工程，而深度学习代表了需要最少特征工程的应用，其中学习通过多层神经元进行。

### 人工神经元或感知器

显然人工神经元从生物神经元中汲取灵感，如前所述。人工神经元的特征如下:

*   有一组从其他神经元接收到的输入在上下文中激活该神经元
*   有一个输出递质传递信号或激活其他神经元
*   最后，核心处理单元负责从输入激活产生输出激活

神经元的理想化是一个应用于建立模型的过程。简而言之，就是一个简化的过程。一旦简化，就有可能应用数学和相关的类比。在这种情况下，我们可以很容易地增加复杂性，并使模型在确定的条件下稳健。需要采取必要的谨慎措施，确保在简化过程中不会删除任何有重大影响的方面。

#### 线性神经元

线性神经元是神经元的最简单形式；它们可以表示如下:

![Linear neurons](img/B03980_11_10.jpg)

输出 *y* 是输入*x[I]与其权重*w[I]的乘积之和。这在数学上表示如下:**

![Linear neurons](img/B03980_11_41.jpg)

这里， *b* 是偏置。

前面等式的图形表示如下:

![Linear neurons](img/B03980_11_11.jpg)

#### 整流线性神经元/线性阈值神经元

整流线性神经元与线性神经元类似，如前一节所述，与的微小区别在于，当小于(<)零(0)时，输出参数的值被设置为零，当输出值大于(>)零(0)时，它继续保持为输入的线性加权和:

![Rectified linear neurons / linear threshold neurons](img/B03980_11_12.jpg)

#### 二元阈值神经元

二进制阈值神经元是由麦卡洛克和皮茨在 1943 年引入的。这类神经元首先计算输入的加权和，类似于线性神经元。如果该值超过定义的阈值，则发出固定大小的活动峰值。这个尖峰被称为命题的*真值。另外很重要的一点是输出。任何给定时间点的输出都是二进制的(0 或 1)。*

展示这种行为的等式如下所示:

![Binary threshold neurons](img/B03980_11_42.jpg)

和

*y = 1* 如果 *z ≥ θ* ，

*y = 0* 否则

这里 *θ = -b(偏置)*

(或)

![Binary threshold neurons](img/B03980_11_43.jpg)

和

*y = 1* 如果 *z ≥ 0* ，

*y = 0* 否则

此外，这里给出了前一个方程的图示:

![Binary threshold neurons](img/B03980_11_13.jpg)

#### 乙状结肠神经元

乙状结肠神经元在人工神经网络中被高度采用。已知这些神经元提供平滑的实值输出，因此是所有输入的有界函数。与我们迄今为止看到的神经元类型不同，这些神经元使用逻辑函数。

众所周知，逻辑函数具有易于计算的导数，这使得学习变得容易。该导数值用于计算权重。以下是乙状结肠神经元输出的等式:

![Sigmoid neurons](img/B03980_11_44.jpg)![Sigmoid neurons](img/B03980_11_45.jpg)

图解/图形表示如下:

![Sigmoid neurons](img/B03980_11_14.jpg)

#### 随机二元神经元

随机二进制神经元使用与逻辑单元相同的等式，其中一个重要的区别是输出是针对概率值测量的，该概率值测量在短时间窗口内产生尖峰的概率。因此，等式看起来像这样:

![Stochastic binary neurons](img/B03980_11_46.jpg)![Stochastic binary neurons](img/B03980_11_47.jpg)

此外，该方程的图形表示为:

![Stochastic binary neurons](img/B03980_11_15.jpg)

总的来说，我们可以观察到的是，每个神经元接受一组输入的加权和，在这些输入上应用了非线性激活函数。校正线性函数通常用于解决回归问题，对于分类问题，应用逻辑函数。这种情况的一般表示如下:

![Stochastic binary neurons](img/B03980_11_16.jpg)

现在，这些输入可以被输入到一系列的神经元层中。让我们看看接下来会发生什么，以及这是如何发生的。输入层推送输入值；隐藏的神经元层将这些值作为输入。在这些隐藏层中可以有多个层，其中一层的输出作为输入提供给下一层。每一层都可以负责专门的学习。此外，最后，隐藏层中的最后一个输入到最终输出层。人工神经网络的典型结构如下图所示。下一张图中的每个圆圈代表一个神经元。**信用分配路径** ( **CAP** ) 的概念是指从输入到输出的路径。在前馈网络中，路径的长度是隐藏层和输出层的总数。下图显示了具有单个隐藏层和层间连接的前馈神经网络:

![Stochastic binary neurons](img/B03980_11_17.jpg)

这里显示了两个隐藏层的情况:

![Stochastic binary neurons](img/B03980_11_18.jpg)

### 神经网络规模

计算神经元的数量或参数如下所示:

*   For the single layer network:

    神经元总数= 4 + 2 = 6(不计算输入)

    总重量= [3 x 4] + [4 x 2] = 20

    对于 26 个可学习参数，总偏差= 4 + 2 = 6。

*   For the two layer network:

    神经元总数= 4 + 4 + 1 = 9(不计算输入)

    总重量= [3 x 4] + [4 x 4] + [4 x 1] = 12 + 16 + 4 = 32

    41 个可学习参数的总偏差= 4 + 4 + 1 = 9

那么，神经网络的最佳规模是多少呢？确定隐藏层的可能数量以及每层的大小非常重要。这些决定决定了网络的容量。更高的值有助于支持更高的容量。

让我们举一个例子，我们将通过获得以下分类器来尝试三种不同大小的隐藏层:

![Neural Network size](img/B03980_11_19.jpg)

显然，有了更多的神经元，可以表达更高复杂性的函数，这很好，但我们需要注意过度拟合的情况。因此，较小规模的网络适用于较简单的数据。随着数据复杂性的增加，需要更大的数据量。权衡总是在处理模型的复杂性和。过度拟合。深度学习解决了这个问题，因为它将复杂的模型应用于极其复杂的问题，并通过采取额外的措施来处理过度拟合。

#### 一个例子

接下来显示了使用多层感知器方法的面部识别情况:

![An example](img/B03980_11_20.jpg)

多个层将该图像作为输入，最后，创建并存储分类器定义。

![An example](img/B03980_11_21.jpg)

给定一张照片，每一层都专注于学习照片的特定部分，并最终存储输出像素。

关于权重和误差测量的一些重要注意事项如下:

*   训练数据是学习神经元权重的来源
*   误差度量或成本函数不同于回归和分类问题。对于分类，应用对数函数，对于回归，使用最小二乘法。
*   这些方法通过使用凸优化技术(如下降梯度法)更新权重，有助于控制这些误差测量

### 神经网络类型

在这一节中，我们将介绍一些关键类型的神经网络。下面的概念图列出了几种主要的神经网络类型:

![Neural network types](img/B03980_11_22.jpg)

#### 多层全连接前馈网络或多层感知器(MLP)

正如关于神经网络的介绍章节中提到的,一个 MLP 有多层，其中一层的输出作为后续层的输入。多层感知器如下图所示:

![Multilayer fully connected feedforward networks or Multilayer Perceptrons (MLP)](img/B03980_11_23.jpg)

#### 乔丹网络

乔丹网络是部分循环网络。这些网络是电流前馈网络，不同之处在于在输入层内具有额外的上下文神经元。这些上下文神经元是自我强加的，并使用来自输入神经元的直接反馈来创建。在 Jordon 网络中，上下文神经元的数量总是等于输入神经元的数量。下图描述了输入层中的差异:

![Jordan networks](img/B03980_11_48.jpg)

#### 埃尔曼网络公司

Elman 网络，正如 Jordon 网络一样，是部分递归前馈网络。这些网络也有上下文神经元，但在这种情况下，主要区别是上下文神经元从输出神经元而不是从隐藏层接收反馈。情境神经元和输入神经元的数量没有直接的相关性；相反，上下文神经元的数量与隐藏神经元的数量相同。反过来，这使得这个模型更加灵活，就像隐藏神经元的数量根据具体情况而定一样:

![Elman networks](img/B03980_11_49.jpg)

#### 径向偏置函数(RBF)网络

径向偏置函数网络也是前馈神经网络。这些网络有一个特殊的隐藏层，称为径向对称神经元。这些神经元用于使用高斯度量转换输入向量和中心之间的距离值。这个附加层的优点在于，它提供了确定所需层数的附加能力，而无需人工干预。线性函数的选择决定了最佳输出层。因此，即使与反向传播相比，在这些网络中学习发生得相对更快。

这种方法唯一的缺点是它处理大输入向量的能力。下图描绘了径向对称神经元的隐藏层。

![Radial Bias Function (RBF) networks](img/B03980_11_50.jpg)

#### 霍普菲尔德网络

霍普菲尔德网络围绕着一个叫做*网络能量*的概念工作。这仅仅是网络的一个最优局部极小值，它定义了功能的一个平衡状态。Hopfield 网络以达到这种平衡状态为目标。平衡状态是指一层的输出等于前一层的输出。下图描述了如何在 Hopfield 网络中检查和管理输入和输出状态:

![Hopfield networks](img/B03980_11_51.jpg)

#### 动态学习矢量量化(DLVQ)网络

**动态学习矢量量化** ( **DLVQ** )网络模型是神经网络的另一种变体，即从较少数量的隐藏层开始，并动态生成这些隐藏层。属于同一类的模式有相似性是很重要的；因此，这种算法最适合分类问题，如模式、数字等的识别。

#### 梯度下降法

在本节中，我们将关注优化神经网络、最小化成本函数、最小化误差并提高神经网络精度的最流行方法之一:梯度下降法。这里的图表显示了实际与。预测值以及预测不准确的情况:

![Gradient descent method](img/B03980_11_52.jpg)

## 反向传播算法

继续讨论训练网络的主题，梯度下降算法帮助神经网络学习权重和偏差。此外，为了计算成本函数的梯度，我们使用一种称为反向传播的算法。反向传播在 20 世纪 70 年代首次被讨论，直到 20 世纪 80 年代才变得更加突出。已经证明，当采用反向传播算法时，神经网络学习要快得多。

在本章的前几节中，我们看到了基于矩阵的算法是如何工作的；类似的符号用于反向传播算法。对于给定的权重 *w* 和偏差 *b* ，成本函数 c 有两个偏导数，分别是 *∂C/∂w* 和 *∂C/∂b* 。

这里陈述了关于反向传播的成本函数的一些关键假设。让我们假设成本函数由下面的等式定义:

![Backpropagation algorithm](img/B03980_11_53.jpg)

其中， *n* =训练样本数

*x* =单个训练集的总和

*y* = *y(x)* 为预期输出

*L* =神经网络的总层数

*a^L*=*a^L(x)*为输出激活向量

假设 1:总成本函数可以是单个成本函数的平均值。对于 *x* 单个训练集，成本函数现在可以表述如下:

![Backpropagation algorithm](img/B03980_11_54.jpg)

此外，单个训练集的成本函数可以如下:

![Backpropagation algorithm](img/B03980_11_55.jpg)

在这种假设下，由于我们可以计算每个训练集的偏导数 *x* 为∂ [x] C/∂w 和∂ [x] C/∂b，所以整体偏导数函数∂C/∂w 和∂C/∂b 可以是每个训练集的偏导数的平均值。

假设 2:该假设是关于成本函数 *C* 的，即 *C* 可以是神经网络输出的函数，如下所示:

![Backpropagation algorithm](img/B03980_11_56.jpg)

扩展之前的成本函数的等式，现在可以将每个训练样本集 *x* 的二次成本函数写成如下。我们可以看到这也是如何作为输出激活的函数的。

![Backpropagation algorithm](img/B03980_11_57.jpg)

反向传播是关于权重和偏差对总成本函数值的影响。

首先，我们计算第*l^(th)层中*j^(th)神经元的误差*δ^(l[j]，然后用这个值计算与这个误差*δ^lj相关的偏导数:*)***

![Backpropagation algorithm](img/B03980_11_58.jpg)

第 *l ^(th)* 层中第*j*神经元的误差函数*δl[j]*可以定义为:

![Backpropagation algorithm](img/B03980_11_59.jpg)

因此，也可以计算层*L*δ^L的误差。这反过来有助于计算成本函数的梯度。

反向传播算法依次使用以下等式，如下所示:

等式 1:给定位置 *j* 的神经元，层 *L* 、*δ^L的计算误差。*

![Backpropagation algorithm](img/B03980_11_60.jpg)

等式 2:给定下一层 *δ ^(L+1)* 的误差，计算上一层 *L* ， *δ ^L* 的误差。

![Backpropagation algorithm](img/B03980_11_61.jpg)

### 注意

哈达玛乘积是一种矩阵乘法技术，用于按元素进行矩阵乘法，如下所示:

![Backpropagation algorithm](img/B03980_11_62.jpg)

符号![Backpropagation algorithm](img/B03980_11_70.jpg)用于表示该方法。

等式 3:该等式衡量对成本的影响，并给出偏差的变化:

![Backpropagation algorithm](img/B03980_11_34.jpg)

此外，我们将从等式 1 和 2 得到以下内容:

![Backpropagation algorithm](img/B03980_11_35.jpg)

这是因为误差值与偏导数的变化率相同。

等式 4:这个等式用于计算与重量相关的成本变化率。

![Backpropagation algorithm](img/B03980_11_36.jpg)

在这些算法的每一个阶段，都有某种影响网络整体输出的学习。

编译后的最终反向传播算法解释如下:

1.  输入层 *x* ，对于 *x =1* 设置激活为*a¹。*
2.  对于其他层的每一层 *L = 2，3，4 …L* ，计算激活次数为:![Backpropagation algorithm](img/B03980_11_37.jpg)
3.  使用等式 1 和等式 2 计算误差*δ^L。*
4.  使用等式 3 反向传播 *l = L-1，L-2，… 2* ，1 的误差。
5.  最后，使用等式 4 计算成本函数的梯度。

如果我们观察算法，误差向量*δ^l从输出层开始向后计算。事实上，成本是网络输出的函数。要了解早期权重对成本的影响，需要应用一个链规则，该规则在所有层中反向工作。*

## Softmax 回归技术

Softmax 回归也称为多项式逻辑回归。本节不深入讨论逻辑回归的概念，因为它在本书中与回归相关的章节中已经讨论过了。相反，我们将特别关注如何理解这项技术在深度学习用例中的数字识别相关问题中的应用。

这种技术是适用于多个类的逻辑回归的特例。我们知道，逻辑回归的结果是一个二进制值 *{0，1}* 。Softmax 回归便于处理 *y(i) < - {1，…，n}* ，其中 *n* 是针对二元分类的类的数量。在 MNIST 数字识别的情况下，值 *n* 是 10，代表 10 个不同的类别。例如，在 MNIST 数字识别任务中，我们会有 *K=10* 个不同的类。

由于其处理多个类的能力，这种技术被积极地用于基于神经网络的问题解决领域。



# 深度学习分类法

这里描述了深度学习案例的特征学习分类:

![Deep learning taxonomy](img/B03980_11_63.jpg)

下面列出了一些用于实现神经网络应用的框架:

*   Theano 是一个 Python 库
*   火炬 Lua 编程语言
*   Deeplearning4J 是一个基于 Java 的开源框架，可以与 Spark 和 Hadoop 协同工作
*   Caffe 是一个基于 C++的框架

## 卷积神经网络(CNN/ConvNets)

CNN，也被称为卷积网络(ConvNets)，是常规神经网络的一种变体。

让我们回顾一下常规神经网络的功能。常规神经网络具有单个基于向量的输入，该输入通过一系列隐藏层进行转换，其中每层中的神经元与其相邻层中的神经元相连接。这个系列的最后一层提供了输出。这一层称为输出层。

当神经网络的输入是图像，并且不仅仅适合单一向量结构时，复杂性就会增加。CNN 有这种微小的变化，其中输入被假设为具有深度(D)、高度(H)和宽度(W)的三维向量。这种假设改变了神经网络的组织方式和运行方式。下图比较了标准三层神经网络和 CNN。

![Convolutional neural networks (CNN/ConvNets)](img/B03980_11_38.jpg)

正如我们看到的，前面显示的卷积网络以三维方式排列神经元；网络中的每一层都将其转化为神经元激活的 3D 输出。

卷积网络体系结构包括为特定功能指定的一组固定层。最关键的层如下:

*   **卷积层** ( **CONV** )
*   **池层** ( **池**)
*   **全连通** ( **FC** )层

在某些情况下，激活函数被写成另一层(RELU)；可能存在用于 FC 层转换的不同的标准化层。

### 卷积层(CONV)

卷积层构成了卷积网络的核心。这一层负责以三维格式保存神经元，因此负责三维输出。以下是尺寸为 32 x 32 x 3 的输入体积的示例。如图所示，每个神经元连接到一个特定的输入区域。沿着深度，可以有很多神经元；在这个例子中，我们可以看到五个神经元。

![Convolutional layer (CONV)](img/B03980_11_39.jpg)

下图显示了网络卷积函数在神经元函数表示中的工作方式:

![Convolutional layer (CONV)](img/B03980_11_40.jpg)

也就是说，神经元的核心函数保持不变，负责计算权重和输入的乘积，然后观察非线性行为。唯一的区别是对本地区域连接的限制。

### 池层(POOL)

可以有多个卷积层，并且在这些卷积层之间，可以有一个汇集层。池层负责通过减小输入体积的空间大小来减少过度拟合的机会。空间大小的减小意味着减少网络中的参数数量或计算量。最大值函数有助于减小空间大小。池层使用最大值函数，并将其应用于三维表示中的每个切片，按深度方向切片。通常，池层沿宽度和高度应用大小为 2 X 2 的过滤器。这可能会丢弃大约 75%的激活。

总体而言，池层具有以下特征:

*   始终将 W1×H1×D1 的卷大小视为输入
*   Applies stride S and spatial extent F and generates the W2×H2×D2 output where:

    w2 =(W1 F)/S+1

    H2 =(H1 F)/S+1

    D2=D1

### 全连接层(FC)

完全连接层非常类似于常规或传统的神经网络，负责建立与前一层激活的广泛连接。使用矩阵乘法技术计算连接激活。关于这一点的更多细节可以在本章前面的章节中找到。

## 循环神经网络

rnn 是神经网络的一个特例，众所周知，它在记忆信息方面非常有效，因为隐藏状态是以分布式方式存储的，因此它可以保存更多关于体验的信息。这些网络还应用非线性函数来更新隐藏状态。下图描述了 RNNs 中隐藏状态的链接方式:

![Recurrent Neural Networks (RNNs)](img/B03980_11_64.jpg)

在大多数真实世界的例子中，输入和输出并不是相互独立的。例如，如果我们必须预测下一个单词，了解在它之前的单词对我们来说是很重要的。顾名思义，“递归”神经网络反复执行同一任务，其中一次执行的输入是前一次执行的输出。通常，已知 rnn 仅返回到过去的几个步骤，并不总是通过所有迭代。下图描述了 rnn 的工作方式；它展示了 rnn 如何在迭代中展开:

![Recurrent Neural Networks (RNNs)](img/B03980_11_65.jpg)

在前面的示例中，要求预测下一个单词，如果输入中有五个单词，那么 RNN 展开到五层。

## 受限玻尔兹曼机器(RBMs)

RBMs 的出现是为了解决 RNNs 训练中的难点。的出现使得处理这些训练困难的受限递归模型简化了问题背景，此外，学习算法也被应用于解决问题。Hopfield 神经网络是解决前述问题的受限模型的一个例子。

作为第一步，玻尔兹曼机器出现了。这些模型是带有随机元素的 Hopfield 神经网络的特例。在这种情况下，神经元分为两类:一类导致可见状态，另一类导致隐藏状态。这也类似于隐马尔可夫模型。RBM 又是玻尔兹曼机器的一个特例，不同之处主要在于同一层神经元之间没有连接。因此，对于一组神经元的给定状态，另一组神经元的状态是独立的。下图描述了典型的 RBN 结构和前面的定义:

![Restricted Boltzmann Machines (RBMs)](img/B03980_11_66.jpg)

把这个定义进一步做更深层次的解释，神经元的一些可见状态是可观测的，还有神经元的隐藏状态是不可见的或者不能直接看到的。基于可用的可见状态，对隐藏状态做出一些概率性的结论，这就是训练模型是如何形成的。

在 RBM 中，连通性受到限制，这反过来又简化了推理和学习。通常只需一步就能达到可见态被箝位的平衡态。假设提供了关于可见状态的信息，下面的公式显示了如何计算隐藏状态的概率:

![Restricted Boltzmann Machines (RBMs)](img/B03980_11_67.jpg)

## 深度玻尔兹曼机器(DBMs)

DBMs 是传统 Boltzmann 机器的一个特例，它有许多丢失的连接，并且与顺序随机更新不同，允许并行更新以确保模型的效率。

DBMs 限制隐藏变量之间的联系，并且主要使用未标记的数据来训练模型。标记的数据用于微调模型。下图描述了三层 DBM 的一般结构:

![Deep Boltzmann Machines (DBMs)](img/B03980_11_68.jpg)

## 自编码器

在了解自编码器之前，我们先来了解一下 **自动关联器** ( **AAs** )。AAs 的目标是尽可能精确地接收输入。

AA 的目的是尽可能精确地接收作为输入图像的输出。原子吸收光谱分为两类:一是生成原子吸收光谱，二是合成原子吸收光谱。前一节中介绍的 RBM 被归类为生成 AAs，而自编码器合成 AAs。

自编码器是一种具有单一开放层的神经网络。应用反向传播和无监督学习技术，自编码器从假设目标值等于输入值开始， *y = x* 。下图描述了学习函数 *h [W，b] (x) ≈ x* 的自编码器:

![Autoencoders](img/B03980_11_69.jpg)

中间的层是开放的，如前面的图所示，为了获得最佳输出，该层的神经元数量必须少于输入层。该模型的目标是学习一个恒等式函数的近似值，使得第**层 L[3]17】的值等于第**层 L[1]T21 的值。****

数据在通过输入层传递到输出层时会被压缩。当某个像素的图像，比如说 100 个像素(10 X 10 个像素)，被输入到具有 50 个神经元的隐藏层的模型中时，预期是网络试图通过保持像素配置不变来压缩图像。这种压缩只有在存在可以减少输入数据的隐藏互连和其他特征相关性的情况下才是可能的。

自编码器的另一种变体是 **去噪自编码器** ( **DA** )。 autoencoder 的这一变体的不同之处在于其额外的功能，即恢复和还原受损坏的输入数据影响的状态。



# 实现人工神经网络和深度学习方法

参考本章提供的源代码来实现人工神经网络和本章涉及的其他深度学习方法(技术的每个文件夹下的源代码路径`.../chapter11/...`)。

## 使用 Mahout

参见文件夹`.../mahout/chapter11/annexample/`。

参见文件夹`.../mahout/chapter11/dlexample/`。

## 使用 R

参见文件夹`.../r/chapter11/annexample/`。

将参考到文件夹`.../r/chapter11/dlexample/`中。

## 使用火花

参考到文件夹`.../spark/chapter11/annexample/`。

参考到文件夹`.../spark/chapter11/dlexample/`。

## 使用 Python (Scikit-learn)

参考至文件夹T2。

参见文件夹`.../python-scikit-learn/chapter11/dlexample/`。

## 使用朱丽亚

参考到文件夹`.../julia/chapter11/annexample/`。

参考到文件夹`.../julia/chapter11/dlexample/`。



# 总结

在这一章中，我们讨论了生物神经元的模型以及人工神经元与其功能的关系。您学习了神经网络的核心概念，以及完全连接的层是如何工作的。我们还探讨了一些与矩阵乘法结合使用的关键激活函数。