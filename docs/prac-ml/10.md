<title>Chapter 10. Regression based learning</title>

# 第十章。回归学习

回归分析允许我们使用简单的代数来数学建模两个变量之间的关系。在这一章中，我们将重点介绍另一种监督学习技术:回归分析或基于回归的学习。在前一章中，我们介绍了将在本章中用到的统计学基础知识。我们将首先了解多个变量如何影响结果，以及如何使用统计调整技术来仲裁这种影响，了解使用真实世界示例的相关性和回归分析，并深入探讨混杂和效应修正等概念。

您将学习这种技术的基本和高级概念，并获得使用 Apache Mahout、R、Julia、Apache Spark 和 Python 进行简单的多元线性回归、多项式回归和逻辑回归的实践指导。

在本章结束时，读者将理解回归模型的用途和局限性，学会如何将线性和逻辑回归模型拟合到数据，对结果进行统计推断，并最终评估和诊断模型的性能。

下图描述了本书中涵盖的不同学习模型，用橙色突出显示的技术将在本章中详细讨论:

![Regression based learning](img/B03980_10_01.jpg)

本章将深入介绍此处列出的主题:

*   相关和回归分析导论；修订其他统计概念，如协方差和相关系数。我们将在回归模型、方差分析模型和诊断的上下文中涵盖期望、方差和协方差的属性
*   您将学习简单的、线性的和多重线性回归:线性关系、线性模型、基本假设(正态性、同方差性、线性和独立性)和最小二乘估计。总的来说，你将学习模型诊断和选择。
*   你将看到广义线性模型(GLMs)的概述和 GLM 回归算法的列表。此外，混淆和效果修改的现象将被提出，因此实现和调整。
*   将涵盖逻辑回归的介绍、优势和风险比的理解、逻辑回归模型的建模和评估。
*   还将介绍使用 Apache Mahout、R、Apache Spark、Julia 和 Python (scikit-learn)库和模块的示例实现。

# 回归分析

在监督学习技术下，在统计方法下分类的学习模型是基于实例的学习方法、贝叶斯学习方法和回归分析。在本章中，我们将重点介绍回归分析和其他相关的回归模型。众所周知，回归分析是最重要的统计技术之一。如上所述，它是一种统计方法，用于测量两个或多个变量之间的关系，并检查其关系的有效性和强度。

传统上，研究人员、分析师和交易员一直使用回归分析来建立交易策略，以了解投资组合中包含的风险。回归方法用于解决分类和预测问题。

我们已经在前面的章节中介绍了一些关键的统计概念；在这一章中，我们将涵盖更多与回归分析相关的概念。举几个概念来说，我们有可变性、线性、协方差、系数、标准误差等的测量。

![Regression analysis](img/B03980_10_02.jpg)

## 重温统计数据

在前面的章节中，我们学习了贝叶斯学习方法，我们涵盖了一些核心统计指标，如均值、中值、众数和标准差。现在让我们将它扩展到更多的度量，如方差、协方差、相关性以及随机变量分布的一阶和二阶矩。

![Revisiting statistics](img/B03980_10_03.jpg)

**方差** 是标准差的平方。如果你还记得什么是标准差，它是样本中每个测量值偏离平均值的平均值。它也被称为均值的标准差。我们可以从理论上计算众数和中位数的标准差。

**范围**被定义为数据集分布的数值范围。范围通常表示为最小值和最大值。

四分位数、十分位数和百分位数细分了类似于中位数的测量分布。众所周知，中位数将分布分成两半，而四分位、十分位和百分位分别将分布分成 1/25、1/10 和 1/100。

**第一个四分位数**(指定为 Q1)或更低的四分位数是第 25 个百分位数。

**第三个四分位数**(指定为 Q3)或上四分位数是第 75 个百分位数。

四分位数间距=第三个四分位数–第一个四分位数

**对称**和**偏斜数据**:对称、正偏斜和负偏斜数据的中值、均值和模式如下所示:

![Revisiting statistics](img/B03980_10_04.jpg)

### 注

对称分布具有相等的平均值和中值。对于正偏态分布，中值大于平均值，对于负偏态分布，平均值大于中值。

离群值是从数据的主聚类中分离出来的观察值。它会对均值等指标产生重大影响。让我们举个例子来更好地理解这一点。我们想了解一个五人小组的平均财富。比方说，单个资产的价值分别为 100 万美元、120 万美元、90 万美元、110 万美元和 1200 万美元。

1+1.2+0.9+1.1+12=16.2

16.2/5=3.24

最后一个观察结果对测量有不切实际的影响。现在让我们看看中位数是如何受到影响的。我们把资产按升序排序:0.9M，1.0M，1.1M，1.2M，12M。中位数是 1.1M，有两个重要的概念我们必须理解。离群值对平均值的影响比中位数更显著。

所以，在选择正确的统计量之前，你应该仔细检查数据。

Mean 表示变量的平均值，median 表示平均变量的值。

**协方差** 是当有两个或两个以上的感兴趣的变量(如公司的股票、材料的物理性质等。);了解它们之间是否有任何联系变得很重要。准确地说，我们想了解的是，如果其中一个变量在变化，另一个变量是如何变化的。

在统计学中，有两个术语可以解释这种现象:

第一个被称为协方差。对于包含两个变量 *x* 和 *y* 的 *n* 个点的数据集，以下等式描述了协方差的计算:

![Revisiting statistics](img/B03980_10_05.jpg)

然而，协方差可能是一个非常大的数字。最好将其表示为-1 和 1 之间的归一化数字，以便理解数量之间的关系。这通过用两个变量的标准偏差对协方差进行归一化来实现( *sx* 和 *sy* )。

这就是 *x* 和 *y* 之间的**相关系数** 。

![Revisiting statistics](img/B03980_10_06.jpg)

**相关性** 衡量X 和 *Y* 之间的线性相关强度，位于-1 和 1 之间。下图直观地说明了相关性如何影响线性相关性:

![Revisiting statistics](img/B03980_10_07.jpg)

在我们进入各种回归模型的细节之前，让我们先看看实现回归模型和分析结果的步骤。

均值和方差是随机变量概率分布函数的*一阶矩和*二阶矩。它们的计算方法如下:

![Revisiting statistics](img/B03980_10_08.jpg)

在计算了给定随机变量的概率分布后，我们将通过简单的积分来计算平均方差。

让我们用一个真实的例子来计算所有这些度量。

以下是三家公司(A 公司、B 公司和 C 公司)在 14 天内的股价数据。首先，使用下面的公式计算回报:

退货=(当日价格-昨日价格)/昨日价格

从这个回报，计算平均值，中位数，成对相关。不要使用内置库。即使使用 Excel，也要使用基本公式。

![Revisiting statistics](img/B03980_10_09.jpg)

首先，让我们用前面给出的公式来计算回报。

![Revisiting statistics](img/B03980_10_10.jpg)

如果我们必须计算平均值,值将如下:

![Revisiting statistics](img/B03980_10_11.jpg)

为了找到中间值，我们将首先对返回值进行升序排序，然后标记中间值。

![Revisiting statistics](img/B03980_10_12.jpg)

最后，让我们用前面协方差部分给出的公式计算协方差和相关性。

![Revisiting statistics](img/B03980_10_13.jpg)

### 期望、方差和协方差的性质

让我们结合对上一章和当前章节的理解，对它们进行总结。

变量的这种分布是取某一特定值的概率。期望值是总体平均值(即加权平均值的概率)。

我们可以定义均值的方差和标准差。

最后，如果我们在看两个不同的变量，我们可以定义协方差和相关性。现在，让我们了解如何计算两组的期望和方差。这在下一节中特别有用，我们将一起分析两个变量进行线性回归，如下所示:

*E(x+y) = E(x) + E(y)*

*E(x+a) = E(x) + E(a) = a + E(x)*

*E(kx) = kE(x)*

这里有一个非常有趣的规则:

![Properties of expectation, variance, and covariance](img/B03980_10_49.jpg)

本质上，这个规则说，如果我们有一个给定分数的资产组合，那么总期望就是单个期望的加权和。这是分析组合中的一个重要概念。如果有一个投资组合，其中 30%是 A 公司的股票，50%是 B 公司的股票，20%是 C 公司的股票，那么我们投资组合的预期收益是:

E(投资组合)= 0.3 E(公司 A) + 0.5 E(公司 A) + 0.2 E(公司 A)

#### 方差的性质

给定 *X* ，一个随机变量:

*Var(X+Y)= Var(X)+Var(Y)+2Cov(X，Y)*

*V(x+a) = V(x)* (加入常数后方差不变)

*V(ax) = a2 V(x)*

让我们证明这一点，因为它并不明显:

说， *Y= aX*

*E(Y) = an E(X)* (来自上一组关系)

*Y-E(Y) = a(X-E(X))*

摆平双方并接受期望:

*E(Y-E(Y))²= a²E(X-E(X))²*

然而，左边是 *Y，*的方差，右边是 *X* 的方差:

*Var(Y)= a²Var(X)*

从上面可以推导出方差的另一对有趣的性质。由此直接得出结论

*Var (-y) = Var (y)*

现在让我们看看投资组合的方差:

![Properties of variance](img/B03980_10_50.jpg)

因此，如果你有一个由三只股票组成的投资组合，你的投资组合的方差(或者标准差的平方根)如前所示变化。标准差通常被称为投资组合的风险。理想情况下，它需要尽可能低。根据前面的公式，这可以通过两种方式实现:

1.  通过选择方差非常低的元素
2.  通过选择协方差非常负的元素

这是成功投资的重要途径。

#### 协方差的性质

以下是协方差的属性:

*cov(X，Y)= E[XY]—E[X]E[Y]*

*cov(x，a) = 0*

*cov(x，x) = var(x)*

*cov(y，x) = cov(x，y)*

*cov(ax，by) =abcov(x，y)*

*cov(X+a，Y+b) = cov (X，Y)*

*cov(aX+bY，cW+dV) = accov(X，W) + adcov(X，V) + bccov(Y，W) + bdcov(Y，V)*

*Cor(X，Y) = E[XY]/σXσY*

现在让我们用一个真实的例子来看看这一点。

#### 例子

你的两个最好的朋友，安娜和丹尼尔，正计划投资股票市场。由于你是朋友圈里最有经验的投资者，他们向你寻求建议。你知道丹尼尔可以承受 10%的风险，而安娜想要最小的风险。你显然想让两者的回报最大化。他们都想投资三个项目:黄金债券、顶级 IT 公司和顶级银行。

![Example](img/B03980_10_14.jpg)

SD—标准偏差

相关性可以计算如下:

![Example](img/B03980_10_15.jpg)

现在，让我们系统地推导建议。

让我们首先为三项资产创建一个所有可能权重的列表(假设您需要计算到小数点后一位)。大约有 66 个可能的值。这意味着我们的朋友必须从这些选择中挑选一个进行投资。现在，用下面的公式计算每个可能的投资组合(一个独特的权重组合)的回报(还是用你喜欢的语言):

*投资组合回报= W*[g]*X R*[g]*+W*[I]*X R*[I]*+W*+b*X R*[b]*W*[g]

*W* [i] *，W* [b] *=权重*和

*R* [i] *，R* [g] *，R* [b] *=返回*

这是因为投资组合的预期是单个投资组合的预期总和乘以单个权重。

前五个投资组合的值是:

![Example](img/B03980_10_16.jpg)

计算所有其他值。

使用以下公式计算每个投资组合的风险:

return = Sqrt((Wg * SDG)²+(Wi * SDI)²+(Wb * sdb)2+(2 * Wg * SDG * Wi * SDI * rgi)+(2 * Wi * SDI * Wb * sdb * rib)+(2 * Wb * sdb * Wg * SDG * rbg))

*sdg，sdb，sdi =风险，rij =和 j 的相关性*

这与前面一节中给出的投资组合方差公式完全相同。

![Example](img/B03980_10_17.jpg)

现在让我们计算所有其他的值。

现在，所有需要做的就是为安娜和丹尼尔推荐平衡的投资组合，因为他们的风险偏好你已经知道了。由于 Ana 倾向于零风险，我们将选择对应于 17.2%回报和 0.87 风险的点。可以查表确认这是用 0.7，0.2，0.1(黄金，IT，银行)的投资组合得到的。由于丹尼尔可以承担 10%的风险，我们将看到对应于 10%风险的投资组合，它具有最高的回报。

同样，这可以理解为 0.2、0.7 和 0.1。

### 方差分析和 F 统计

在双变量和多变量分布的情况下，需要理解的一个很好的量是人口或群体内以及人口或群体之间方差的分布方式。这是将数据分组为多个子集的过程。正如你可以清楚地看到的，在这种情况下，知道它们之间的差异是如何分布的真的很有帮助。这样的分析称为 **ANOVA** ( **方差分析**)。所涉及的计算相当简单。

让我们取三个样本，它们有自己的平均值和分布，如下所示:

![ANOVA and F Statistics](img/B03980_10_18.jpg)

就示例而言，请参见以下内容:

*样本 1= {3，2，1}*

*样本 2= {5，3，4}*

*样本 3= {5，6，7}*

*样本 1 = 2 的平均值*

*样本 2 = 4 的平均值*

*样品 3 = 6 的平均值*

*总体宏大平均数= (3+2+1+5+3+4+5+6+7) / 9 = 4*

大平均值(如果这些组覆盖了整个人口，则为人口平均值)等于平均值。

有没有可能这三种手段来自同一人群？如果一个平均值与其他的非常不同或者相差很远，这是否意味着他们不是来自同一个人群？还是说他们相隔的距离相等？

所有之前的样本都是关于与大平均值的相对距离度量。

![ANOVA and F Statistics](img/B03980_10_19.jpg)

现在让我们计算整个样本集的平方和:

*(3-4)²+(2-4)²+…= 30*

我们可以通过将前面提到的量除以自由度 *(n*m-1)* 来计算方差:

*n*——每个样品中元素的数量

*m*——样本数量

我们试图建立的属性不会改变。因此，让我们坚持使用平方和而不是方差。现在，让我们计算两个量:组和组间的平方和。

*   **组的平方和**:我们取均值为 2 的第一组(3，2，1)。变异(我们不称之为方差。但是，它绝对是组内方差的度量)等于 *(3-2)2+…=2* 。类似地，组 2 和组 3 内的变化等于 *2* 和 *2* 。因此，组内贡献的总变化是 6。每组内自由度的总数为 *n-1* 。总自由度为 *(n-1)*m* 。这种情况下是 6。
*   **组之间的平方和**:这是组的平均值和大平均值之间的距离，乘以组 1 的组平均值为 2，大平均值为 4。所以，这组相对于大平均值的变化量是 *(2-4)2 * 3 = 12* 。第二组的变化为 0，第三组的变化为 12。所以两组之间的差异是 24。在这种情况下，自由度是 *m-1 = 2* 。

所以，让我们来记录一下:

![ANOVA and F Statistics](img/B03980_10_20.jpg)

因此，我们看到在 30 个总变异中，6 个是组内变异，24 个是组间变异。因此，最有可能的是，将它们分开分组是有意义的。现在，让我们做一些推断统计。我们假设前面的数值是三个教练中心得出的排名。我们想知道把人们放在教练中心实际上是否对他们的最终排名有影响。

让我们从一个假设的论点开始。

**零假设**是教练中心对排名没有影响。替代教练中心确实对排名有影响。

![ANOVA and F Statistics](img/B03980_10_21.jpg)

如果我们观察，这种方法不是关于值是否相等，而是检查样本是否来自同一个更大的总体。这种方法被称为样本均值之间的变异性。

简而言之，ANOVA 是一个变异率，表示如下:

ANOVA =之间的方差/内部方差=总体平均值/内部分布之间的距离

总差异=之间的差异+内部的差异

将总方差分成两部分的过程称为分割:

*   如果均值间方差>均值内方差，则意味着变异率> 1。因此，我们可以得出结论，这些样本不属于同一个群体。
*   如果均值之间的方差和均值内的方差相似，那么比率几乎变成 1，这将表示重叠。
*   如果方差的意思是< the variance with the means, it will mean that the samples are close to the overall mean or the distributions *融化*在一起。

因此，正如我们在处理多个变量时所看到的，可能有许多因素会影响结果。需要评估这些变量中的每一个对变量之间关系的独立影响。在下一节中，两个概念，**混杂** 和 **效应修正**，将解释对结果的不同类型的影响因素。

## 混杂

我们将通过一个例子开始理解什么是混淆。让我们假设我们正在做一项研究，我们想确定患心脏病的风险是否与吸烟有关。当对吸烟者和非吸烟者以及那些在一段时间内被检测出患有心脏病的人的样本数据进行研究时，进行了一项相关性测量，如*风险比*，发现其为 2.0。这可以解释为吸烟者患心脏病的风险是不吸烟者的两倍。现在，当我们仔细观察数据时，假设我们发现吸烟者和不吸烟者的年龄分布不一样，结果是样本中吸烟者的年龄比不吸烟者的年龄高得多。如果我们必须将这些信息联系起来，那么患心脏病的后果是与年老、吸烟还是两者都有关系呢？

测量吸烟对心脏病发展的定量影响的一种理想方法是，对人们进行抽样，观察他们在一段时间内吸烟的情况，收集心脏病发展的数据，使用同一组人，并及时回到他们不吸烟时进行相同的评估。这将有助于衡量反事实的结果。同一组人代表吸烟者和不吸烟者。因为这是不可能的，我们需要假设有*可交换性*。不吸烟者描述吸烟者，如果他们曾经吸烟，反之亦然。换句话说，这意味着两组在所有方面都是可比较的。在数据样本不可比的情况下，这种情况被称为混杂，造成这种情况的属性(在这种情况下，年龄)被称为 **混杂因素**。如果我们必须用一个例子来解释这一点，即所有的非吸烟者都更年轻，非吸烟者会低估老年吸烟者不吸烟的结果。

这种情况可以表示如下:

![Confounding](img/B03980_10_22.jpg)

我们观察到的是有一条后门路径(通过年龄属性)。混杂因此可以用更简单的术语来定义，即*后门途径的存在*。

### 注意

混杂是指暴露和结果之间的影响或关联被另一个变量的存在所扭曲。

## 效果修改

效应修正是指不同组的曝光值不同的情况。当关联估计的测量值，如优势比、比率比和风险比值，非常接近于来自关联的特定群体估计的加权平均值时，可以观察到这种情况。

效果修正因子是有差别地(这可以是正面的或负面的)修正观察到的对结果的影响的变量。

让我们看一个例子。乳腺癌可以发生在男性和女性身上；男女都有这一比率，但女性的比率是男性的 800 倍，性别因素是一个明显的区别因素。

如果效应修正因子没有被正确识别，这可能会导致不正确的粗略估计，并导致错过了解风险因素和结果之间关系的机会。

需要遵循以下步骤来研究用于分析数据的效果修改:

1.  收集关于潜在效果修改的信息。
2.  研究效果修改器的效果，测量差异，并暂停匹配值。
3.  根据潜在影响修正值对数据进行分层，并计算风险对结果的影响估计值。确定是否存在效果修改。如果是这样，可以提供/使用估计值。

回顾一下，混杂因素掩盖了真实的影响，而影响修饰语意味着不同的群体有不同的影响。

<title>Regression methods</title>

# 回归方法

正如我们所知，回归允许我们对两个或更多变量之间的关系进行建模，尤其是当基于几个自变量预测一个连续的因变量时。回归中使用的自变量可以是连续的，也可以是二分的。在因变量为二分变量的情况下，应用逻辑回归。在因变量的两个水平之间的分裂相等的情况下，那么线性回归和逻辑回归将得到相同的结果。

![Regression methods](img/B03980_10_23.jpg)

回归假设(大多数适用于线性回归模型族)

*   **样本案例大小**:为了应用回归模型，案例与独立变量(**IV**)的比例理想情况下应为 20:1(对于模型中的每个 IV，需要有 20 个案例)，最小为 5:1(模型中的每个 IV 有 5 个案例)。
*   **数据准确性**:回归假设数据的基本有效性，在运行回归方法之前需要运行基本的数据验证。例如，如果一个变量的值可以在 1-5 之间，则任何不在该范围内的值都需要被校正。
*   异常值:正如我们所知，异常值是那些通常有极值的数据点，它们自然不会出现在总体中。回归假设处理异常值。
*   **缺失数据**:重要的是寻找缺失数据并解决。如果一个特定的变量有许多缺失值，除非有太多的变量有许多缺失值，否则最好消除该变量。一旦回归过程运行，没有值的变量可以作为排除的候选。为了避免因剔除而丢失数据的风险，需要应用缺失值技术
*   **正态分布**:需要对数据进行检查，确保你的数据是正态分布的。在直方图上绘制数据是检查数据是否正态分布的一种方式。下面的直方图是正态分布的一个例子:![Regression methods](img/B03980_10_24.jpg)
*   **线性行为**:线性行为简单来说就是在因变量和自变量之间看到一条直线关系。IV 和 DV 之间的任何非线性关系都被忽略。双变量散点图用于测试线性。
*   **同方差**:同方差是指因变量的变化对自变量的恒定变化。下面的散点图是数据均方的一个例子，我们可以看到图集中在中心:![Regression methods](img/B03980_10_25.jpg)

与线性假设相似，违反同方差假设不会使回归无效，但会削弱回归。

*   **多重共线性和奇异性**:多重共线性是自变量高度相关的情况。在奇点的情况下，独立变量是完全相关的，通常，一个 IV 是一个或多个其他 IV 的组合。多重共线性和奇异性都可以使用 iv 之间的相关性轻松识别。

从下一节开始，我们将深入讨论概念图中列出的每种回归方法:

![Regression methods](img/B03980_10_26.jpg)

## 简单回归或简单线性回归

在这种情况下，我们将只使用两个变量；一个因变量和另一个自变量。简单的线性回归就是比较两个模型；一种是没有自变量，使用因变量形成最佳拟合线，另一种是使用最佳拟合回归线。现在让我们看一个例子来理解最佳拟合线和回归线的定义。

我们将从一个真实世界的例子开始。让我们假设有一个房地产交易商，他每做一笔房地产交易，就会得到一笔佣金。很明显，佣金的多少取决于交易的价值；交易额越高，佣金越高。所以在这种情况下，佣金成为因变量，交易金额成为自变量。为了预测下一笔佣金金额，让我们考虑最近六笔交易的样本数据，如下所示:

![Simple regression or simple linear regression](img/B03980_10_27.jpg)

让我们假设我们没有交易总额的数据。如果我们要预测前面数据中的给出的下一个佣金，我们首先在图表上绘制,如下所示:

![Simple regression or simple linear regression](img/B03980_10_28.jpg)

我们必须确定数据中给出的下一个佣金金额的选项之一是计算平均值，这是对样本的最佳预测。

![Simple regression or simple linear regression](img/B03980_10_29.jpg)

让我们在图上画出这个点，这将成为*最佳*拟合。在上一个图表上绘制平均值值:

![Simple regression or simple linear regression](img/B03980_10_30.jpg)

计算每个点与平均值的距离，得到下图所示的值。这种距离度量被称为误差或残差。所有点的误差总和总是为零，这是拟合优度的度量。

![Simple regression or simple linear regression](img/B03980_10_31.jpg)

在图上绘制距离。

![Simple regression or simple linear regression](img/B03980_10_32.jpg)

我们在前面的章节中已经了解了 **SSE** ( **误差平方和** ) 值。误差是平方的，因为它使值为正，也强调了较大的偏差。下表显示了为样本数据计算的 SSE 值:

![Simple regression or simple linear regression](img/B03980_10_33.jpg)

简单线性回归的总体目标是建立一个模型，最大程度地减少 SSE。到目前为止，我们已经看到了使用单个变量的最佳拟合，其中是因变量。现在，假设我们得到了例子中另一个自变量的数据。事实上，这为我们得到了一条新的回归线，它不同于我们之前得到的最佳拟合线。预计新的自变量将显著降低上证综指的价值。换句话说，这条新的回归线应该更适合给定的数据。

如果早期的最佳拟合线和回归线没有差异，这将意味着所识别的自变量对结果没有影响。总体而言，简单线性回归旨在使用 SSE 值最少的数据找到最佳拟合线。

现在让我们将自变量数据添加到我们的分析中——房地产交易价值，如下表所示:

![Simple regression or simple linear regression](img/B03980_10_34.jpg)

我们将绘制因变量和自变量之间的散点图。

![Simple regression or simple linear regression](img/B03980_10_35.jpg)

在这种情况下，可能会有多条线/方程式，如下一张图表中的所示。如果数据似乎符合要求，我们可以继续。如果数据点分散在各处，则表明数据没有线性，我们可以选择停止推导回归线。我们可以选择如下计算相关系数:

r = 0.866

这表明两个变量之间的关系很强，我们可以着手建立回归模型。

![Simple regression or simple linear regression](img/B03980_10_36.jpg)

现在我们来计算对于 *x* 和 *y* 轴的平均值；以下是这些值:

![Simple regression or simple linear regression](img/B03980_10_37.jpg)

然后将这些平均值绘制为分散标度法图上的质心，如下所示:

![Simple regression or simple linear regression](img/B03980_10_38.jpg)

最佳拟合的回归线必须通过质心，该质心包括*x*和 *y* 变量的平均值。计算如下:

![Simple regression or simple linear regression](img/B03980_10_51.jpg)![Simple regression or simple linear regression](img/B03980_10_52.jpg)![Simple regression or simple linear regression](img/B03980_10_53.jpg)![Simple regression or simple linear regression](img/B03980_10_39.jpg)

最终的回归线方程如下所示:

![Simple regression or simple linear regression](img/B03980_10_40.jpg)

在上绘制之前的式散点图如下:

![Simple regression or simple linear regression](img/B03980_10_41.jpg)

## 多元回归

多元回归是简单线性回归的延伸，有一个重要的差异，即可以有两个或多个自变量用于预测或解释一个因变量的方差。加入更多的自变量并不一定会使回归更好。可能会出现两个问题，其中之一是过度拟合。我们在前面的章节中已经谈到了这一点。过多的独立变量会增加方差，但实际上，它们对模型没有任何影响，从而导致过度拟合。此外，增加更多的自变量会增加更多的关系。不仅自变量可能与因变量相关，而且自变量之间也可能存在相关性。这种情况称为多重共线性。理想的期望是自变量与因变量相关，但彼此不相关。

由于过度拟合和多重共线性问题，在开始多元回归分析工作之前，需要进行准备工作。准备工作可以包括计算相关性、绘制散点图和运行简单的线性回归等。

假设，我们有一个因变量和四个自变量，存在多重共线性风险。这意味着在四个自变量和一个因变量之间有四种关系，在自变量中，可能有六个以上的关系。因此，这里有 10 种关系需要考虑。DV 代表因变量，IV 代表自变量。

![Multiple regression](img/B03980_10_42.jpg)

有些自变量比其他变量更适合预测因变量，而有些变量可能对预测没有任何贡献。需要决定考虑哪一个因变量。

在多元回归中，每个系数被解释为对应于变量一个单位变化的 *y* 的估计变化，而其余变量被假定为常数。

以下是多元回归方程。

![Multiple regression](img/B03980_10_43.jpg)

假设我们想要将一个独立变量拟合为许多变量的函数( *x* 、 *y* 和 *x* ² )。我们可以遵循一个简单的程序来得到所有变量的系数。这适用于线性、二次和三次函数。

以下是的分步过程:

1.  在单独的列中对每个变量的所有点进行排序。
2.  将独立变量的所有列组合起来表示为一个矩阵。
3.  在矩阵开头的 1 中添加一列。
4.  将此矩阵命名为 *X* 矩阵。
5.  把所有自变量做一个单独的列矩阵，称之为 *Y* 矩阵。
6.  Compute the coefficients using the formula here (this is the least square regression):

    *B =(X*^T*X)*^(-1)*X*^T*Y*

这是一个矩阵运算，得到的向量就是系数。

在多元回归中，在运行回归模型之前需要做大量的准备工作。有必要退后一步，对所考虑的变量进行一些分析。可以绘制一些基本的散点图来检查任何相关性，并分析因变量之间的关系。可以使用散点图、相关性分析、个体或群体回归等技术。如果有任何定性或分类变量，我们将需要使用虚拟变量来建立回归模型。

## 多项式(非线性)回归

虽然线性回归模型 *y = Xβ + ε* 是一个通用模型，将拟合未知参数 *β* 中的任何线性关系，但多项式模型适用于分析师知道真实响应函数中存在曲线效应的情况。多项式模型也被用作未知的和可能非常复杂的非线性关系的近似函数。多项式模型是未知函数的泰勒级数展开。

如果两个变量线性相关，散点图如下所示:

![Polynomial (non-linear) regression](img/B03980_10_44.jpg)

从前面的二元散点图可以清楚地看出，朋友和幸福之间存在线性关系。图表显示朋友越多，越幸福。如果我们谈论变量之间的曲线关系，朋友的数量和幸福呢？这意味着随着朋友数量的增加，幸福感也会增加，但只是到了一定程度。下图显示了数据中的这种行为:

![Polynomial (non-linear) regression](img/B03980_10_45.jpg)

如果数据不是线性的，那么过程是通过转换 IVs 或 DV 使其成为线性的，从而在它们之间存在线性关系。这种转换并不总是有效的，因为在数据和行为中可能存在真正的非线性。在这种情况下，我们需要在回归中包含独立变量的平方。这也称为多项式/二次回归。**最小二乘法**方法用于拟合多项式回归模型，因为它最小化了系数估计中的方差。

## 广义线性模型(GLM)

我们来看看一个线性回归模型不起作用的原因。

简单线性回归是预测另一个多元回归的定量变量。这是一种扩展的简单线性回归，但有更多的独立变量，最后，非线性或多项式回归是有两个定量变量，但数据是曲线的情况。

现在，运行一个典型的线性回归，以同样的方式，有一些问题。二进制数据没有正态分布。这就是需要其他回归模型的地方。其次，因变量的预测值可以超越 0 和 1，这违背了概率的概念。最后，概率通常是非线性的，在极端情况下可能取很低或很高的值。

GLM 是线性回归的推广，支持独立变量可能具有非正态分布的分布误差模型的情况。GLM 推广了线性回归，因为它允许线性模型通过链接函数与独立变量相关联，并且它还允许每个度量的方差程度是其预测值的函数。

简而言之，GLM 概括了线性、逻辑和泊松回归模型。

## 逻辑回归(logit link)

逻辑回归是线性回归的延伸，其中因变量是负责观察分类的分类变量。

例如，如果 *Y* 表示特定客户是否可能购买产品(1)或不太可能购买(0)，我们就有一个包含两个类别(0 和 1)的分类变量。逻辑回归可以解决类别未知的分类问题。这是通过使用预测值将未知类别的新观测值根据变量分类到其中一个类别中来实现的。

这些例子如下:

*   将客户分类为退货(1)或不退货(0)
*   根据信用评分预测贷款是否会被批准或拒绝

一个重要的用途是找出预测值之间的相似性。

在我们开始深入研究逻辑回归之前，让我们回顾一下前面章节中提到的概率和赔率的概念。

概率=感兴趣的结果/所有可能的结果。

比如抛一枚公平的硬币， *P(正面)= = 0.5* 。掷出一个骰子， *P(1 或 2) = 2/6 = 1/3 = 0.33* 。一副牌中， *P(菱形牌)= 13/52 = = 0.25* 。

赔率= P(某事发生)/P(某事没有发生)= p/1-p

比如抛硬币的时候，*赔率(人头= 0.5/0.5= 1)* 。掷出一个骰子，*赔率(1 或 2) = 0.333/0.666 = = 0.5* 。一副牌中，*赔率(钻石卡)= 0.25/0.75 = 1/3 = 0.333* 。

赔率是两个赔率的比值。

例如，当抛硬币时，在公平抛硬币的情况下:

*P(人头)= = 0.5* 和*赔率(人头)= 0.5/0.5 = 1 = 1:1*

在装有硬币的情况下:

*P(人头)= 0.7，赔率(人头)= 0.7/0.3 = 2.333*

*赔率= odds1/odds0 = 2.333/1 = 2.333*

这意味着当一枚装满硬币的硬币被投掷时，得到正面的几率是普通硬币的 2.333 倍。

总的来说，逻辑回归旨在:

*   **模型**事件发生的概率取决于自变量的值，这些值可以是分类的或数字的
*   **估计**事件发生与不发生的概率
*   **预测**一组变量对一个二元响应变量的影响
*   **根据概率估计，将观察结果分类为特定类别**

### logistic 回归中的优势比

逻辑回归中变量的优势比表示一个变量的优势如何随着该变量中一个单位的增加而变化，保持其余变量不变。

让我们举个例子来理解这一点——体重是否依赖于睡眠呼吸暂停。让我们假设体重变量的优势比为 1.07。这意味着体重增加一磅可能会使睡眠呼吸暂停的几率增加 1.07 倍。这可能并不重要。在体重增加 10 磅的情况下，几率增加到 1.98，这使患有睡眠呼吸暂停的人的几率增加一倍。重要的是，我们要把概率和赔率分开来衡量。例如，尽管体重增加 20 磅增加了这个人睡了 4 次的可能性，但是这个人体重增加 20 磅的可能性可能非常低。

在逻辑回归中，有两个重要步骤:

1.  找出属于某一特定阶层的概率。所以，如果 *Y = 0* 或 *1* ，属于*类 1* 的概率为 *P(Y=1)* 。
2.  我们将需要使用概率的截止值来确保每个案例进入其中一个类别。在二进制截止的情况下，a *P(Y=1) > 0.5* 将被归类为 *1* ，而 *P(Y=0) < 0.5* 将被归类为 *0* 。

#### 型号

*y* [i] 正态分布，对于 *i = 0，1，…，n* 取值为 0 或 1。

*y* [i] 等于{0，1}其中*P(y*[I]*= 1)= P*和*P(y*[I]*= 0)= 1-P*

*Y = a + bx 为 P(Y*[I]*= 1)*

*p*[I]*= a+bx*[I]

注意*p*I 不会取(0，1)之间的值。这可以通过使用预测值的非线性函数来解决，例如:

![Model](img/B03980_10_46.jpg)

很明显，当 *x* 从-∞到∞变化时，它取 0 到 1 之间的值。由此可以得到*a+bx*I 如下:

![Model](img/B03980_10_47.jpg)

下面的曲线显示了函数如何变化:

![Model](img/B03980_10_48.jpg)

## 泊松回归

在 GLM 的上下文中，泊松回归是独立变量具有泊松分布的数据计数，应用的连接函数是可以使用未知参数的线性组合建模的响应的对数。

<title>Implementing linear and logistic regression</title>

# 实现线性和逻辑回归

参考本章提供的源代码实现线性回归。(技术的每个文件夹下的源代码路径`.../chapter10/...`)

## 使用看象人

参考至中的文件夹`.../mahout/chapter10/linearregressionexample/`。

参考文件夹`.../mahout/chapter10/logisticregressionexample/`。

## 使用 R

参考至文件夹`.../r/chapter10/linearregressionexample/`。

参见文件夹`.../r/chapter10/logisticregressionexample/`。

## 使用火花

参考至中的文件夹`.../spark/chapter10/linearregressionexample/`。

参见文件夹`.../spark/chapter10/logisticregressionexample/`。

## 使用 scikit-learn

将参考到文件夹`.../python-scikit-learn/chapter10/linearregressionexample/`

参考到文件夹`.../python-scikit-learn/chapter10/logisticregressionexample/`

## 利用朱丽亚

参考到的文件夹`.../julia/chapter10/linearregressionexample/`。

参见文件夹`.../julia/chapter10/logisticregressionexample/`。

<title>Summary</title>

# 总结

在本章中，您学习了基于回归分析的机器学习，特别是如何使用 Mahout、R、Python、Julia 和 Spark 实现线性和逻辑回归模型。此外，我们还介绍了统计学的其他相关概念，如方差、协方差和方差分析等。我们通过示例深入讨论了回归模型，以理解如何将它们应用到现实世界的问题中。在下一章，我们将讨论深度学习方法。