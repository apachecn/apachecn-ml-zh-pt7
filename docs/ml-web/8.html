<html><head/><body>



<title>Chapter 8. Sentiment Analyser Application for Movie Reviews</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch08"/>第八章。用于电影评论的情感分析器应用程序</h1></div></div></div><p>在这一章中，我们描述了一个应用程序，使用整本书中描述的算法和方法来确定电影评论的情感。此外，<a id="id578" class="indexterm"/> <strong> Scrapy </strong>库将用于通过一个搜索引擎API (Bing搜索引擎)收集不同网站的评论。使用报纸库或遵循HTML格式页面的一些预定义提取规则来提取电影评论的文本和标题。使用朴素贝叶斯分类器对信息量最大的单词(使用X <em> 2 </em>度量)来确定每个评论的情感，其方式与第4章、<em> Web挖掘技术</em>中的方式相同。此外，使用第4章、<em> Web挖掘技术</em>中讨论的<a class="link" href="ch04.html" title="Chapter 4. Web Mining Techniques">page rank算法，计算与每个电影查询相关的每个页面的排名</a><a id="id579" class="indexterm"/>完整性。本章将讨论用于构建应用程序的代码，包括Django模型和视图，Scrapy scraper用于从电影评论的网页中收集数据。我们首先给出一个web应用程序的例子，并解释所使用的搜索引擎API以及我们如何将它包含在应用程序中。然后，我们描述如何收集电影评论，将Scrapy库集成到Django中，存储数据的模型，以及管理应用程序的主要命令。本章讨论的所有代码都可以在作者位于<a class="ulink" href="https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_8">https://GitHub . com/ai 2010/machine _ learning _ for _ the _ web/tree/master/chapter _ 8</a>的<code class="literal">chapter_8</code>文件夹中的GitHub资源库中找到。</p><div><div><div><div><h1 class="title"><a id="ch08lvl1sec52"/>应用使用概述</h1></div></div></div><p><a id="id580" class="indexterm"/>主页如下:</p><div><img src="img/5143_08_01.jpg" alt="Application usage overview"/></div><p>如果用户想知道评论的观点和相关性，他们可以键入电影名称。例如，我们在下面的截图中寻找<em>蝙蝠侠大战超人正义黎明</em>:</p><div><img src="img/5143_08_02.jpg" alt="Application usage overview"/></div><p>应用程序<a id="id582" class="indexterm"/>从Bing搜索引擎收集并抓取18条评论，并使用Scrapy库分析他们的情绪(15条正面评论和3条负面评论)。所有数据都存储在Django模型中，准备好用于使用PageRank算法计算每个页面的相关性(页面底部的链接如前面的截图所示)。在这种情况下，使用PageRank算法，我们得到以下结果:</p><div><img src="img/5143_08_03.jpg" alt="Application usage overview"/></div><p>这是与我们的电影评论搜索最相关的页面的列表，在抓取爬虫上设置深度参数2(参考下面的部分以获得进一步的细节)。请注意，要获得良好的页面相关性结果，您必须抓取数千个页面(前面的屏幕截图显示了大约50个抓取页面的结果)。</p><p>为了编写应用程序，我们照常启动服务器(参见<a class="link" href="ch06.html" title="Chapter 6. Getting Started with Django">第6章</a>、<em>Django入门</em>和<a class="link" href="ch07.html" title="Chapter 7. Movie Recommendation System Web Application">第7章</a>、<em>电影推荐系统Web应用</em>)和Django中的主app。首先，我们创建一个文件夹来存储我们所有的代码，<code class="literal">movie_reviews_analyzer_app</code>，然后我们使用下面的命令初始化Django:</p><div><pre class="programlisting">
<strong>mkdir  movie_reviews_analyzer_app</strong>
<strong>cd  movie_reviews_analyzer_app</strong>
<strong>django-admin startproject webmining_server</strong>
<strong>python manage.py startapp startapp pages</strong>
</pre></div><p>我们像在<a class="link" href="ch06.html" title="Chapter 6. Getting Started with Django">第六章</a><em>Django</em>入门<em>设置</em>部分、<a class="link" href="ch07.html" title="Chapter 7. Movie Recommendation System Web Application">第七章</a>的<em>应用设置</em>部分、<em>电影推荐系统Web应用</em>中一样设置<code class="literal">.py</code>文件中的设置(当然，在这种情况下名称是<code class="literal">webmining_server</code>而不是<code class="literal">server_movierecsys</code>)。</p><p>情感<a id="id583" class="indexterm"/>分析器应用程序将<code class="literal">.py</code>文件中的主要视图放在主<code class="literal">webmining_server</code>文件夹中，而不是我们之前所做的<code class="literal">app</code> (pages)文件夹中(参见<a class="link" href="ch06.html" title="Chapter 6. Getting Started with Django">第6章</a>、<em>Django入门</em>和<a class="link" href="ch07.html" title="Chapter 7. Movie Recommendation System Web Application">第7章</a>、<em>电影推荐系统Web应用程序</em>)，因为这些功能现在更多地指的是服务器的一般功能，而不是特定的应用程序(pages)。</p><p>使web服务可操作的最后一个操作是创建一个<code class="literal">superuser</code>帐户，并与服务器一起运行:</p><div><pre class="programlisting">
<strong>python manage.py createsuperuser (admin/admin)</strong>
<strong>python manage.py runserver</strong>
</pre></div><p>既然已经解释了应用程序的结构，我们可以从用于收集URL的搜索引擎API开始，更详细地讨论不同的部分。</p></div></div>





<title>Search engine choice and the application code</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch08lvl1sec53"/>搜索引擎选择和应用代码</h1></div></div></div><p>由于直接从最相关的搜索引擎如Google、Bing、Yahoo和其他搜索引擎抓取<a id="id584" class="indexterm"/>违反了他们的服务条款，我们需要从他们的REST API中获取初始<a id="id585" class="indexterm"/>评论页面(使用抓取服务如Crawlera、<a class="ulink" href="http://www.crawlera.com/">http://crawlera.com/</a>也是可能的)。我们决定使用Bing服务，它允许每月5000次免费查询。</p><p>为了做到这一点，我们向微软服务注册，以获得允许搜索所需的密钥。简而言之，我们遵循以下步骤:</p><div><ol class="orderedlist arabic"><li class="listitem">在<a class="ulink" href="https://datamarket.azure.com">https://datamarket.azure.com</a>上在线注册。</li><li class="listitem">在<strong>我的账户</strong>中，取<strong>主账户密钥</strong>。</li><li class="listitem">注册一个新的应用程序(在<strong>开发者</strong>下)| <strong>注册</strong>；放<strong>雷迪</strong>T28】CT URI:<code class="literal">https://www.</code><code class="literal">bing.com</code></li></ol></div><p>之后，我们可以编写一个函数来检索尽可能多的与我们的查询相关的URL:</p><div><pre class="programlisting">num_reviews = 30 
def bing_api(query):
    keyBing = API_KEY        # get Bing key from: https://datamarket.azure.com/account/keys
    credentialBing = 'Basic ' + (':%s' % keyBing).encode('base64')[:-1] # the "-1" is to remove the trailing "\n" which encode adds
    searchString = '%27X'+query.replace(" ",'+')+'movie+review%27'
    top = 50#maximum allowed by Bing
    
    reviews_urls = []
    if num_reviews&lt;top:
        offset = 0
        url = 'https://api.datamarket.azure.com/Bing/Search/Web?' + \
              'Query=%s&amp;$top=%d&amp;$skip=%d&amp;$format=json' % (searchString, num_reviews, offset)

        request = urllib2.Request(url)
        request.add_header('Authorization', credentialBing)
        requestOpener = urllib2.build_opener()
        response = requestOpener.open(request)
        results = json.load(response)
        reviews_urls = [ d['Url'] for d in results['d']['results']]
    else:
        nqueries = int(float(num_reviews)/top)+1
        for i in xrange(nqueries):
            offset = top*i
            if i==nqueries-1:
                top = num_reviews-offset
                url = 'https://api.datamarket.azure.com/Bing/Search/Web?' + \
                      'Query=%s&amp;$top=%d&amp;$skip=%d&amp;$format=json' % (searchString, top, offset)

                request = urllib2.Request(url)
                request.add_header('Authorization', credentialBing)
                requestOpener = urllib2.build_opener()
                response = requestOpener.open(request) 
            else:
                top=50
                url = 'https://api.datamarket.azure.com/Bing/Search/Web?' + \
                      'Query=%s&amp;$top=%d&amp;$skip=%d&amp;$format=json' % (searchString, top, offset)

                request = urllib2.Request(url)
                request.add_header('Authorization', credentialBing)
                requestOpener = urllib2.build_opener()
                response = requestOpener.open(request) 
            results = json.load(response)
            reviews_urls += [ d['Url'] for d in results['d']['results']]
    return reviews_urls</pre></div><p><code class="literal">API_KEY</code>参数取自微软账户，<code class="literal">query</code>是指定<a id="id586" class="indexterm"/>电影名称的字符串，<code class="literal">num_reviews = 30</code>是从Bing API返回的URL总数。有了包含评论的URL列表，我们现在可以设置一个scraper，使用Scrapy从每个网页中提取标题和评论文本。</p></div>





<title>Scrapy setup and the application code</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch08lvl1sec54"/> Scrapy设置和应用代码</h1></div></div></div><p>Scrapy是一个<a id="id587" class="indexterm"/> Python库，用于从网页中提取内容或抓取链接到给定网页的页面(详见<a class="link" href="ch04.html" title="Chapter 4. Web Mining Techniques">第4章</a>、<em> Web挖掘技术</em>的<em> Web爬虫(或蜘蛛)</em>部分)。要安装该库，请在终端中键入以下内容:</p><div><pre class="programlisting">
<strong>sudo pip install Scrapy </strong>
</pre></div><p>将可执行文件安装在<code class="literal">bin</code>文件夹中:</p><div><pre class="programlisting">
<strong>sudo easy_install scrapy</strong>
</pre></div><p>从<code class="literal">movie_reviews_analyzer_app</code>文件夹，我们如下初始化我们的Scrapy项目:</p><div><pre class="programlisting">
<strong>scrapy startproject scrapy_spider</strong>
</pre></div><p>该命令将在<code class="literal">scrapy_spider</code>文件夹中创建以下树:</p><div><pre class="programlisting">
<strong>├── __init__.py</strong>
<strong>├── items.py</strong>
<strong>├── pipelines.py</strong>
<strong>├── settings.py ├── spiders</strong>
<strong>├── spiders</strong>
<strong>│   ├── __init__.py</strong>
</pre></div><p><code class="literal">pipelines.py</code>和<code class="literal">items.py</code>文件管理如何存储和操作抓取的数据，它们将在后面的<em>蜘蛛</em>和<em>集成Django和Scrapy </em>部分讨论。<code class="literal">settings.py</code>文件设置在<code class="literal">spiders</code>文件夹中定义的每个蜘蛛(或爬虫)用来操作的参数。在接下来的两节中，我们将描述这个应用程序中使用的主要参数和蜘蛛。</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec46"/>刺痒设置</h2></div></div></div><p><code class="literal">settings.py</code>文件<a id="id588" class="indexterm"/>收集了Scrapy项目中每个蜘蛛抓取网页时使用的所有参数。主要参数如下:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">DEPTH_LIMIT</code>:跟随初始URL爬行的后续页面的数量。默认值是<code class="literal">0</code>，这意味着没有设置限制。</li><li class="listitem" style="list-style-type: disc"><code class="literal">LOG_ENABLED</code>:执行默认为真时，允许/拒绝Scrapy登录终端。</li><li class="listitem" style="list-style-type: disc"><code class="literal">ITEM_PIPELINES = {'scrapy_spider.pipelines.ReviewPipeline': 1000,}</code>:管道函数的路径，用于操作从每个网页中提取的数据。</li><li class="listitem" style="list-style-type: disc"><code class="literal">CONCURRENT_ITEMS = 200</code>:流水线中处理的并发项目数。</li><li class="listitem" style="list-style-type: disc"><code class="literal">CONCURRENT_REQUESTS = 5000</code>:Scrapy处理的最大并发请求数。</li><li class="listitem" style="list-style-type: disc"><code class="literal">CONCURRENT_REQUESTS_PER_DOMAIN = 3000</code>:Scrapy为每个指定域处理的最大并发请求数。</li></ul></div><p>深度越大，刮擦的页面就越多，因此刮擦所需的时间也会增加。为了加速这个过程，可以在最后三个参数上设置较高的值。在这个应用程序(<code class="literal">spiders</code>文件夹)中，我们设置了两个爬行器:一个从每个电影评论URL提取数据的抓取器(<code class="literal">movie_link_results.py</code>)和一个生成链接到初始电影评论URL的网页图的爬行器(<code class="literal">recursive_link_results.py</code>)。</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec47"/>刮刀</h2></div></div></div><p><code class="literal">movie_link_results.py</code>上的<a id="id589" class="indexterm"/>刮刀外观如下:</p><div><pre class="programlisting">
<strong>from newspaper import Article</strong>
<strong>from urlparse import urlparse</strong>
<strong>from scrapy.selector import Selector</strong>
<strong>from scrapy import Spider</strong>
<strong>from scrapy.spiders import BaseSpider,CrawlSpider, Rule</strong>
<strong>from scrapy.http import Request</strong>
<strong>from scrapy_spider import settings</strong>
<strong>from scrapy_spider.items import PageItem,SearchItem</strong>

<strong>unwanted_domains = ['youtube.com','www.youtube.com']</strong>
<strong>from nltk.corpus import stopwords</strong>
<strong>stopwords = set(stopwords.words('english'))</strong>

<strong>def CheckQueryinReview(keywords,title,content):</strong>
<strong>    content_list = map(lambda x:x.lower(),content.split(' '))</strong>
<strong>    title_list = map(lambda x:x.lower(),title.split(' '))</strong>
<strong>    words = content_list+title_list</strong>
<strong>    for k in keywords:</strong>
<strong>        if k in words:</strong>
<strong>            return True</strong>
<strong>    return False</strong>

<strong>class Search(Spider):</strong>
<strong>    name = 'scrapy_spider_reviews'</strong>
<strong>    </strong>
<strong>    def __init__(self,url_list,search_key):#specified by -a</strong>
<strong>        self.search_key = search_key</strong>
<strong>        self.keywords = [w.lower() for w in search_key.split(" ") if w not in stopwords]</strong>
<strong>        self.start_urls =url_list.split(',')</strong>
<strong>        super(Search, self).__init__(url_list)</strong>
<strong>    </strong>
<strong>    def start_requests(self):</strong>
<strong>        for url in self.start_urls:</strong>
<strong>            yield Request(url=url, callback=self.parse_site,dont_filter=True)</strong>
<strong>                        </strong>
<strong>    def parse_site(self, response):</strong>
<strong>        ## Get the selector for xpath parsing or from newspaper</strong>
<strong>        </strong>
<strong>        def crop_emptyel(arr):</strong>
<strong>            return [u for u in arr if u!=' ']</strong>
<strong>        </strong>
<strong>        domain = urlparse(response.url).hostname</strong>
<strong>        a = Article(response.url)</strong>
<strong>        a.download()</strong>
<strong>        a.parse()</strong>
<strong>        title = a.title.encode('ascii','ignore').replace('\n','')</strong>
<strong>        sel = Selector(response)</strong>
<strong>        if title==None:</strong>
<strong>            title = sel.xpath('//title/text()').extract()</strong>
<strong>            if len(title)&gt;0:</strong>
<strong>                title = title[0].encode('utf-8').strip().lower()</strong>
<strong>                </strong>
<strong>        content = a.text.encode('ascii','ignore').replace('\n','')</strong>
<strong>        if content == None:</strong>
<strong>            content = 'none'</strong>
<strong>            if len(crop_emptyel(sel.xpath('//div//article//p/text()').extract()))&gt;1:</strong>
<strong>                contents = crop_emptyel(sel.xpath('//div//article//p/text()').extract())</strong>
<strong>                print 'divarticle'</strong>
<strong>            ….</strong>
<strong>            elif len(crop_emptyel(sel.xpath('/html/head/meta[@name="description"]/@content').extract()))&gt;0:</strong>
<strong>                contents = crop_emptyel(sel.xpath('/html/head/meta[@name="description"]/@content').extract())</strong>
<strong>            content = ' '.join([c.encode('utf-8') for c in contents]).strip().lower()</strong>
<strong>                </strong>
<strong>        #get search item </strong>
<strong>        search_item = SearchItem.django_model.objects.get(term=self.search_key)</strong>
<strong>        #save item</strong>
<strong>        if not PageItem.django_model.objects.filter(url=response.url).exists():</strong>
<strong>            if len(content) &gt; 0:</strong>
<strong>                if CheckQueryinReview(self.keywords,title,content):</strong>
<strong>                    if domain not in unwanted_domains:</strong>
<strong>                        newpage = PageItem()</strong>
<strong>                        newpage['searchterm'] = search_item</strong>
<strong>                        newpage['title'] = title</strong>
<strong>                        newpage['content'] = content</strong>
<strong>                        newpage['url'] = response.url</strong>
<strong>                        newpage['depth'] = 0</strong>
<strong>                        newpage['review'] = True</strong>
<strong>                        #newpage.save()</strong>
<strong>                        return newpage  </strong>
<strong>        else:</strong>
<strong>            return null</strong>
</pre></div><p>我们可以看到<a id="id590" class="indexterm"/>来自<code class="literal">scrapy</code>的<code class="literal">Spider</code>类被<code class="literal">Search</code>类继承，必须定义以下标准方法来覆盖这些标准方法:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">__init__</code>:蜘蛛的构造器需要定义<code class="literal">start_urls</code>列表，该列表包含要从中提取内容的URL。此外，我们还有自定义变量，如<code class="literal">search_key</code>和<code class="literal">keywords</code>，它们存储与搜索引擎API上使用的电影名称查询相关的信息。</li><li class="listitem" style="list-style-type: disc"><code class="literal">start_requests</code>:该函数在<code class="literal">spider</code>被调用时被触发，它声明对<code class="literal">start_urls</code>列表中的每个URL做什么；对于每个URL，将调用自定义的<code class="literal">parse_site</code>函数(而不是默认的<code class="literal">parse</code>函数)。</li><li class="listitem" style="list-style-type: disc"><code class="literal">parse_site</code>:从每个URL解析数据是一个自定义函数。为了提取评论的标题及其文本内容，我们使用了报纸库(<code class="literal">sudo pip install newspaper</code>)，或者，如果失败，我们使用一些定义好的规则直接解析HTML文件，以避免由于不需要的标签而产生的干扰(每个规则结构都是用<code class="literal">sel.xpath</code>命令定义的)。为了达到这个结果，我们选择了一些流行的域名(<code class="literal">rottentomatoes</code>、<code class="literal">cnn</code>等等)，并确保解析能够从这些网站中提取内容(并非所有的提取规则都显示在前面的代码中，但它们通常可以在GitHub文件中找到)。然后使用相关的废料项目和<code class="literal">ReviewPipeline</code>功能将数据存储在页面<code class="literal">Django</code>模型中(见下一节)。</li><li class="listitem" style="list-style-type: disc"><code class="literal">CheckQueryinReview</code>:这是一个自定义函数，用于检查电影标题(来自查询)是否包含在每个网页的内容或标题中。</li></ul></div><p>要运行<a id="id591" class="indexterm"/>蜘蛛，我们需要从<code class="literal">scrapy_spider</code>(内部)文件夹中键入以下命令:</p><div><pre class="programlisting">
<strong>scrapy crawl scrapy_spider_reviews -a url_list=listname -a search_key=keyname</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec48"/>管道</h2></div></div></div><p>管道<a id="id592" class="indexterm"/>定义了当蜘蛛抓取新页面时要做什么。在前面的例子中，<code class="literal">parse_site</code>函数返回一个<code class="literal">PageItem</code>对象，它触发下面的管道(<code class="literal">pipelines.py</code>):</p><div><pre class="programlisting">
<strong>class ReviewPipeline(object):</strong>
<strong>    def process_item(self, item, spider):</strong>
<strong>        #if spider.name == 'scrapy_spider_reviews':#not working</strong>
<strong>           item.save()</strong>
<strong>           return item</strong>
</pre></div><p>这个类只是保存每个条目(蜘蛛符号中的一个新页面)。</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec49"/>爬虫</h2></div></div></div><p>正如我们在概述(上一节)中显示的，在我们存储了从评论的URL开始的所有链接页面之后，使用PageRank算法计算评论的相关性。爬虫<code class="literal">recursive_link_results.py</code>执行此操作:</p><div><pre class="programlisting">
<strong>#from scrapy.spider import Spider</strong>
<strong>from scrapy.selector import Selector</strong>
<strong>from scrapy.contrib.spiders import CrawlSpider, Rule</strong>
<strong>from scrapy.linkextractors import LinkExtractor</strong>
<strong>from scrapy.http import Request</strong>

<strong>from scrapy_spider.items import PageItem,LinkItem,SearchItem</strong>

<strong>class Search(CrawlSpider):</strong>
<strong>    name = 'scrapy_spider_recursive'</strong>
<strong>    </strong>
<strong>    def __init__(self,url_list,search_id):#specified by -a</strong>
<strong>    </strong>
<strong>        #REMARK is allowed_domains is not set then ALL are allowed!!!</strong>
<strong>        self.start_urls = url_list.split(',')</strong>
<strong>        self.search_id = int(search_id)</strong>
<strong>        </strong>
<strong>        #allow any link but the ones with different font size(repetitions)</strong>
<strong>        self.rules = (</strong>
<strong>            Rule(LinkExtractor(allow=(),deny=('fontSize=*','infoid=*','SortBy=*', ),unique=True), callback='parse_item', follow=True), </strong>
<strong>            )</strong>
<strong>        super(Search, self).__init__(url_list)</strong>

<strong>    def parse_item(self, response):</strong>
<strong>        sel = Selector(response)</strong>
<strong>        </strong>
<strong>        ## Get meta info from website</strong>
<strong>        title = sel.xpath('//title/text()').extract()</strong>
<strong>        if len(title)&gt;0:</strong>
<strong>            title = title[0].encode('utf-8')</strong>
<strong>            </strong>
<strong>        contents = sel.xpath('/html/head/meta[@name="description"]/@content').extract()</strong>
<strong>        content = ' '.join([c.encode('utf-8') for c in contents]).strip()</strong>

<strong>        fromurl = response.request.headers['Referer']</strong>
<strong>        tourl = response.url</strong>
<strong>        depth = response.request.meta['depth']</strong>
<strong>        </strong>
<strong>        #get search item </strong>
<strong>        search_item = SearchItem.django_model.objects.get(id=self.search_id)</strong>
<strong>        #newpage</strong>
<strong>        if not PageItem.django_model.objects.filter(url=tourl).exists():</strong>
<strong>            newpage = PageItem()</strong>
<strong>            newpage['searchterm'] = search_item</strong>
<strong>            newpage['title'] = title</strong>
<strong>            newpage['content'] = content</strong>
<strong>            newpage['url'] = tourl</strong>
<strong>            newpage['depth'] = depth</strong>
<strong>            newpage.save()#cant use pipeline cause the execution can finish here</strong>
<strong>        </strong>
<strong>        #get from_id,to_id</strong>
<strong>        from_page = PageItem.django_model.objects.get(url=fromurl)</strong>
<strong>        from_id = from_page.id</strong>
<strong>        to_page = PageItem.django_model.objects.get(url=tourl)</strong>
<strong>        to_id = to_page.id</strong>
<strong>        </strong>
<strong>        #newlink</strong>
<strong>        if not LinkItem.django_model.objects.filter(from_id=from_id).filter(to_id=to_id).exists():</strong>
<strong>            newlink = LinkItem()</strong>
<strong>            newlink['searchterm'] = search_item</strong>
<strong>            newlink['from_id'] = from_id</strong>
<strong>            newlink['to_id'] = to_id</strong>
<strong>            newlink.save()</strong>
</pre></div><p>来自<code class="literal">scrapy</code>的<code class="literal">CrawlSpider</code>类<a id="id594" class="indexterm"/>被<code class="literal">Search</code>类继承，必须定义以下标准方法来覆盖这些标准方法(对于蜘蛛的情况):</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">__init__</code>:是类的构造函数。<code class="literal">start_urls</code>参数定义了蜘蛛开始爬行的起始URL，直到达到<code class="literal">DEPTH_LIMIT</code>值。<code class="literal">rules</code>参数设置允许/拒绝抓取的URL的类型(在这种情况下，相同的页面但不同的字体大小被忽略),它定义调用的函数来操作每个检索的页面(<code class="literal">parse_item</code>)。此外，还定义了一个定制变量<code class="literal">search_id</code>，它需要在其他数据中存储查询的ID。</li><li class="listitem" style="list-style-type: disc"><code class="literal">parse_item</code>:这是一个自定义函数，调用它来存储每个检索页面的重要数据。从每个页面中创建一个新的<code class="literal">Page</code>模型的Django项(见下一节)，它包含页面的标题和内容(使用<code class="literal">xpath</code> HTML解析器)。为了执行PageRank算法，使用相关的Scrapy项将链接到每个页面的页面连接和页面本身保存为<code class="literal">Link</code>模型的对象(参见以下章节)。</li></ul></div><p>要运行<a id="id595" class="indexterm"/>爬虫，我们需要从(internal) <code class="literal">scrapy_spider</code>文件夹中键入以下内容:</p><div><pre class="programlisting">
<strong>scrapy crawl scrapy_spider_recursive -a url_list=listname -a search_id=keyname</strong>
</pre></div></div></div>





<title>Django models</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch08lvl1sec55"/> Django车型</h1></div></div></div><p>使用蜘蛛收集的数据<a id="id596" class="indexterm"/>需要存储在数据库中。在<a id="id597" class="indexterm"/> Django中，数据库表被称为模型，并在<code class="literal">models.py</code>文件中定义(在<code class="literal">pages</code>文件夹中)。该文件的内容如下:</p><div><pre class="programlisting">
<strong>from django.db import models</strong>
<strong>from django.conf import settings</strong>
<strong>from django.utils.translation import ugettext_lazy as _</strong>

<strong>class SearchTerm(models.Model):</strong>
<strong>    term = models.CharField(_('search'), max_length=255)</strong>
<strong>    num_reviews = models.IntegerField(null=True,default=0)</strong>
<strong>    #display term on admin panel</strong>
<strong>    def __unicode__(self):</strong>
<strong>            return self.term</strong>

<strong>class Page(models.Model):</strong>
<strong>     searchterm = models.ForeignKey(SearchTerm, related_name='pages',null=True,blank=True)</strong>
<strong>     url = models.URLField(_('url'), default='', blank=True)</strong>
<strong>     title = models.CharField(_('name'), max_length=255)</strong>
<strong>     depth = models.IntegerField(null=True,default=-1)</strong>
<strong>     html = models.TextField(_('html'),blank=True, default='')</strong>
<strong>     review = models.BooleanField(default=False)</strong>
<strong>     old_rank = models.FloatField(null=True,default=0)</strong>
<strong>     new_rank = models.FloatField(null=True,default=1)</strong>
<strong>     content = models.TextField(_('content'),blank=True, default='')</strong>
<strong>     sentiment = models.IntegerField(null=True,default=100)</strong>
<strong>     </strong>
<strong>class Link(models.Model):</strong>
<strong>     searchterm = models.ForeignKey(SearchTerm, related_name='links',null=True,blank=True)</strong>
<strong>     from_id = models.IntegerField(null=True)</strong>
<strong>     to_id = models.IntegerField(null=True)</strong>
</pre></div><p>在应用程序主页上输入的每个电影<a id="id598" class="indexterm"/>标题都存储在<code class="literal">SearchTerm</code>模型中，而每个网页的数据都收集在<code class="literal">Page</code>模型的一个对象中。除了内容字段(HTML、标题、URL、内容)之外，还记录了<a id="id599" class="indexterm"/>评论的情绪和在图形网络中的深度(布尔值还指示网页是电影评论页面还是简单的链接页面)。<code class="literal">Link</code>模型存储页面之间的所有图形链接，然后由PageRank算法用来计算评论网页的相关性。注意<code class="literal">Page</code>模型和<code class="literal">Link</code>模型都通过外键链接到相关的<code class="literal">SearchTerm</code>。像往常一样，要将这些模型写成数据库表，我们键入以下命令:</p><div><pre class="programlisting">
<strong>python manage.py makemigrations</strong>
<strong>python manage.py migrate</strong>
</pre></div><p>为了填充这些Django模型，我们需要让Scrapy与Django交互，这是下一节的主题。</p></div>





<title>Integrating Django with Scrapy</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch08lvl1sec56"/>整合姜戈和Scrapy</h1></div></div></div><p>为了使<a id="id600" class="indexterm"/>路径更容易调用，我们移除了外部的<code class="literal">scrapy_spider</code>文件夹，这样在<code class="literal">movie_reviews_analyzer_app</code>内部，<code class="literal">webmining_server</code>文件夹<a id="id601" class="indexterm"/>与<code class="literal">scrapy_spider</code>文件夹处于同一层级:</p><div><pre class="programlisting">
<strong>├── db.sqlite3</strong>
<strong>├── scrapy.cfg</strong>
<strong>├── scrapy_spider</strong>
<strong>│   ├── ...</strong>
<strong>│   ├── spiders</strong>
<strong>│   │   ...</strong>
<strong>└── webmining_server</strong>
</pre></div><p>我们将Django路径设置到Scrapy <code class="literal">settings.py</code>文件中:</p><div><pre class="programlisting">
<strong># Setting up django's project full path.</strong>
<strong>import sys</strong>
<strong>sys.path.insert(0, BASE_DIR+'/webmining_server')</strong>
<strong># Setting up django's settings module name.</strong>
<strong>os.environ['DJANGO_SETTINGS_MODULE'] = 'webmining_server.settings'</strong>
<strong>#import django to load models(otherwise AppRegistryNotReady: Models aren't loaded yet):</strong>
<strong>import django</strong>
<strong>django.setup()</strong>
</pre></div><p>现在我们可以安装库，允许从Scrapy管理Django模型:</p><div><pre class="programlisting">
<strong>sudo pip install scrapy-djangoitem</strong>
</pre></div><p>在<code class="literal">items.py</code>文件中，我们将Django模型和Scrapy项目之间的链接编写如下:</p><div><pre class="programlisting">
<strong>from scrapy_djangoitem import DjangoItem</strong>
<strong>from pages.models import Page,Link,SearchTerm</strong>

<strong>class SearchItem(DjangoItem):</strong>
<strong>    django_model = SearchTerm</strong>
<strong>class PageItem(DjangoItem):</strong>
<strong>    django_model = Page</strong>
<strong>class LinkItem(DjangoItem):</strong>
<strong>    django_model = Link</strong>
</pre></div><p>每个类<a id="id602" class="indexterm"/>继承了<code class="literal">DjangoItem</code>类，因此用<code class="literal">django_model</code>变量声明的<a id="id603" class="indexterm"/>原始Django模型被自动链接。Scrapy项目现在已经完成了，所以我们可以继续讨论解释处理Scrapy提取的数据的Django代码和管理应用程序所需的Django命令。</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec50"/>命令(情感分析模型和删除查询)</h2></div></div></div><p>应用程序需要管理一些不允许服务的最终用户进行的操作，例如<a id="id604" class="indexterm"/>定义一个情感<a id="id605" class="indexterm"/>分析模型和删除一个电影的查询以便重做它，而不是从内存中检索现有的数据。以下部分将解释执行这些操作的命令。</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec51"/>情感分析模型加载器</h2></div></div></div><p>这个应用程序的最终目标是确定电影评论的情绪(积极的或消极的)。为了实现这一点，必须使用一些外部数据来构建情感分类器，<a id="id606" class="indexterm"/>，然后将它存储在内存(缓存)中，供每个查询请求使用。这就是下面显示的<code class="literal">load_sentimentclassifier.py</code>命令的目的:</p><div><pre class="programlisting">
<strong>import nltk.classify.util, nltk.metrics</strong>
<strong>from nltk.classify import NaiveBayesClassifier</strong>
<strong>from nltk.corpus import movie_reviews</strong>
<strong>from nltk.corpus import stopwords</strong>
<strong>from nltk.collocations import BigramCollocationFinder</strong>
<strong>from nltk.metrics import BigramAssocMeasures</strong>
<strong>from nltk.probability import FreqDist, ConditionalFreqDist</strong>
<strong>import collections</strong>
<strong>from django.core.management.base import BaseCommand, CommandError</strong>
<strong>from optparse import make_option</strong>
<strong>from django.core.cache import cache</strong>

<strong>stopwords = set(stopwords.words('english'))</strong>
<strong>method_selfeatures = 'best_words_features'</strong>

<strong>class Command(BaseCommand):</strong>
<strong>    option_list = BaseCommand.option_list + (</strong>
<strong>                make_option('-n', '--num_bestwords',	</strong>
<strong>                             dest='num_bestwords', type='int',</strong>
<strong>                             action='store',</strong>
<strong>                             help=('number of words with high information')),)</strong>
<strong>    </strong>
<strong>    def handle(self, *args, **options):</strong>
<strong>         num_bestwords = options['num_bestwords']</strong>
<strong>         self.bestwords = self.GetHighInformationWordsChi(num_bestwords)</strong>
<strong>         clf = self.train_clf(method_selfeatures)</strong>
<strong>         cache.set('clf',clf)</strong>
<strong>         cache.set('bestwords',self.bestwords)</strong>
</pre></div><p>在文件的开头<a id="id607" class="indexterm"/>处，变量<code class="literal">method_selfeatures</code>设置了特征选择的方法(在这种情况下，特征是评论中的单词；参见<a class="link" href="ch04.html" title="Chapter 4. Web Mining Techniques">第4章</a>、<em> Web挖掘技术</em>，了解更多细节)用于训练分类器<code class="literal">train_clf</code>。最佳单词(特征)的最大数量由输入参数<code class="literal">num_bestwords</code>定义。然后，分类器和最佳特征(<code class="literal">bestwords</code>)存储在缓存中，以备应用程序使用(使用<code class="literal">cache</code>模块)。选择最佳单词(特征)的分类器和方法如下:</p><div><pre class="programlisting">
<strong>    def train_clf(method):</strong>
<strong>        negidxs = movie_reviews.fileids('neg')</strong>
<strong>        posidxs = movie_reviews.fileids('pos')</strong>
<strong>        if method=='stopword_filtered_words_features':</strong>
<strong>            negfeatures = [(stopword_filtered_words_features(movie_reviews.words(fileids=[file])), 'neg') for file in negidxs]</strong>
<strong>            posfeatures = [(stopword_filtered_words_features(movie_reviews.words(fileids=[file])), 'pos') for file in posidxs]</strong>
<strong>        elif method=='best_words_features':</strong>
<strong>            negfeatures = [(best_words_features(movie_reviews.words(fileids=[file])), 'neg') for file in negidxs]</strong>
<strong>            posfeatures = [(best_words_features(movie_reviews.words(fileids=[file])), 'pos') for file in posidxs]</strong>
<strong>        elif method=='best_bigrams_words_features':</strong>
<strong>            negfeatures = [(best_bigrams_words_features(movie_reviews.words(fileids=[file])), 'neg') for file in negidxs]</strong>
<strong>            posfeatures = [(best_bigrams_words_features(movie_reviews.words(fileids=[file])), 'pos') for file in posidxs]</strong>
<strong>            </strong>
<strong>        trainfeatures = negfeatures + posfeatures</strong>
<strong>        clf = NaiveBayesClassifier.train(trainfeatures)</strong>
<strong>        return clf</strong>

<strong>    def stopword_filtered_words_features(self,words):</strong>
<strong>        return dict([(word, True) for word in words if word not in stopwords])</strong>

<strong>    #eliminate Low Information Features</strong>
<strong>    def GetHighInformationWordsChi(self,num_bestwords):</strong>
<strong>        word_fd = FreqDist()</strong>
<strong>        label_word_fd = ConditionalFreqDist()</strong>

<strong>        for word in movie_reviews.words(categories=['pos']):</strong>
<strong>            word_fd[word.lower()] +=1</strong>
<strong>            label_word_fd['pos'][word.lower()] +=1</strong>

<strong>        for word in movie_reviews.words(categories=['neg']):</strong>
<strong>            word_fd[word.lower()] +=1</strong>
<strong>            label_word_fd['neg'][word.lower()] +=1</strong>

<strong>        pos_word_count = label_word_fd['pos'].N()</strong>
<strong>        neg_word_count = label_word_fd['neg'].N()</strong>
<strong>        total_word_count = pos_word_count + neg_word_count</strong>

<strong>        word_scores = {}</strong>
<strong>        for word, freq in word_fd.iteritems():</strong>
<strong>            pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],</strong>
<strong>                (freq, pos_word_count), total_word_count)</strong>
<strong>            neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],</strong>
<strong>                (freq, neg_word_count), total_word_count)</strong>
<strong>            word_scores[word] = pos_score + neg_score</strong>

<strong>        best = sorted(word_scores.iteritems(), key=lambda (w,s): s, reverse=True)[:num_bestwords]</strong>
<strong>        bestwords = set([w for w, s in best])</strong>
<strong>        return bestwords</strong>

<strong>    def best_words_features(self,words):</strong>
<strong>        return dict([(word, True) for word in words if word in self.bestwords])</strong>
<strong>    </strong>
<strong>    def best_bigrams_word_features(self,words, measure=BigramAssocMeasures.chi_sq, nbigrams=200):</strong>
<strong>        bigram_finder = BigramCollocationFinder.from_words(words)</strong>
<strong>        bigrams = bigram_finder.nbest(measure, nbigrams)</strong>
<strong>        d = dict([(bigram, True) for bigram in bigrams])</strong>
<strong>        d.update(best_words_features(words))</strong>
<strong>        return d</strong>
</pre></div><p>编写了三个方法<a id="id608" class="indexterm"/>来选择前面代码中的单词:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">stopword_filtered_words_features</code>:使用<strong>自然语言工具包</strong> ( <strong> NLTK </strong>)的连词列表删除<code class="literal">stopwords</code>，并将剩余的视为<a id="id609" class="indexterm"/>相关词</li><li class="listitem" style="list-style-type: disc"><code class="literal">best_words_features</code>:使用<em>X<sup>2</sup>T24】度量(<code class="literal">NLTK</code>库)，选择与正面或负面评论相关的最有信息量的词(参见<a class="link" href="ch04.html" title="Chapter 4. Web Mining Techniques">第四章</a>、<em>网络挖掘技术</em>，了解更多细节)</em></li><li class="listitem" style="list-style-type: disc"><code class="literal">best_bigrams_word_features</code>:使用<em>X<sup>2</sup>T32】度量(<code class="literal">NLTK</code>库)从单词集中找到200个最有信息的二元模型(详见<em>第4章</em>、<em> Web挖掘技术</em>)</em></li></ul></div><p>所选择的分类器是朴素贝叶斯算法(参见<a class="link" href="ch03.html" title="Chapter 3. Supervised Machine Learning">第3章</a>、<em>监督机器学习</em>)，标记文本(正面、负面情绪)取自<code class="literal">movie_reviews</code>的<code class="literal">NLTK.corpus</code>。要安装它，在Python中打开一个终端并从<code class="literal">corpus</code>安装<code class="literal">movie_reviews</code>:</p><div><pre class="programlisting">
<strong>nltk.download()--&gt; corpora/movie_reviews corpus</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec52"/>删除已经执行的查询</h2></div></div></div><p>由于我们可以<a id="id610" class="indexterm"/>指定不同的参数(比如特征选择方法、最佳词的数量等等)，我们可能希望使用不同的值来执行和存储评论的情感。为此需要<code class="literal">delete_query</code>命令<a id="id611" class="indexterm"/>，如下所示:</p><div><pre class="programlisting">
<strong>from pages.models import Link,Page,SearchTerm</strong>
<strong>from django.core.management.base import BaseCommand, CommandError</strong>
<strong>from optparse import make_option</strong>

<strong>class Command(BaseCommand):</strong>
<strong>    option_list = BaseCommand.option_list + (</strong>
<strong>                make_option('-s', '--searchid',</strong>
<strong>                             dest='searchid', type='int',</strong>
<strong>                             action='store',</strong>
<strong>                             help=('id of the search term to delete')),)</strong>

<strong>    def handle(self, *args, **options):</strong>
<strong>         searchid = options['searchid']</strong>
<strong>         if searchid == None:</strong>
<strong>             print "please specify searchid: python manage.py --searchid=--"</strong>
<strong>             #list</strong>
<strong>             for sobj in SearchTerm.objects.all():</strong>
<strong>                 print 'id:',sobj.id,"  term:",sobj.term</strong>
<strong>         else:</strong>
<strong>             print 'delete...'</strong>
<strong>             search_obj = SearchTerm.objects.get(id=searchid)</strong>
<strong>             pages = search_obj.pages.all()</strong>
<strong>             pages.delete()</strong>
<strong>             links = search_obj.links.all()</strong>
<strong>             links.delete()</strong>
<strong>             search_obj.delete()</strong>
</pre></div><p>如果我们运行<a id="id612" class="indexterm"/>命令而没有指定<a id="id613" class="indexterm"/><code class="literal">searchid</code>(查询的ID)，那么将会显示所有查询和相关ID的列表。之后，我们可以通过键入以下命令来选择要删除的查询:</p><div><pre class="programlisting">
<strong>python manage.py delete_query --searchid=VALUE</strong>
</pre></div><p>我们可以使用缓存的情感分析模型向用户显示所选电影的在线情感，正如我们在下一节中解释的那样。</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec53"/>情感评论分析器–Django视图和HTML</h2></div></div></div><p>本章中解释的大部分代码(命令、Bing搜索引擎、Scrapy和Django模型)在<code class="literal">views.py</code>中的函数分析器中使用<a id="id614" class="indexterm"/>来驱动<em>应用程序使用概述</em>部分中显示的主页(在将<code class="literal">urls.py</code>文件中的URL声明为<code class="literal">url(r'^$','webmining_server.views.analyzer')</code>之后)。</p><div><pre class="programlisting">
<strong>def analyzer(request):</strong>
<strong>    context = {}</strong>

<strong>    if request.method == 'POST':</strong>
<strong>        post_data = request.POST</strong>
<strong>        query = post_data.get('query', None)</strong>
<strong>        if query:</strong>
<strong>            return redirect('%s?%s' % (reverse('webmining_server.views.analyzer'),</strong>
<strong>                                urllib.urlencode({'q': query})))   </strong>
<strong>    elif request.method == 'GET':</strong>
<strong>        get_data = request.GET</strong>
<strong>        query = get_data.get('q')</strong>
<strong>        if not query:</strong>
<strong>            return render_to_response(</strong>
<strong>                'movie_reviews/home.html', RequestContext(request, context))</strong>

<strong>        context['query'] = query</strong>
<strong>        stripped_query = query.strip().lower()</strong>
<strong>        urls = []</strong>
<strong>        </strong>
<strong>        if test_mode:</strong>
<strong>           urls = parse_bing_results()</strong>
<strong>        else:</strong>
<strong>           urls = bing_api(stripped_query)</strong>
<strong>           </strong>
<strong>        if len(urls)== 0:</strong>
<strong>           return render_to_response(</strong>
<strong>               'movie_reviews/noreviewsfound.html', RequestContext(request, context))</strong>
<strong>        if not SearchTerm.objects.filter(term=stripped_query).exists():</strong>
<strong>           s = SearchTerm(term=stripped_query)</strong>
<strong>           s.save()</strong>
<strong>           try:</strong>
<strong>               #scrape</strong>
<strong>               cmd = 'cd ../scrapy_spider &amp; scrapy crawl scrapy_spider_reviews -a url_list=%s -a search_key=%s' %('\"'+str(','.join(urls[:num_reviews]).encode('utf-8'))+'\"','\"'+str(stripped_query)+'\"')</strong>
<strong>               os.system(cmd)</strong>
<strong>           except:</strong>
<strong>               print 'error!'</strong>
<strong>               s.delete()</strong>
<strong>        else:</strong>
<strong>           #collect the pages already scraped </strong>
<strong>           s = SearchTerm.objects.get(term=stripped_query)</strong>
<strong>           </strong>
<strong>        #calc num pages</strong>
<strong>        pages = s.pages.all().filter(review=True)</strong>
<strong>        if len(pages) == 0:</strong>
<strong>           s.delete()</strong>
<strong>           return render_to_response(</strong>
<strong>               'movie_reviews/noreviewsfound.html', RequestContext(request, context))</strong>
<strong>               </strong>
<strong>        s.num_reviews = len(pages)</strong>
<strong>        s.save()</strong>
<strong>         </strong>
<strong>        context['searchterm_id'] = int(s.id)</strong>

<strong>        #train classifier with nltk</strong>
<strong>        def train_clf(method):</strong>
<strong>            ...           </strong>
<strong>        def stopword_filtered_words_features(words):</strong>
<strong>            ... </strong>
<strong>        #Eliminate Low Information Features</strong>
<strong>        def GetHighInformationWordsChi(num_bestwords):</strong>
<strong>            ...            </strong>
<strong>        bestwords = cache.get('bestwords')</strong>
<strong>        if bestwords == None:</strong>
<strong>            bestwords = GetHighInformationWordsChi(num_bestwords)</strong>
<strong>        def best_words_features(words):</strong>
<strong>            ...       </strong>
<strong>        def best_bigrams_words_features(words, measure=BigramAssocMeasures.chi_sq, nbigrams=200):</strong>
<strong>            ...</strong>
<strong>        clf = cache.get('clf')</strong>
<strong>        if clf == None:</strong>
<strong>            clf = train_clf(method_selfeatures)</strong>

<strong>        cntpos = 0</strong>
<strong>        cntneg = 0</strong>
<strong>        for p in pages:</strong>
<strong>            words = p.content.split(" ")</strong>
<strong>            feats = best_words_features(words)#bigram_word_features(words)#stopword_filtered_word_feats(words)</strong>
<strong>            #print feats</strong>
<strong>            str_sent = clf.classify(feats)</strong>
<strong>            if str_sent == 'pos':</strong>
<strong>               p.sentiment = 1</strong>
<strong>               cntpos +=1</strong>
<strong>            else:</strong>
<strong>               p.sentiment = -1</strong>
<strong>               cntneg +=1</strong>
<strong>            p.save()</strong>

<strong>        context['reviews_classified'] = len(pages)</strong>
<strong>        context['positive_count'] = cntpos</strong>
<strong>        context['negative_count'] = cntneg</strong>
<strong>        context['classified_information'] = True</strong>
<strong>    return render_to_response(</strong>
<strong>        'movie_reviews/home.html', RequestContext(request, context))</strong>
</pre></div><p>插入的<a id="id615" class="indexterm"/>电影标题存储在<code class="literal">query</code>变量中，并发送给<code class="literal">bing_api</code>函数以收集评论的URL。然后调用Scrapy来抓取URL，以找到评论文本，使用<code class="literal">clf</code>分类器模型和从缓存中检索的所选最具信息性的单词(<code class="literal">bestwords</code>)来处理评论文本(或者在缓存为空的情况下再次生成相同的模型)。评论的预测情绪计数(<code class="literal">positive_counts</code>、<code class="literal">negative_counts</code>和<code class="literal">reviews_classified</code>)然后被发送回<code class="literal">home.html</code>(<code class="literal">templates</code>文件夹)页面，该页面使用以下Google饼状图代码:</p><div><pre class="programlisting">        &lt;h2 align = Center&gt;Movie Reviews Sentiment Analysis&lt;/h2&gt;
        &lt;div class="row"&gt;
        &lt;p align = Center&gt;&lt;strong&gt;Reviews Classified : {{ reviews_classified }}&lt;/strong&gt;&lt;/p&gt;
        &lt;p align = Center&gt;&lt;strong&gt;Positive Reviews : {{ positive_count }}&lt;/strong&gt;&lt;/p&gt;
        &lt;p align = Center&gt;&lt;strong&gt; Negative Reviews : {{ negative_count }}&lt;/strong&gt;&lt;/p&gt;
        &lt;/div&gt; 
  &lt;section&gt;
      &lt;script type="text/javascript" src="img/jsapi"&gt;&lt;/script&gt;
      &lt;script type="text/javascript"&gt;
        google.load("visualization", "1", {packages:["corechart"]});
        google.setOnLoadCallback(drawChart);
        function drawChart() {
          var data = google.visualization.arrayToDataTable([
            ['Sentiment', 'Number'],
            ['Positive',     {{ positive_count }}],
            ['Negative',      {{ negative_count }}]
          ]);
          var options = { title: 'Sentiment Pie Chart'};
          var chart = new google.visualization.PieChart(document.getElementById('piechart'));
          chart.draw(data, options);
        }
      &lt;/script&gt;
        &lt;p align ="Center" id="piechart" style="width: 900px; height: 500px;display: block; margin: 0 auto;text-align: center;" &gt;&lt;/p&gt;
      &lt;/div&gt;</pre></div><p>函数<code class="literal">drawChart</code>调用Google <code class="literal">PieChart</code>可视化函数，该函数将数据(正计数和负计数)作为输入<a id="id616" class="indexterm"/>来创建饼图。要了解HTML代码如何与Django视图交互的更多细节，请参考<em>HTML网页背后的URL和视图</em>部分的<a class="link" href="ch06.html" title="Chapter 6. Getting Started with Django">第6章</a>、<em>Django入门</em>。从带有观点计数的结果页面(参见<em>应用程序使用概述</em>部分)，可以使用页面底部的两个链接之一来计算抓取评论的PagerRank相关性。这个操作背后的Django代码将在下一节讨论。</p></div></div>





<title>PageRank: Django view and the algorithm code</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch08lvl1sec57"/> PageRank: Django视图和算法代码</h1></div></div></div><p>为了对在线评论的重要性进行排名，我们在应用程序中实现了PageRank算法(参见<a class="link" href="ch04.html" title="Chapter 4. Web Mining Techniques">第4章</a>、<em> Web挖掘技术</em>，在<em>排名:PageRank算法</em>部分)。<a id="id618" class="indexterm"/><code class="literal">webmining_server</code>文件夹内<code class="literal">pgrank</code>文件夹中的<code class="literal">pgrank.py</code>文件实现如下算法:</p><div><pre class="programlisting">
<strong>from pages.models import Page,SearchTerm</strong>

<strong>num_iterations = 100000</strong>
<strong>eps=0.0001</strong>
<strong>D = 0.85</strong>

<strong>def pgrank(searchid):</strong>
<strong>    s = SearchTerm.objects.get(id=int(searchid))</strong>
<strong>    links = s.links.all()</strong>
<strong>    from_idxs = [i.from_id for i in links ]</strong>
<strong>    # Find the idxs that receive page rank </strong>
<strong>    links_received = []</strong>
<strong>    to_idxs = []</strong>
<strong>    for l in links:</strong>
<strong>        from_id = l.from_id</strong>
<strong>        to_id = l.to_id</strong>
<strong>        if from_id not in from_idxs: continue</strong>
<strong>        if to_id  not in from_idxs: continue</strong>
<strong>        links_received.append([from_id,to_id])</strong>
<strong>        if to_id  not in to_idxs: to_idxs.append(to_id)</strong>
<strong>        </strong>
<strong>    pages = s.pages.all()</strong>
<strong>    prev_ranks = dict()</strong>
<strong>    for node in from_idxs:</strong>
<strong>        ptmp  = Page.objects.get(id=node)</strong>
<strong>        prev_ranks[node] = ptmp.old_rank</strong>
<strong>        </strong>
<strong>    conv=1.</strong>
<strong>    cnt=0</strong>
<strong>    while conv&gt;eps or cnt&lt;num_iterations:</strong>
<strong>        next_ranks = dict()</strong>
<strong>        total = 0.0</strong>
<strong>        for (node,old_rank) in prev_ranks.items():</strong>
<strong>            total += old_rank</strong>
<strong>            next_ranks[node] = 0.0</strong>
<strong>        </strong>
<strong>        #find the outbound links and send the pagerank down to each of them</strong>
<strong>        for (node, old_rank) in prev_ranks.items():</strong>
<strong>            give_idxs = []</strong>
<strong>            for (from_id, to_id) in links_received:</strong>
<strong>                if from_id != node: continue</strong>
<strong>                if to_id  not in to_idxs: continue</strong>
<strong>                give_idxs.append(to_id)</strong>
<strong>            if (len(give_idxs) &lt; 1): continue</strong>
<strong>            amount = D*old_rank/len(give_idxs)</strong>
<strong>            for id in give_idxs:</strong>
<strong>                next_ranks[id] += amount</strong>
<strong>        tot = 0</strong>
<strong>        for (node,next_rank) in next_ranks.items():</strong>
<strong>            tot += next_rank</strong>
<strong>        const = (1-D)/ len(next_ranks)</strong>
<strong>        </strong>
<strong>        for node in next_ranks:</strong>
<strong>            next_ranks[node] += const</strong>
<strong>        </strong>
<strong>        tot = 0</strong>
<strong>        for (node,old_rank) in next_ranks.items():</strong>
<strong>            tot += next_rank</strong>
<strong>        </strong>
<strong>        difftot = 0</strong>
<strong>        for (node, old_rank) in prev_ranks.items():</strong>
<strong>            new_rank = next_ranks[node]</strong>
<strong>            diff = abs(old_rank-new_rank)</strong>
<strong>            difftot += diff</strong>
<strong>        conv= difftot/len(prev_ranks)</strong>
<strong>        cnt+=1</strong>
<strong>        prev_ranks = next_ranks</strong>

<strong>    for (id,new_rank) in next_ranks.items():</strong>
<strong>        ptmp = Page.objects.get(id=id)</strong>
<strong>        url = ptmp.url</strong>
<strong>    </strong>
<strong>    for (id,new_rank) in next_ranks.items():</strong>
<strong>        ptmp = Page.objects.get(id=id)</strong>
<strong>        ptmp.old_rank = ptmp.new_rank</strong>
<strong>        ptmp.new_rank = new_rank</strong>
<strong>        ptmp.save()</strong>
</pre></div><p>此代码<a id="id619" class="indexterm"/>获取与给定的<code class="literal">SearchTerm</code>对象相关联的所有链接存储，并在时间<em> t </em>实现每页<em> i </em>的PageRank分数，其中<em> P(i) </em>由递归等式给出:</p><div><img src="img/B05143_08_05.jpg" alt="PageRank: Django view and the algorithm code"/></div><p>这里，<em> N </em>是总页数，<img src="img/B05143_08_06.jpg" alt="PageRank: Django view and the algorithm code"/>(<em>N<sub>j</sub>T23】是页面<em> j </em>的出链接数)如果页面<em> j </em>指向<em>I</em>；否则，<em> N </em>为<code class="literal">0</code>。参数<em> D </em>是所谓的<strong>阻尼因子</strong>(在前面的代码中设置为0.85)，它<a id="id620" class="indexterm"/>表示遵循转移矩阵<em> A </em>给出的转移的概率。方程被迭代，直到满足收敛参数<code class="literal">eps</code>或达到最大迭代次数<code class="literal">num_iterations</code>。在显示电影评论的观点后，通过点击<code class="literal">home.html</code>页面底部的<strong>抓取并计算页面排名(可能需要很长时间)</strong>或<strong>计算页面排名</strong>链接来调用该算法。链接链接到<code class="literal">views.py</code>中的函数<code class="literal">pgrank_view</code>(通过<code class="literal">urls.py: url(r'^pg-rank/(?P&lt;pk&gt;\d+)/','webmining_server.views.pgrank_view', name='pgrank_view')</code>中声明的URL):</em></p><div><pre class="programlisting">
<strong>def pgrank_view(request,pk): </strong>
<strong>    context = {}</strong>
<strong>    get_data = request.GET</strong>
<strong>    scrape = get_data.get('scrape','False')</strong>
<strong>    s = SearchTerm.objects.get(id=pk)</strong>
<strong>    </strong>
<strong>    if scrape == 'True':</strong>
<strong>        pages = s.pages.all().filter(review=True)</strong>
<strong>        urls = []</strong>
<strong>        for u in pages:</strong>
<strong>            urls.append(u.url)</strong>
<strong>        #crawl</strong>
<strong>        cmd = 'cd ../scrapy_spider &amp; scrapy crawl scrapy_spider_recursive -a url_list=%s -a search_id=%s' %('\"'+str(','.join(urls[:]).encode('utf-8'))+'\"','\"'+str(pk)+'\"')</strong>
<strong>        os.system(cmd)</strong>

<strong>    links = s.links.all()</strong>
<strong>    if len(links)==0:</strong>
<strong>       context['no_links'] = True</strong>
<strong>       return render_to_response(</strong>
<strong>           'movie_reviews/pg-rank.html', RequestContext(request, context))</strong>
<strong>    #calc pgranks</strong>
<strong>    pgrank(pk)</strong>
<strong>    #load pgranks in descending order of pagerank</strong>
<strong>    pages_ordered = s.pages.all().filter(review=True).order_by('-new_rank')</strong>
<strong>    context['pages'] = pages_ordered</strong>
<strong>    </strong>
<strong>    return render_to_response(</strong>
<strong>        'movie_reviews/pg-rank.html', RequestContext(request, context)) </strong>
</pre></div><p>这段代码<a id="id621" class="indexterm"/>调用爬虫收集所有链接到评论的页面，并使用前面讨论的代码计算PageRank分数。然后，分数会显示在<code class="literal">pg-rank.html</code>页面中(按页面排名分数降序排列)，如本章的<em>应用程序使用概述</em>部分所示。因为这个函数可能需要很长时间来处理(抓取数千个页面)，所以编写了命令<code class="literal">run_scrapelinks.py</code>来运行Scrapy crawler(作为一个练习，请读者按照自己喜欢的方式<a id="id622" class="indexterm"/>阅读或修改脚本)。</p></div>





<title>Admin and API</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch08lvl1sec58"/>管理和API</h1></div></div></div><p>作为本章的最后一部分<a id="id623" class="indexterm"/>，我们简要描述了模型的一些可能的<a id="id624" class="indexterm"/>管理和API端点的实现，以检索应用程序处理的数据。在<code class="literal">pages</code>文件夹中，我们可以在<code class="literal">admin.py</code>文件中设置两个管理界面来检查<code class="literal">SearchTerm</code>和<code class="literal">Page</code>模型采集的数据:</p><div><pre class="programlisting">
<strong>from django.contrib import admin</strong>
<strong>from django_markdown.admin import MarkdownField, AdminMarkdownWidget</strong>
<strong>from pages.models import SearchTerm,Page,Link</strong>

<strong>class SearchTermAdmin(admin.ModelAdmin):</strong>
<strong>    formfield_overrides = {MarkdownField: {'widget': AdminMarkdownWidget}}</strong>
<strong>    list_display = ['id', 'term', 'num_reviews']</strong>
<strong>    ordering = ['-id']</strong>
<strong>    </strong>
<strong>class PageAdmin(admin.ModelAdmin):</strong>
<strong>    formfield_overrides = {MarkdownField: {'widget': AdminMarkdownWidget}}</strong>
<strong>    list_display = ['id', 'searchterm', 'url','title','content']</strong>
<strong>    ordering = ['-id','-new_rank']</strong>
<strong>    </strong>
<strong>admin.site.register(SearchTerm,SearchTermAdmin)</strong>
<strong>admin.site.register(Page,PageAdmin)</strong>
<strong>admin.site.register(Link)</strong>
</pre></div><p>注意<code class="literal">SearchTermAdmin</code>和<code class="literal">PageAdmin</code>都显示ID递减的对象(在<code class="literal">PageAdmin</code>的情况下显示<code class="literal">new_rank</code>)。下面的截图就是一个例子:</p><div><img src="img/5143_08_04.jpg" alt="Admin and API"/></div><p>注意<a id="id625" class="indexterm"/>虽然不是必须的，<code class="literal">Link</code>模型也被<a id="id626" class="indexterm"/>包含在管理界面(<code class="literal">admin.site.register(Link)</code>)中。更有趣的是，我们可以设置一个API端点来检索与电影标题相关的情感计数。在pages文件夹内的<code class="literal">api.py</code>文件中，我们可以有以下内容:</p><div><pre class="programlisting">
<strong>from rest_framework import views,generics</strong>
<strong>from rest_framework.permissions import AllowAny</strong>
<strong>from rest_framework.response import Response</strong>
<strong>from rest_framework.pagination import PageNumberPagination</strong>
<strong>from pages.serializers import SearchTermSerializer</strong>
<strong>from pages.models import SearchTerm,Page</strong>

<strong>class LargeResultsSetPagination(PageNumberPagination):</strong>
<strong>    page_size = 1000</strong>
<strong>    page_size_query_param = 'page_size'</strong>
<strong>    max_page_size = 10000</strong>
<strong>  </strong>
<strong>class SearchTermsList(generics.ListAPIView):</strong>

<strong>    serializer_class = SearchTermSerializer</strong>
<strong>    permission_classes = (AllowAny,)</strong>
<strong>    pagination_class = LargeResultsSetPagination</strong>
<strong>    </strong>
<strong>    def get_queryset(self):</strong>
<strong>        return SearchTerm.objects.all()  </strong>
<strong>        </strong>
<strong>class PageCounts(views.APIView):</strong>

<strong>    permission_classes = (AllowAny,)</strong>
<strong>    def get(self,*args, **kwargs):</strong>
<strong>        searchid=self.kwargs['pk']</strong>
<strong>        reviewpages = Page.objects.filter(searchterm=searchid).filter(review=True)</strong>
<strong>        npos = len([p for p in reviewpages if p.sentiment==1])</strong>
<strong>        nneg = len(reviewpages)-npos</strong>
<strong>        return Response({'npos':npos,'nneg':nneg})</strong>
</pre></div><p><code class="literal">PageCounts</code>类将搜索的ID(电影的标题)作为输入，并返回对电影的<a id="id628" class="indexterm"/>评论的意见，<a id="id627" class="indexterm"/>即正面和负面的计数。要从电影的标题中获取<code class="literal">earchTerm</code>的ID，您可以查看管理界面或者使用其他API端点<code class="literal">SearchTermsList</code>；这只是返回电影标题列表以及相关的ID。串行器设置在<code class="literal">serializers.py</code>文件上:</p><div><pre class="programlisting">
<strong>from pages.models import SearchTerm</strong>
<strong>from rest_framework import serializers</strong>
<strong>        </strong>
<strong>class SearchTermSerializer(serializers.HyperlinkedModelSerializer):</strong>
<strong>    class Meta:</strong>
<strong>        model = SearchTerm</strong>
<strong>        fields = ('id', 'term')</strong>
</pre></div><p>为了调用这些端点，我们可以再次使用swagger接口(参见<a class="link" href="ch06.html" title="Chapter 6. Getting Started with Django">第6章</a>、<em>Django入门</em>)或者在终端中使用<code class="literal">curl</code>命令来进行这些调用。例如:</p><div><pre class="programlisting">
<strong>curl -X GET localhost:8000/search-list/</strong>
<strong>{"count":7,"next":null,"previous":null,"results":[{"id":24,"term":"the martian"},{"id":27,"term":"steve jobs"},{"id":29,"term":"suffragette"},{"id":39,"term":"southpaw"},{"id":40,"term":"vacation"},{"id":67,"term":"the revenant"},{"id":68,"term":"batman vs superman dawn of justice"}]}</strong>
</pre></div><p>和</p><div><pre class="programlisting">
<strong>curl -X GET localhost:8000/pages-sentiment/68/</strong>
<strong>{"nneg":3,"npos":15}</strong>
</pre></div></div>





<title>Summary</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch08lvl1sec59"/>总结</h1></div></div></div><p>在本章中，我们描述了一个电影评论情感分析器web应用程序，以使您熟悉我们在<a class="link" href="ch03.html" title="Chapter 3. Supervised Machine Learning">第3章</a>、<em>监督机器学习</em>、<a class="link" href="ch04.html" title="Chapter 4. Web Mining Techniques">第4章</a>、<em> Web挖掘技术</em>和<a class="link" href="ch06.html" title="Chapter 6. Getting Started with Django">第6章</a>、<em>Django入门</em>中讨论的一些算法和库。</p><p>这是一个旅程的终点:通过阅读本书并使用提供的代码进行实验，您应该已经获得了关于当今商业环境中使用的最重要的机器学习算法的重要实用知识。</p><p>通过阅读这本书，你应该已经准备好使用Python和一些机器学习算法来开发你自己的web应用程序和想法了。当今的现实世界中存在许多与数据相关的挑战性问题，等待着那些能够掌握并应用本书所讨论的材料的人去解决，而你，已经到了这一步，肯定是那些人中的一员。</p></div>
</body></html>