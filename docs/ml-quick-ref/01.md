

# 量化学习算法

我们已经步入了一个时代，在这个时代，我们正在建造智能机器。在基于数学/统计学的智能算法的帮助下，这种聪明或智能被注入到机器中。这些算法使系统或机器能够在没有任何人工干预的情况下自动学习。作为一个例子，今天我们被大量的移动应用所包围。WhatsApp(目前归脸书所有)中最主要的消息应用之一。每当我们在 WhatsApp 的文本框中输入一条信息时，例如，我们输入 *I am...*，我们得到几个单词提示弹出，如*..回家*、*拉胡尔*、*今晚出行*等等。我们能猜出这里发生了什么吗？为什么？出现了多个问题:

*   系统在学习什么？
*   它从哪里学来的？
*   它是如何学习的？

让我们在这一章中回答所有这些问题。

在本章中，我们将讨论以下主题:

*   统计模型
*   学习曲线
*   曲线拟合
*   建模文化
*   过度拟合和正则化
*   培训、验证和测试
*   交叉验证和模型选择
*   自举方法



# 统计模型

统计模型是通过数据和数学或统计获得的真实情况的近似值，在这里充当使能器。这个近似值用于预测一个事件。统计模型只不过是一个数学方程式。

例如，假设我们向银行申请住房贷款。银行问我们什么？他们要求我们做的第一件事是提供许多文件，如工资条、身份证明文件、关于我们将要购买的房子的文件、公用事业账单、我们当前贷款的数量、我们有多少家属等等。所有这些文件只不过是银行用来评估和检查我们信誉的数据:

![](img/50ec8721-c5e9-49c9-8a58-f60aacf91fa8.png)

这意味着你的信用度是工资、贷款数量、家属数量等的函数。我们可以用数学方法得出这个等式或关系。

统计模型是一个数学等式，它是使用特定业务场景的给定数据得出的。

在下一节中，我们将看到模型如何学习，以及模型如何不断变得更好。



# 学习曲线

学习曲线背后的基本前提是，你花在做某事上的时间越多，你往往会做得越好。最终，完成一项任务的时间持续下降。这有不同的名称，如**改进曲线**、**进度曲线**、**启动功能**。

例如，当你开始学习驾驶手动挡汽车时，你会经历一个学习周期。最初，你要格外小心地操作刹车、离合器和齿轮。你必须不断提醒自己何时以及如何操作这些组件。

但是，随着时间的推移，你继续练习，你的大脑会习惯并适应整个过程。随着每一天的过去，你的驾驶会越来越平稳，你的大脑会在没有任何意识的情况下做出反应。这叫做**潜意识智能**。你通过大量的练习达到这个阶段，并从有意识的智能过渡到有循环的潜意识智能。



# 机器学习

让我来定义机器学习及其组成部分，这样当它抛给你时，你就不会被大量的术语迷惑了。

用 Tom Mitchell 的话来说，“如果一个计算机程序在 T 类任务中的性能，如 P 所测量的，随着经验 E 的增加而提高，那么就可以说它从经验 E 中学习了一些任务 T 和性能测量 P。”此外，另一种理论认为，机器学习是在没有明确编程的情况下赋予计算机学习能力的领域。

举个例子，如果一台计算机已经给了诸如， *[(父亲，母亲)，(叔叔，阿姨)，(兄弟，姐妹)]* 这样的案例，基于此，它需要找出*(儿子，)*。也就是说，给定儿子，关联项是什么？为了解决这个问题，计算机程序将仔细检查以前的记录，并在从一个记录跳到另一个记录时，试图理解和学习这些组合的关联和模式。这被称为**学习**，它通过算法发生。记录多了，也就是经验多了，机器变得越来越聪明。

让我们看看机器学习的不同分支，如下图所示:

![](img/171db42c-de53-4ce2-9ec5-5acda2329ca5.png)

我们将对前面的图表解释如下:

*   **监督学习**:在这种类型的学习中，输入变量和输出变量都是我们已知的。在这里，我们应该在输入变量和输出变量之间建立一种关系，学习将以此为基础。它下面有两类问题，如下:
    *   **回归问题**:有连续输出。例如，房价数据集，其中需要根据输入变量(如面积、地区、城市、房间数量等)来预测房价。要预测的价格是一个连续变量。
    *   **分类**:有离散输出。例如，根据工资、性别、家庭成员数量等，预测员工是否会离开某个组织。
*   **无监督学习**:在这类场景中，没有输出变量。我们应该根据给定的所有变量提取一个模式。例如，根据年龄、性别、收入等对客户进行细分。
*   **强化学习**:这是机器学习的一个领域，在其中采取适当的行动来最大化回报。例如，训练一只狗抓住一个球并给它——如果它们执行了这个动作，我们就奖励它；否则，我们会责备他们，导致惩罚。



# 赖特模型

在 Wright 的模型中，学习曲线函数定义如下:

![](img/f534dff2-a5c4-4f11-b694-b14f97180b55.png)

这些变量如下:

*   *Y* :单位累计平均时间
*   *X* :累计生产的单位数
*   *a* :生产第一台设备所需的时间
*   *b* :在图表纸上绘制时的函数斜率*(学习率的对数/2 的对数)*

下面的曲线有一个垂直轴( *y* 轴)代表关于特定工作的学习，水平轴对应于学习所花费的时间。一条陡峭的学习曲线可以被理解为快速进步的标志。下图为**莱特的学习曲线模型**:

![](img/f10a6a22-e064-4c82-9f37-ebe5f7384a35.png)

然而，由此产生的问题是，*它是如何与机器学习联系在一起的？*我们现在将详细讨论这一点。

让我们通过以下步骤来讨论一个恰好是监督学习问题的场景:

1.  我们获取数据，并将其划分为一个训练集(在这个训练集上，我们让系统学习并作为一个模型出现)和一个验证集(在这个验证集上，我们测试系统学习得有多好)。
2.  下一步是获取训练集的一个实例(观察值),并利用它来估计模型。训练集上的模型误差将为 0。
3.  最后，我们会找出验证数据上的模型错误。

*第 2 步*和*第 3 步*通过取多个实例(训练规模)如 10、50 和 100，并研究训练误差和验证误差，以及它们与多个实例(训练规模)的关系来重复。这条曲线——或关系——在机器学习场景中被称为**l**收入曲线。

让我们处理一个合并的发电厂数据集。这些特征包括每小时平均环境变量，即**温度**(**T**)**环境压力**(**AP**)**相对湿度** ( **RH** )和排气**真空度** ( **V** )，以预测工厂每小时净电能输出 ( **PE**

```
# importing all the libraries
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt

#reading the data
data= pd.read_excel("Powerplant.xlsx")

#Investigating the data
print(data.info())
data.head()
```

由此，我们能够看到数据中变量的数据结构:

![](img/05d57d50-1764-4cb0-a172-83bb011dc055.png)

输出如下所示:

![](img/52260431-6fcc-4767-87d9-f47a04b70257.png)

第二个输出让您对数据有很好的感觉。

数据集有五个变量，其中**环境温度** ( **在**)和 PE(目标变量)。

让我们改变数据的训练规模，研究它对学习的影响。为`train_size`创建一个具有不同训练大小的列表，如以下代码所示:

让我们生成`learning_curve`:

我们得到以下输出:

```
# As discussed here we are trying to vary the size of training set
train_size = [1, 100, 500, 2000, 5000]
features = ['AT', 'V', 'AP', 'RH']
target = 'PE'
# estimating the training score & validation score
train_sizes, train_scores, validation_scores = learning_curve(estimator = LinearRegression(), X = data[features],y = data[target], train_sizes = train_size, cv = 5,scoring ='neg_mean_squared_error')
```

![](img/0e15e0c1-657c-495d-a65f-14de600ca4fa.png)

```
# Generating the Learning_Curve
train_scores_mean = -train_scores.mean(axis = 1) 
validation_scores_mean = -validation_scores.mean(axis = 1)
import matplotlib.pyplot as plt 
plt.style.use('seaborn') 
plt.plot(train_sizes, train_scores_mean, label = 'Train_error') 
plt.plot(train_sizes, validation_scores_mean, label = 'Validation_error') 
plt.ylabel('MSE', fontsize = 16) 
plt.xlabel('Training set size', fontsize = 16) 
plt.title('Learning_Curves', fontsize = 20, y = 1) 
plt.legend() 

```

从上图可以看出，当训练规模为 1 时，训练误差为 0，但验证误差超过了 **400** 。

随着我们继续增加训练集的大小(从 1 到 100)，训练误差继续上升。然而，随着模型在验证集上表现得更好，验证误差开始直线下降。训练规模达到 500 大关后，验证误差和训练误差开始收敛。那么，从中可以推断出什么呢？无论培训岗位的规模有多大，模型的性能都不会改变。但是，如果您尝试添加更多功能，可能会有所不同，如下图所示:

![](img/21d6445c-9045-477b-8f6b-6f8203070ac2.png)

上图显示验证和定型曲线已经收敛，因此添加定型数据没有任何帮助。但是，在下图中，曲线尚未收敛，因此添加训练数据将是一个好主意:

![](img/f6ec0074-8426-4428-b50a-f7d40b70373d.png)

曲线拟合

到目前为止，我们已经了解了学习曲线及其意义。然而，只有当我们试图根据可用的数据和特征拟合曲线时，它才会出现。但是曲线拟合是什么意思呢？让我们试着理解这一点。



# 曲线拟合不过是在多个特征和目标之间建立一种关系。它有助于找出特征与目标之间的关联。

建立关系(曲线拟合)无非是提出一个数学函数，它应该能够以一种最适合数据集的方式来解释行为模式。

我们进行曲线拟合有多种原因:

进行系统模拟和优化

确定中间点的值(插值)

*   进行趋势分析(外推)
*   进行假设检验
*   有两种类型的曲线拟合:
*   **精确拟合**:在这种情况下，曲线会通过所有点。在这种情况下，没有残余误差(我们将很快讨论什么被归类为误差)。目前，您可以将误差理解为实际误差和预测误差之间的差异。它可用于插值，主要用于分布拟合。

下图显示了多项式但精确的拟合:

1.  ![](img/324d5cff-68ea-43a5-9285-805f65773b79.png)

下图显示了直线，但完全吻合:

![](img/af0953fd-f87c-4546-a6f8-193044a707eb.png)

**最佳拟合**:曲线不通过所有的点。会有与此相关的残留。

![](img/af0953fd-f87c-4546-a6f8-193044a707eb.png)

2.  **Best fit**: The curve doesn't pass through all the points. There will be a residual associated with this.

让我们看一些不同的场景，并研究它们来理解这些差异。

这里，我们将拟合两个数字的曲线:

由此，我们将获得以下输出:

![](img/e4a9a050-461b-4b09-9b31-ac5dd5c9602e.png)

```
# importing libraries
 import numpy as np
 import matplotlib.pyplot as plt
 from scipy.optimize import curve_fit

# writing a function of Line
 def func(x, a, b):
 return a + b * x 
 x_d = np.linspace(0, 5, 2) # generating 2 numbers between 0 & 5
 y = func(x_d,1.5, 0.7) 
 y_noise = 0.3 * np.random.normal(size=x_d.size)
 y_d = y + y_noise
 plt.plot(x_d, y_d, 'b-', label='data')

 popt, pcov = curve_fit(func, x_d, y_d) # fitting the curve
 plt.plot(x_d, func(x_d, *popt), 'r-', label='fit')
```

这里，我们使用了两个点来拟合直线，我们可以很好地看到它变成了一个**精确拟合**。当介绍三点时，我们会得到以下几点:

![](img/e4a9a050-461b-4b09-9b31-ac5dd5c9602e.png)

运行整个代码并关注输出:

```
 x_d = np.linspace(0, 5, 3) # generating 3 numbers between 0 & 5
```

![](img/c682382b-fb8a-4d93-abc0-99925922b1b8.png)

现在，你可以看到噪声的漂移和影响。它开始呈现出曲线的形状。一条线可能不太适合这里(不过，现在说还为时过早)。它不再是完全合适的了。

如果我们尝试引入 100 分并研究它的影响呢？现在，我们知道如何引入点数。

通过这样做，我们得到以下输出:

![](img/ef332a8e-1123-44ca-bade-917751904b26.png)

这不是一个精确的拟合，而是试图概括整个数据集的最佳拟合。

![](img/ef332a8e-1123-44ca-bade-917751904b26.png)

This is not an exact fit, but rather a best fit that tries to generalize the whole dataset.

残留的；剩余的

残差是观测值或真实值与预测值(拟合值)之间的差值。例如，在下图中，其中一个残差是 **(A-B)** ，其中 **A** 是观测值， **B** 是拟合值:



# ![](img/554e01db-d173-45cc-a834-039d7a111e50.png)

前面的散点图描述了我们正在拟合一条可以代表所有数据点行为的线。然而，值得注意的一点是，这条线并不通过所有的点。大部分点都下线了。

残差的和与均值将始终为 0。 *∑e =0* 和 *e =0* 的平均值。

统计建模——利奥·布雷曼的两种文化

每当我们试图分析数据并最终做出预测时，我们会考虑两种方法，这两种方法都是由伯克利教授 Leo Breiman 在 2001 年发表的题为*统计建模:两种文化*的论文中发现的。



# 任何分析都需要数据。分析可以如下:

![](img/5fa263dc-3aee-4d76-a18c-216f46278c34.png)

一个 **X** ( **特性**)的向量经历了一个自然盒子，它被翻译成一个响应。一个自然框试图在 **X** 和 **Y** 之间建立关系。通常，与此分析相关的目标如下:

**预测**:预测未来输入特征的响应

**信息**:找出并理解响应和驱动输入变量之间的关联

*   Breiman 指出，在解决业务问题时，有两种截然不同的方法:
*   **数据建模文化**:在这种模型中，自然采用随机模型的形式来估计必要的参数。线性回归、逻辑回归和 Cox 模型通常在自然盒子下起作用。这个模型讨论观察数据的模式，并试图设计一个观察到的东西的近似值。根据他们的经验，科学家或统计学家将决定使用哪种模型。这是一个模型先于问题和数据的情况，这个模型的解决方案更倾向于模型的架构。布雷曼说，过度依赖这种方法无助于统计学家应对各种各样的问题。当涉及到寻找与地震预测、降雨预测和全球变暖原因相关的解决方案时，它不会给出准确的结果，因为这种方法不关注准确性，而是关注这两个目标。

**算法建模文化**:在这种方法中，预先设计的算法被用来做更好的近似。在这里，算法使用复杂的数学来得出结论，并在自然的盒子里行动。随着更好的计算能力和使用这些模型，很容易复制驱动因素，因为模型会继续运行，直到它学习和理解驱动结果的模式。它使我们能够解决更复杂的问题，并且更加强调准确性。随着更多的数据通过，它可以给出比数据建模文化更好的结果。

*   培训数据开发数据-测试数据
*   这是构建模型的最重要的步骤之一，它可能会导致许多关于我们是否真的需要所有三个数据集(培训、开发和测试)的辩论，如果需要，这些数据集的分解应该是什么。让我们来理解这些概念。



# 在我们有了足够的数据开始建模后，我们需要做的第一件事就是将数据分成三个部分，即**训练集**、**开发集**、**集**和**测试集**:

![](img/936279c1-3f92-41cf-ae3c-751f3f5afc0f.png)

让我们来看看拥有这三个集合的目标:

**训练集** : 训练集用于训练模型。当我们应用任何算法时，我们都是在拟合训练集中的参数。在神经网络的情况下，找出权重。

假设在一个场景中，我们试图拟合不同次数的多项式:

1.  **Training Set**:The training set is used to train the model. When we apply any algorithm, we are fitting the parameter in the training set. In the case of a neural network, finding out about the weights takes place.

*f(x) = a+ bx* → 1 ^(st) 次多项式

*   *f(x)= a+bx+CX²*→2^(nd)次多项式
    *   *f(x)= a+bx+CX²+dx³→3^(rd)次多项式*
    *   拟合模型后，我们计算所有拟合模型的训练误差:
    *   ![](img/4fb00d4e-01ed-4227-9e89-bd91428ee696.png)

我们不能根据训练误差来评估模型有多好。如果我们这样做，它将把我们引向一个有偏见的模型，可能无法在看不见的数据上表现良好。为了解决这个问题，我们需要进入开发环境。

**开发** **设置**:这也叫**保持设置**或**验证设置**。这一组的目标是调整我们从训练组获得的参数。这也是评估模型性能的一部分。基于它的性能，我们必须采取措施来调整参数。例如，控制学习速率、最小化过拟合以及选择批次中的最佳模型都发生在开发集中。这里，再次计算开发集误差，并且在看到哪个模型给出的误差最小之后，进行模型的调整。在这个阶段给出最小误差的模型仍然需要调整以最小化过度拟合。一旦我们确信了最佳模型，它就被选中了，我们就向测试集前进。

**测试集**:测试集主要用于评估最佳选择的模型。在这个阶段，计算模型的精度，如果模型的精度没有太偏离训练精度和开发精度，我们就发送这个模型进行部署。

2.  培训、开发和测试集的规模

3.  通常，机器学习实践者以 60:20:20 或 70:15:15 的比例选择三个集合的大小。然而，并没有硬性规定开发集和测试集的大小应该相等。下图显示了不同大小的定型集、开发集和测试集:



# ![](img/9123ff4d-f4ae-454d-9636-21f3f0f47e60.png)

三个不同集合的另一个例子如下:

![](img/badde5d1-a170-4982-80a6-f8bb01bdc94c.png)

但是我们有大数据要处理的场景呢？例如，如果我们有 10，000，000 条记录或观察值，我们将如何划分数据？在这样的场景中，ML 实践者获取训练集的大部分数据——多达 98-99%——剩下的数据被划分给开发和测试集。这样做是为了让从业者可以考虑不同种类的场景。因此，即使我们有 1%的数据用于开发，同样的数据用于测试，我们最终都会得到 100，000 条记录，这是一个很好的数字。

偏差-方差权衡

在我们开始建模并试图找出折衷方案之前，让我们从下图中了解一下什么是偏差和方差:



# ![](img/13bf10e0-73cb-42ad-afcb-acd933d0dda9.png)

在偏差-方差权衡中会产生两种类型的误差，如下所示:

**训练误差**:这是在使用训练输入预测输出时，拟合值与实际值的偏差。这个误差主要取决于模型的复杂性。随着模型复杂性的增加，误差似乎直线下降。

**开发误差**:这是预测值与实际值偏差的度量，被开发集用作输入(同时使用根据训练数据训练的相同模型)。这里，预测是基于看不见的数据进行的。我们需要最小化这个误差。最小化这个误差将决定这个模型在实际场景中有多好。

*   随着算法复杂度的增加，训练误差下降。但是，开发误差或验证误差一直下降，直到某一点，然后上升，如下图所示:
*   ![](img/24f381ef-b08b-44e5-9e20-201ac3b1d0cf.png)

上图可以解释如下:

**欠拟合**:由于数据集中现有的变量，每个数据集都有特定的模式和属性。除此之外，它还具有随机和潜在的模式，这是由不属于数据集的变量引起的。无论何时我们提出一个模型，这个模型都应该从现有的变量中学习模式。然而，这些模式的学习也取决于你的算法有多好，多健壮。假设我们选择了一个模型，它甚至不能从数据集中导出基本模式——这被称为**欠拟合**。在前面的图中，这是一个分类的场景，我们试图对 *x* 和 *o* 进行分类。在图 1 中，我们尝试使用线性分类算法对数据进行分类，但我们可以看到这会导致大量的错误分类错误。这是一个不适应的例子。

**过度拟合**:从图 1 开始，我们试图使用复杂的算法找出模式并进行分类。值得注意的是，错误分类误差在第二个图中已经下降，因为这里使用的复杂模型能够检测模式。开发错误(如上图所示)也会减少。我们将增加模型的复杂性，看看会发生什么。图 3 表明现在模型中没有错误分类误差。然而，如果我们看它下面的图，我们可以看到开发误差现在太高了。发生这种情况是因为模型正在从由于数据集中不存在的变量而表现出的误导和随机模式中学习。这意味着它已经开始学习设备中存在的噪音。这种现象称为**过拟合**。

*   偏见:这种情况我们见过多少次了？这发生在我们使用了一个算法，但它不适合的情况下。这意味着这里使用的函数与这个场景没有什么关系，它不能提取正确的模式。这导致了一个称为**偏差**的错误。它的出现主要是由于对数据做了某种假设，并使用了一个可能正确但不正确的模型。例如，如果我们必须对一种情况使用二次多项式，我们将使用简单的线性回归，它不能在响应和解释变量之间建立正确的关系。
*   **方差**:当我们有一个数据集用于训练模型时，模型应该保持免疫，即使我们将训练集更改为来自同一人群的集合。如果数据集中的变化带来了模型性能的变化，它被称为**方差误差**。这是由于模型学习到了噪声(一种无法解释的变化),因此，该模型在看不见的数据上没有给出好的结果:
*   ![](img/c371896e-14ed-449f-a25c-72c9f845cd9f.png)
*   我们将对前面的图表解释如下:

如果**训练误差**下降，(**开发误差** - **训练误差**)上升，则意味着**高方差**情况(上表中的场景 1)

如果**训练误差**和**开发误差**上升，(**开发误差** - **训练误差**)下降，则意味着**高偏差**情况(上表中的场景 2)

*   如果**训练误差**和**开发误差**上升并且(**开发误差** - **训练误差**)也上升，则意味着**高偏差**和**高偏差**(上表中的场景 3)
*   如果**训练误差**上升，**开发误差**下降，即(**开发误差** - **训练误差**)下降，则意味着**低偏**和**低方差**(上表中的场景 4)
*   我们应该始终努力实现第四种情况，这种情况描述了较低的训练误差，以及较低的开发集误差。在上表中，这是我们必须找出偏差方差权衡的地方，用垂直线表示。
*   现在，下面的问题出现了:我们如何才能对付过度拟合？让我们通过进入下一部分来找到这个问题的答案。

正规化

对于机器学习建模，我们现在已经对过度拟合的含义有了相当的理解。只是重申一下，当模型学习已经进入数据的噪声时，它试图学习由于随机机会而发生的模式，因此过度拟合发生了。由于这种现象，模型的泛化能力面临危险，并且在看不见的数据上表现不佳。结果是，模型的准确性急剧下降。



# 我们能打击这种现象吗？答案是肯定的。正规化有助于解决问题。让我们弄清楚它能提供什么以及它是如何工作的。

We have now got a fair understanding of what overfitting means when it comes to machine learning modeling. Just to reiterate, when the model learns the noise that has crept into the data, it is trying to learn the patterns that take place due to random chance, and so overfitting occurs. Due to this phenomenon, the model's generalization runs into jeopardy and it performs poorly on unseen data. As a result of that, the accuracy of the model takes a nosedive.

正则化是一种使模型不会变得复杂以避免过度拟合的技术。

让我们来看看下面的回归方程:

![](img/107b0b1a-755e-4c96-b235-5e4a9a676cf2.png)

对此的损失函数如下:

![](img/69cb5478-5ca8-45e7-9123-c85fd63b62e0.png)

损失函数将有助于调整系数并检索最佳系数。在训练数据中有噪声的情况下，系数不能很好地概括，并且会陷入过拟合。正则化有助于通过使这些估计或系数向 0 下降来消除这种情况。

现在，我们将讨论两种类型的正规化。在后面的章节中，其他类型将被涵盖。

岭回归(L2)

由于岭回归，我们需要对损失函数做一些修改。原始损失函数增加了收缩分量:



# ![](img/c423cec4-f053-4798-8e0e-bfcf116acdcd.png)

现在，这个修改的损失函数需要被最小化以调整估计或系数。这里，lambda 正在调整调整损失函数的参数。也就是说，它决定了应该对模型的灵活性进行多少惩罚。模型的灵活性取决于系数。如果模型的系数上升，灵活性也会上升，这对我们的模型来说不是一个好兆头。同样，随着系数的下降，灵活性受到限制，模型开始表现得更好。每个估计参数的收缩使模型在这里变得更好，这就是岭回归的作用。当λ越来越高，也就是 *λ → ∞* 时，惩罚分量上升，估计开始缩小。但是，当 *λ* *→ 0* 时，罚分量减小，开始变成一个**普通最小二乘** ( **OLS** )用于估计线性回归中的未知参数。

最小绝对收缩和选择算子

**最小绝对收缩和选择算子** ( **套索**)也被称为 *L1* 。在这种情况下，前面的惩罚参数被替换为 *|βj|* :



# ![](img/41f4f28d-b58f-4302-b8b4-39ccc8340d14.png)

通过最小化前面的函数，找到并调整系数。在这种情况下，随着λ变大， *λ → ∞* ，惩罚分量上升，因此估计值开始缩小并变为 0(这在岭回归的情况下不会发生；相反，它将接近于 0)。

交叉验证和模型选择

我们已经讨论过过度拟合。这与模型的稳定性有关，因为模型的真正测试发生在它处理未知的新数据时。一个模型最重要的一个方面是，除了常规模式之外，它不应该拾取噪声。



# 验证只不过是保证模型是作为输入特征而不是噪声的结果的响应和预测之间的关系。模型的一个好的指标不是通过训练数据和误差。这就是为什么我们需要交叉验证。

在这里，我们将坚持使用 k-fold 交叉验证，并了解如何使用它。

k 倍交叉验证

让我们来看一下 k 倍交叉验证的步骤:



# 数据被分成 k 个子集。

保留一组用于测试/开发，模型建立在其余数据上( *k-1* )。也就是说，其余的数据构成了训练数据。

1.  *步骤 2* 重复 k 次。也就是说，一旦执行了前面的步骤，我们就转移到第二个集合，它形成了一个测试集合。剩余的( *k-1* )数据可用于构建模型:
2.  ![](img/b90b29ab-dfe7-4c11-9a2f-321e84f79495.png)

3.  4.计算误差，并对所有 k 次试验取平均值。

每个子集都有一次机会成为验证/测试集，因为大多数数据都用作训练集。这有助于减少偏差。同时，几乎所有的数据都被用作验证集，这减少了方差。

如上图所示，已经选择了 *k = 5* 。这意味着我们必须将整个数据集分成五个子集。在第一次迭代中，子集 5 成为测试数据，其余的成为训练数据。同样，在第二次迭代中，子集 4 变成测试数据，其余的变成训练数据。这将持续五次迭代。

现在，让我们尝试在 Python 中使用 K neighbors 分类器来分割训练和测试数据:

使用交叉验证的模型选择

通过使用以下代码，我们可以利用交叉验证来找出哪个模型执行得更好:

```
from sklearn.datasets import load_breast_cancer # importing the dataset
from sklearn.cross_validation import train_test_split,cross_val_score # it will help in splitting train & test
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics

BC =load_breast_cancer() 
X = BC.data
y = BC.target

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
print(metrics.accuracy_score(y_test, y_pred))

knn = KNeighborsClassifier(n_neighbors=5)
scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')
print(scores)
print(scores.mean())
```



# 10 重交叉验证如下:

引导中的 0.632 规则

```
knn = KNeighborsClassifier(n_neighbors=20)
print(cross_val_score(knn, X, y, cv=10, scoring='accuracy').mean())
```

在我们进入自举的 0.632 规则之前，我们需要理解什么是自举。Bootstrapping 是这样一个过程，其中使用由 *n* 个观察值组成的总体中的替换值来执行随机采样。在这种情况下，样本可以有重复的观察值。例如，如果总体是(2，3，4，5，6)并且我们正在尝试抽取两个大小为 4 的随机样本进行替换，那么样本 1 将是(2，3，3，6)，样本 2 将是(4，4，6，2)。

```
# 10-fold cross-validation with logistic regression
from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
print(cross_val_score(logreg, X, y, cv=10, scoring='accuracy').mean())
```



# 现在，让我们深入研究 0.632 法则。

我们已经看到，在使用预测时，训练误差的估计是 *1/n ∑L(y [i，] y-hat)* 。这就是损失函数:

![](img/dfea359d-89d5-48f1-93e5-2d6bf1a50c7c.png)

交叉验证是一种估计样本误差预期输出的方法:

![](img/5863896c-0524-4013-a73a-cd6b176f4183.png)

但是，在 k 倍交叉验证的情况下，如下所示:

![](img/baff7e70-d144-4371-a088-6ffd520061d0.png)

这里，训练数据是 *X=(x1，x2.....，xn)* 并且我们从这个集合 *(z1，.....，zb)* 其中每个*子*是一组 *n* 样本。

在这种情况下，以下是我们的样本外误差:

![](img/5571d600-2600-4985-a2d5-ae703b7eceec.png)

这里， *fb(xi)* 是来自模型的 *xi* 处的预测值，该模型已被拟合到引导数据集。

不幸的是，这不是一个特别好的估计量，因为用于产生 *fb(xi)* 的自举样本可能包含 *xi* 。OOSE 解决了过拟合问题，但仍有偏差。这种偏差是由于替换抽样导致的自举样本中不明显的观察结果造成的。每个样本中不同观察值的平均数量约为 *0.632n* 。为了解决偏差问题，Efron 和 Tibshirani 提出了 *0.632* 估计量:

![](img/ff94ac0d-9c57-454c-9943-1fb1983f5632.png)

模型评估

让我们看看目前正在使用的一些模型评估技术。



# 混淆矩阵

混淆矩阵是一个帮助评估分类模型好坏的表格。当真实值/标签已知时使用。大多数数据科学领域的初学者被混淆矩阵吓倒，认为它看起来比实际更难理解；让我告诉你——这非常简单容易。



# 让我们通过一个例子来理解这一点。假设我们已经建立了一个分类模型，预测客户是否愿意购买某种产品。为此，我们需要根据看不见的数据来评估模型。

有两类:

**是**:顾客会购买产品

**否**:顾客不会购买该产品

*   由此，我们将矩阵放在一起:
*   ![](img/d2619d94-1b89-4ec3-b363-e816077f5e39.png)

从前面的矩阵中我们第一眼能得出什么推论？

该分类器总共进行了 80 次预测。这意味着总共有 80 名顾客接受了测试，以确定他/她是否会购买该产品。

顾客买了产品，而 T2 没有。

*   分类器预测 **56** 顾客会购买产品，而 **24** 不会:
*   ![](img/0b20dc6e-24bf-4e3b-bd6a-538c81b2a894.png)
*   与混淆矩阵相关的不同术语如下:

**真阳性(TP)** :这些情况下，我们预测客户会购买产品，他们也购买了。

**真阴性(TN)** :这些是我们预测客户不会购买产品，而他们也没有购买的情况。

*   **假阳性(FP)** :我们预测*是的顾客会购买产品*，但是他们没有。这就是所谓的*类型 1* 错误。
*   **假阴性(FN)** :我们预测*没有*，但是顾客购买了产品。这就是所谓的*类型 2* 错误。
*   现在，我们来谈谈评估分类模型所需的一些指标:
*   **精度**:测量分类器的整体精度。为了计算这个，我们将使用以下公式:*(TP+TN)/总病例数*。在前面的场景中，精度是(50+20)/80，结果是 0.875。因此，我们可以说这个分类器将在 87.5%的情况下正确预测。

**误分类率**:这是衡量分类器得出错误结果的频率。公式*(FP+FN)/总病例数*将给出结果。在前面的场景中，误分类率为 *(6+4)/80* ，为 0.125。因此，在 12.5%的情况下，它不会产生正确的结果。也可以计算为(1-精度)。

*   **TP 率**:这是一个衡量它预测*是*作为答案的几率，答案实际上是*是*。计算这个的公式是 **TP/(实际:是)**。在这个场景中， *TPR = (50/54)= 0.92* 。也叫**灵敏度**或**回忆**。
*   **FP rate** :这是一个衡量当实际答案为*否*时，它预测*是*的概率。计算该比率的公式为 **FP/(实际:否)**。对于前面的例子， *FPR = (6/26)= 0.23* 。
*   TN 比率:这是一个衡量当答案实际上是*否*时，它预测到*否*的几率。计算这个的公式是 *TN/(实际:否)*。在这种情况下， *TNR= (20/26)= 0.76* 。也可以使用(1-FPR)来计算。也叫**特异性**。
*   **精度**:这是对所有*是*预测中*是*的预测正确性的衡量。它会找出在所有*是*的预测中，有多少次*是*的预测是正确的。计算这个的公式是 *TP/(预测:是)*。这里，*精度= (50/56)=0.89* 。
*   **发生率**:这是对总样本中有多少*是*的测量。公式为*(实际:是/总样本)*。这里，这是 *54/80 = 0.67* 。
*   **零错误率**:这是一个衡量分类器如果只预测多数类会有多错误的指标。公式为*(实际:无/总样本)*。这里，这是 *26/80=0.325* 。
*   **Cohen 的 Kappa 值**:这是一个衡量分类器的表现与它的表现相比有多好的指标。
*   **F-Score** :这是查全率和查准率的调和平均值，即*(2 *查全率*查准率)/(查全率+查准率)*。它认为查全率和查准率都是模型评价的重要指标。F 值的最佳值是 1，此时查全率和查准率最高。F 值的最差值为 0。分数越高，模型越好:
*   ![](img/c1c5e09b-fc65-4721-92c0-474cf4b1ff56.png)
*   接受者操作特征曲线

我们遇到过许多初露头角的数据科学家，他们会构建一个模型，以评估的名义，仅仅满足于总体准确性。然而，这不是评估模型的正确方法。例如，假设有一个数据集，它的响应变量有两个类别:愿意购买产品的客户和不愿意购买产品的客户。假设数据集有 95%的客户不愿意购买该产品，有 5%的客户愿意购买。假设分类器能够正确预测多数类而不是少数类。所以，如果有 100 个观察值， *TP=0* ， *TN= 95* ，其余的分类错误，这仍然会导致 95%的准确率。然而，得出这是一个好模型的结论是不正确的，因为它根本不能对少数民族进行分类。



# 因此，我们需要超越准确性，以便对模型有更好的判断。在这种情况下，回忆、特异性、精确性和**受试者操作特征** ( **ROC** )曲线就来拯救了。在上一节中，我们学习了召回率、特异性和精确度。现在，让我们了解什么是 ROC 曲线。

大多数分类器产生 0 到 1 之间的分数。下一步是我们设置阈值，基于这个阈值，决定分类。通常，0.5 是阈值，如果大于 0.5，则创建一个类 1，如果阈值小于 0.5，则属于另一个类 2:

![](img/46bc9f8a-2b3c-4985-8b09-17bafb7d919a.png)

对于 ROC 来说，在 **0.0** 和 **1.0** 之间的每一点都被当作一个阈值，所以阈值线从 **0.0** 一直移动到 **1.0** 。阈值将导致我们具有 TP、TN、FP 和 FN。在每个阈值，计算以下指标:

*真阳性率= TP/(TP+FN)*

*真阴性率= TN/(TN + FP)*

*   *假阳性率= 1-真阴性率*

*   (TPR 和 FPR)的计算从 0 开始。当阈值线为 0 时，我们将能够对所有愿意购买的客户进行分类(阳性案例)，而那些不愿意购买的客户将被错误分类，因为会有太多的假阳性。这意味着阈值线将从零开始向右移动。随着这种情况的发生，假阳性开始下降，而真阳性将继续增加。

*   最后，我们需要在阈值的每一点计算 TPR 和 FPR 之后，绘制出它们的关系图:

![](img/d6092b1e-d657-46ef-9d5e-a8b25de6f5c5.png)

Finally, we will need to plot a graph of the TPR versus FPR after calculating them at every point of the threshold:

![](img/d6092b1e-d657-46ef-9d5e-a8b25de6f5c5.png)

红色斜线代表随机分类，即没有模型的分类。完美的 ROC 曲线将沿着 *y* 轴，并且将呈现绝对三角形的形状，其将穿过 *y* 轴的顶部。

ROC 下的区域

为了评估模型/分类器，我们需要确定 ROC ( **AUROC** )下的**区域。FPR 和 TPR 的最大值为 1 时，该图的整个面积为 1——这里两者都为 1。因此，它呈正方形。随机线完美地定位成 45 度，将整个区域分割成两个对称的等边三角形。这意味着红线下方和上方的面积为 0.5。最佳和完美的分类器将是试图获得 AUROC 为 1 的分类器。AUROC 越高，模型越好。**



# 在你有多个分类器的情况下，你可以使用 AUROC 来决定哪一个是最好的。

h-测度

二进制分类必须应用技术，以便能够将独立变量映射到不同的标签。例如，存在许多变量，如性别、收入、现有贷款数量和按时/不按时付款，这些变量被映射以产生一个分数，该分数帮助我们将客户分类为好客户(更倾向于支付)和坏客户。



# 通常，每个人似乎都被错误分类率或衍生形式所困扰，因为曲线 ( **AUC** )下的**区域被认为是我们分类模型的最佳评估者。您可以通过将错误分类的示例总数除以示例总数来获得该比率。但这能给我们一个公正的评价吗？让我们看看。在这里，我们有一个错误的分类率，让一些重要的东西隐藏起来。通常，分类器会提出一个调整参数，其副作用往往是有利于假阳性而不是假阴性，反之亦然。此外，选择 AUC 作为唯一的模型评估者对我们来说是双重打击。AUC 对于不同的分类器有不同的误分类代价，这是不希望的。这意味着使用它相当于使用不同的度量来评估不同的分类规则。**

正如我们已经讨论过的，对任何分类器的真正测试都是在看不见的数据上进行的，这给模型造成了一些小数点的损失。相反，如果我们遇到了类似前面的情况，决策支持系统将不能很好地运行。它将开始产生误导性的结果。

Typically, everyone seems to be caught up with the misclassification rate or derived form since the **area under curve** (**AUC**) is known to be the best evaluator of our classification model. You get this rate by dividing the total number of misclassified examples by the total number of examples. But does this give us a fair assessment? Let's see. Here, we have a misclassification rate that keeps something important under wraps. More often than not, classifiers come up with a tuning parameter, the side effect of which tends to be favoring false positives over false negatives, or vice versa. Also, picking the AUC as sole model evaluator can act as a double whammy for us. AUC has got different misclassification costs for different classifiers, which is not desirable. This means that using this is equivalent to using different metrics to evaluate different classification rules.

H-measure 克服了不同分类器产生不同误分类代价的情况。它需要一个严重性比率作为输入，该比率检查错误分类 0 类实例比错误分类 1 类实例严重多少:

*严重性比率=成本 _ 0/成本 _1*

这里， *cost_0 > 0* 是将 0 类数据点误分类为 1 类的代价。

有时考虑归一化成本 *c =成本 _0/(成本 _0 +成本 _1)* 会更方便。例如， *severity.ratio = 2* 意味着假阳性的成本是假阴性的两倍。

降维

让我们来讨论一个场景，在这个场景中，我们从一家银行获得了一个数据集，它具有与银行客户相关的特征。这些特征包括客户的收入、年龄、性别、支付行为等等。一旦您查看了数据维度，就会发现有 850 个特性。你应该建立一个模型来预测客户谁会违约，如果贷款给了。你会考虑所有这些特征并建立模型吗？



# 答案应该是明确的**否**。数据集中的要素越多，模型越有可能过度拟合。尽管拥有更少的特征并不能保证不会发生过度拟合，但它降低了发生过度拟合的可能性。不错的交易，对吧？

降维是解决这一问题的方法之一。这意味着特征空间中维数的减少。

有两种方法可以实现这一点:

**特征剔除**:这是一个剔除不能给模型增加价值的特征的过程。这样做使得模型非常简单。从奥卡姆剃刀理论中我们知道，在构建模型时，我们应该力求简单。但是，执行此步骤可能会导致信息丢失，因为这些变量的组合可能会对模型产生影响。

**特征提取**:这是一个我们创建新的独立变量的过程，这些变量是现有变量的组合。基于这些变量的影响，我们要么保留，要么放弃。

*   **Feature elimination**: This is a process in which features that are not adding value to the model are rejected. Doing this makes the model quite simple. We know from Occam's Razor that we should strive for simplicity when it comes to building models. However, doing this step may result in the loss of information as a combination of such variables may have an impact on the model.
*   主成分分析是一种特征提取技术，它将所有变量考虑在内，并形成变量的线性组合。稍后，最不重要的变量可以被丢弃，而该变量最重要的部分被保留。

新形成的变量(组件)相互独立，这对于数据分布是线性可分的模型构建过程来说是一个福音。线性模型的基本假设是变量相互独立。

为了理解 PCA 的功能，我们必须熟悉几个术语:

**方差**:这是平均值的均方差。它也被称为**分布**，用于测量数据的可变性:

![](img/7a19ac0f-a890-4f5e-afd1-ed3d108cbf02.png)

*   这里， *x* 是平均值。

**协方差**:这是两个变量向同一方向移动的程度的度量:

![](img/ad894e3b-abe7-4f79-9626-915f5215f0db.png)

*   在主成分分析中，我们发现数据的模式如下:在用维度 *n* 表示的数据集具有高协方差的情况下，我们用相同的 *n* 维度的线性组合来表示那些维度。这些组合相互正交，这就是它们相互独立的原因。此外，维度遵循方差排序。先来顶配组合。

让我们通过讨论以下步骤来了解 PCA 的工作原理:

让我们将我们的数据集分成 *Y* 和 *X* 集合，并且只关注 *X* 。

取一个矩阵 *X* 并标准化，平均值为 0，标准差为 *1* 。我们把新矩阵叫做 *Z* 。

1.  让我们现在开始研究 T21 Z。我们要转置它，把转置后的矩阵乘以 *Z* 。通过这样做，我们得到了协方差矩阵:
2.  *协方差矩阵= Z ^T Z*
3.  Let's work on *Z* now. We have to transpose it and multiply the transposed matrix by *Z*. By doing this, we have got our covariance matrix:

*Covariance Matrix = Z^TZ*

现在我们需要计算 *Z ^T Z* 的特征值及其对应的特征向量。典型地，协方差矩阵到 *PDP* ⁻的本征分解被完成，其中 *P* 是本征向量的矩阵，并且 *D* 是对角线上具有本征值并且在其他任何地方都是 0 值的对角矩阵。

取特征值 *λ₁* 、 *λ₂* 、…、 *λp* 从大到小排序。这样做时，相应地对 *P* 中的特征向量进行排序。称这个特征向量排序矩阵 *P** 。

4.  计算*Z **=***。这个新矩阵， *Z** ，是 *X* 的中心化/标准化版本，但现在每个观察值都是原始变量的组合，其中权重由特征向量确定。另外，因为我们在 *P** 中的特征向量彼此独立，所以 *Z** 的列也彼此独立。
5.  摘要
6.  在本章中，我们学习了统计模型、学习曲线和曲线拟合。我们还研究了 Leo Breiman 介绍的两种文化，这两种文化描述了任何分析都需要数据。我们检查了不同类型的培训、开发和测试数据，包括它们的大小。我们学习了正则化，正则化解释了机器学习建模中过拟合的含义。



# 本章还解释了交叉验证和模型选择，自举中的 0.632 规则，以及 ROC 和 AUC 的深度。

在下一章，我们将学习评估核学习，这是机器学习中最广泛使用的方法。

This chapter also explained cross validation and model selection, the 0.632 rule in bootstrapping, and also ROC and AUC in depth.

In the next chapter, we will study evaluating kernel learning, which is the most widely used approach in machine learning.