

# 四、监督学习算法——预测年收入

概观

在这一章中，我们将看看用于分类的三种不同的监督学习算法。我们还将使用这些算法解决一个监督学习分类问题，并通过比较三种不同算法的结果进行误差分析。

在本章结束时，你将能够识别出具有最佳性能的算法。

# 简介

在前一章中，我们介绍了处理监督学习数据问题的关键步骤。正如在前一章所解释的，这些步骤旨在创建高性能的算法。

本章重点介绍如何将不同的算法应用于真实数据集，其基本目标是应用我们之前学到的步骤，为案例研究选择性能最佳的算法。考虑到这一点，您将预处理和分析一个数据集，然后使用不同的算法创建三个模型。这些模型将相互比较，以衡量它们的性能。

我们将使用的人口普查收入数据集包含人口统计和财务信息，可用于尝试和预测个人的收入水平。通过创建一个能够预测新观察结果的模型，将有可能确定一个人是否可以预先批准接受贷款。

# 探索数据集

现实生活中的应用对巩固知识至关重要。因此，本章包含一个涉及分类任务的真实案例研究，其中将应用您在上一章中了解到的关键步骤来选择性能最佳的模型。

为了实现这一点，将使用人口普查收入数据集，该数据集可在加州大学欧文分校机器学习知识库中获得。

注意

将在下一节以及本章的活动中使用的数据集可以在本书位于 https://packt.live/2xUGShx[的 GitHub 资源库中找到。](https://packt.live/2xUGShx)

引用:Dua，d .和 Graff，C. (2019)。http://archive.ics.uci.edu/ml 的 UCI 机器学习库。加州欧文:加州大学信息与计算机科学学院。

您可以从这本书的 GitHub 资源库下载数据集。或者，要从原始源下载数据集，请按照下列步骤操作:

1.  访问以下链接:[http://archive.ics.uci.edu/ml/datasets/Census+Income](http://archive.ics.uci.edu/ml/datasets/Census+Income)。
2.  首先，点击`Data Folder`链接。
3.  For this chapter, the data available under `adult.data` will be used. Once you click this link, the download will be triggered. Save it as a `.csv` file.

    注

    打开文件，在每一列上添加标题名称，使预处理更容易。例如，根据数据集中可用的特性，第一列应该有标题`Age`。这些可以在前面的链接中的`Attribute Information`下看到。

## 了解数据集

如前几章所述，要构建精确拟合数据的模型，理解数据集的不同细节非常重要。

首先，修改可用的数据，以了解数据集的大小和要开发的监督学习任务的类型:分类或回归。接下来，研究的目的应该明确，即使是显而易见的。对于监督学习，目的与类别标签密切相关。最后，对每个特征进行分析，这样我们就可以知道它们的类型，以便进行预处理。

人口普查收入数据集是关于成年人的人口统计数据的集合，是从美国 1994 年人口普查数据库中提取的。本章仅使用`adult.data`链接下的可用数据。数据集由 32，561 个实例、14 个要素和 1 个二进制类标签组成。考虑到类别标签是离散的，我们的任务是实现不同观测值的分类。

注意

以下对数据集的探索不需要任何类型的编码，而是通过在 Excel 或类似程序中打开数据集进行简单的评估。

通过快速评估数据，可以观察到一些要素以问号的形式显示缺失值。这在处理在线可用的数据集时很常见，应该通过用空值(而不是空格)替换符号来处理。缺少值的其他常见形式是`NULL`值和破折号。

要在 Excel 中编辑丢失的数值符号，使用**替换**功能，如下所示:

*   `?`)。
*   **替换为**:留空(不要输入空格)。

这样，一旦我们将数据集导入到代码中，NumPy 就能够找到缺失的值，以便处理它们。

该数据集的预测任务包括确定一个人的年收入是否超过 5 万美元。据此，两个可能的结果标签是`>50K`(大于 50K)或`<=50K`(小于或等于 50K)。

下表显示了数据集中每个要素的简要说明:

![Figure 4.1: Dataset feature analysis
](img/B15781_04_01.jpg)

图 4.1:数据集要素分析

注意

*出版商注意:性别和种族会影响个人在本研究进行之日的收入潜力。然而，为了本章的目的，我们决定从我们的练习和活动中排除这些类别。

我们认识到，由于偏见和歧视性做法，不可能将性别、种族、教育和职业机会等问题分开。在这些工作的预处理阶段从我们的数据集中移除某些特征并不是为了忽视这些问题，也不是为了忽视在民权领域工作的组织和个人所做的有价值的工作。

我们强烈建议您考虑数据及其使用方式的社会政治影响，并考虑如何通过使用历史数据将偏见引入新算法来延续过去的偏见。

从上表可以得出以下结论:

*   五个特征与研究无关:`fnlwgt`、`education`、`relationship`、`race`和`sex`。在我们继续预处理和训练模型之前，必须从数据集中删除这些要素。
*   在剩下的特征中，有四个是定性值。考虑到许多算法没有将定性特征考虑在内，这些值应该用数字形式表示。

使用我们在前面章节中学到的概念，前面的语句，以及处理异常值和缺失值的预处理过程，都可以处理。以下步骤解释了该过程的逻辑:

1.  您需要导入数据集并删除与研究无关的要素。
2.  您应该检查丢失的值。考虑到具有最多缺失值的特征(`occupation`，具有 1，843 个缺失值)，没有必要删除或替换缺失值，因为它们仅占整个数据集的 5%或更少。
3.  您必须将定性值转换成数字表示。
4.  您应该检查异常值。在使用三个标准差来检测异常值时，具有最大异常值数量的特征是`capital-loss`，其包含 1470 个异常值。同样，异常值代表整个数据集的不到 5%，这意味着它们可以保持不变，而不会影响模型的结果。

前面的过程会将原始数据集转换为具有 32，561 个实例的新数据集(因为没有删除任何实例)，但具有 9 个要素和一个分类标签。所有值都应该是数字形式。使用 pandas 的`to_csv`函数将预处理的数据集保存到一个文件中，如下面的代码片段所示:

```
preprocessed_data.to_csv("census_income_dataset_preprocessed.csv")
```

前面的代码片段获取存储在 Pandas DataFrame 中的预处理数据，并将其保存到一个 CSV 文件中。

注意

请确保您执行了前面的预处理步骤，因为这是将在本章的不同活动中用于训练模型的数据集。

要回顾这些步骤，请访问本书的 GitHub 资源库，在名为`Chapter04`的文件夹下，在名为`Census income dataset preprocessing`的文件中。

# 朴素贝叶斯算法

**朴素贝叶斯**是一种基于**贝叶斯定理**的分类算法，该定理*天真地*假设特征之间相互独立，并为所有特征分配相同的权重(重要程度)。这意味着该算法假设没有单个特征与另一个特征相关或影响另一个特征。例如，尽管在预测一个人的年龄时，体重和身高在某种程度上是相关的，但该算法假设每个特征都是独立的。此外，该算法认为所有功能同等重要。例如，即使教育程度对一个人收入的影响可能比这个人有多少孩子更大，算法仍然认为这两个特征同等重要。

注意

贝叶斯定理是一个计算条件概率的数学公式。要了解这个定理的更多信息，请访问以下网址:[https://plato.stanford.edu/entries/bayes-theorem/](https://plato.stanford.edu/entries/bayes-theorem/)。

虽然现实生活中的数据集包含的特征并不同等重要，也不独立，但这种算法在科学家中很受欢迎，因为它在大型数据集上表现得令人惊讶。此外，由于该算法的简单方法，它运行迅速，从而允许它应用于需要实时预测的问题。此外，它经常用于文本分类，因为它通常优于更复杂的算法。

## 朴素贝叶斯算法是如何工作的？

该算法将输入数据转换为每个类别标签相对于每个要素的出现次数摘要，然后在给定要素组合的情况下，使用该摘要计算一个事件(类别标签)的可能性。最后，这种可能性相对于其他类别标签的可能性被标准化。结果是实例属于每个类标签的概率。概率的总和必须是 1，并且具有较高概率的类标签是算法选择作为预测的类标签。

例如，让我们看看下表中的数据:

![Figure 4.2: Table A - Input data and Table B - Occurrence count](img/B15781_04_02.jpg)

图 4.2:表 A -输入数据和表 B -出现次数

表 A 显示了提供给算法以构建模型的数据。表 B 指的是算法隐式使用来计算概率的出现计数。

在给定一组特征的情况下，为了计算事件发生的可能性，该算法将事件发生的概率(给定每个单独的特征)乘以事件发生的概率，与其余特征无关，如下所示:

```
Likelihood [A1|E] = P[A1|E1] * P[A1|E2] * … * P[A1|En] * P[A1]
```

这里， *A* 1 指的是一个事件(其中一个类标签)，而 *E* 表示特征集，其中 *E* 1 是数据集中的第一个特征， *E* n 是最后一个特征。

注意

这些概率的相乘只能通过假设特征之间的独立性来实现。

对所有可能的结果(所有类别标签)计算上述等式，然后按如下方式计算每个结果的归一化概率:

![Figure 4.3: Formula to calculate normalized probability
](img/B15781_04_03.jpg)

图 4.3:计算标准化概率的公式

对于*图 4.2* 中的例子，给定一个天气等于*晴朗*温度等于*凉爽*的新实例，概率的计算如下:

![Figure 4.4: Calculation of the likelihood and probabilities for the example dataset
](img/B15781_04_04.jpg)

图 4.4:示例数据集的可能性和概率的计算

通过查看前面的等式，可以得出预测应该是*是*的结论。

值得一提的是，对于连续特征，通过创建范围来完成出现次数的汇总。例如，对于价格的特征，该算法可以计算价格低于 100K 的实例的数量，以及价格高于 100K 的实例的数量。

此外，如果特征的一个值从未与结果之一相关联，则该算法可能会遇到一些问题。这是一个问题，主要是因为给定该特征的结果的概率将为零，这会影响整个计算。在前面的示例中，为了预测天气等于*温和*且温度等于*凉爽*的实例的结果，在给定特征集的情况下，*没有*的概率将等于零，考虑到在给定*温和*天气的情况下，*没有*的概率计算为零，因为当结果为*没有*时，没有出现*温和*天气。

为了避免这种情况，应该使用**拉普拉斯估计器**技术。在此，表示给定特征的事件发生概率的分数*P[A | E*1*】*通过将分子加 1 同时将该特征的可能值的数量加到分母来修改。

对于本例，要使用拉普拉斯估计器对天气等于*温和*且温度等于*凉爽*的新实例进行预测，将按如下方式进行:

![Figure 4.5: Calculation of the likelihood and probability using the Laplace estimator for the example dataset
](img/B15781_04_05.jpg)

图 4.5:使用拉普拉斯估计器计算样本数据集的可能性和概率

这里，考虑到*温和*的天气，计算*是*的发生率的分数从 2/7 变为 3/10，这是分子加 1 和分母加 3(对于*晴天*、*温和*和*雨天*)的结果。在给定特征的情况下，计算事件概率的其他分数也是如此。请注意，计算独立于任何特征的事件发生概率的分数保持不变。

然而，正如您到目前为止所了解的，scikit-learn 库允许您训练模型，然后使用它们进行预测，而不需要硬编码数学。

## 练习 4.01:应用朴素贝叶斯算法

现在，让我们将朴素贝叶斯算法应用于生育数据集，该数据集旨在确定个体的生育水平是否受到其人口统计数据、环境条件和既往医疗状况的影响。按照以下步骤完成本练习:

注意

对于本章中的练习和活动，您需要在系统上安装 Python 3.7、NumPy、Jupyter、Pandas 和 scikit-learn。

1.  Download the Fertility dataset from [http://archive.ics.uci.edu/ml/datasets/Fertility](http://archive.ics.uci.edu/ml/datasets/Fertility). Go to the link and click on `Data Folder`. Click on `fertility_Diagnosis.txt`, which will trigger the download. Save it as a `.csv` file.

    注意

    数据集也可以在本书位于 https://packt.live/39SsSSN 的 GitHub 知识库中找到。

    它是从加州大学欧文分校的机器学习知识库下载的:大卫·吉尔、何塞·路易斯·吉雷拉、华金·德·胡安、m·何塞·戈麦斯·托雷斯和马格努斯·约翰松。*用人工智能方法预测精液质量*。专家系统及其应用。

2.  打开一个 Jupyter 笔记本来实现这个练习。导入熊猫，以及 scikit-learn 的`naive_bayes`模块中的`GaussianNB`类:

    ```
    import pandas as pd from sklearn.naive_bayes import GaussianNB
    ```

3.  读取您在第一步中下载的`.csv`文件。考虑到数据集不包含标题行:

    ```
    data = pd.read_csv("fertility_Diagnosis.csv", header=None)
    ```

    ，确保将等于`None`的`header`参数添加到`read_csv`函数中
4.  将数据拆分为`X`和`Y`，考虑在索引等于 9 的列下找到类标签。使用下面的代码来做到这一点:

    ```
    X = data.iloc[:,:9] Y = data.iloc[:,9]
    ```

5.  Instantiate the `GaussianNB` class that we imported previously. Next, use the `fit` method to train the model using `X` and `Y`:

    ```
    model = GaussianNB()
    model.fit(X, Y)
    ```

    运行该脚本的输出如下:

    ```
    GaussianNB(priors=None, var_smoothing=1e-09)
    ```

    这表明该类的实例化是成功的。括号内的信息表示该类接受的参数所使用的值，这些值就是超参数。

    例如，对于`GaussianNB`类，可以设置先验概率来考虑每个类标签和稳定方差的平滑参数。尽管如此，该模型在没有设置任何参数的情况下被初始化，这意味着它将使用每个参数的默认值，对于`priors`的情况是`None`，对于平滑超参数是`1e-09`。

6.  Finally, perform a prediction using the model that you trained before, for a new instance with the following values for each feature: `−0.33`, `0.69`, `0`, `1`, `1`, `0`, `0.8`, `0`, `0.88`. Use the following code to do so:

    ```
    pred = model.predict([[-0.33,0.69,0,1,1,0,0.8,0,0.88]])
    print(pred)
    ```

    注意，我们将值放在双方括号内，考虑到`predict`函数将预测值作为数组的数组，其中第一组数组对应于要预测的新实例的列表，第二组数组引用每个实例的特征列表。

    上述代码片段的输出如下:

    ```
    ['N']
    ```

    该受试者的预测类别等于`N`，这意味着该受试者的生育能力没有受到影响。

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/2Y2wW0c](https://packt.live/2Y2wW0c)。

    你也可以在 https://packt.live/3e40LTt 在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

您已经成功地训练了一个朴素贝叶斯模型，并对一个新观察值执行了预测。

## 活动 4.01:为我们的人口普查收入数据集训练一个朴素贝叶斯模型

要在真实数据集上测试不同的分类算法，请考虑以下场景:您为一家银行工作，他们决定实现一个能够预测一个人的年收入并使用该信息来决定是否批准贷款的模型。您将获得一个包含 32，561 个合适观测值的数据集，您已经对其进行了预处理。您的工作是在数据集上训练三个不同的模型，并确定哪一个最适合案例研究。要建立的第一个模型是高斯朴素贝叶斯模型。使用以下步骤完成本练习:

1.  在 Jupyter 笔记本中，导入所有必需的元素来加载和拆分数据集，以及训练朴素贝叶斯算法。
2.  Load the pre-processed Census Income dataset. Next, separate the features from the target by creating two variables, `X` and `Y`.

    注意

    预处理过的人口普查收入数据集可以在本书位于 https://packt.live/2JMhsFB 的 GitHub 知识库中找到。它由本章开始时经过预处理的转换后的人口普查收入数据集组成。

3.  Divide the dataset into training, validation, and testing sets, using a split ratio of 10%.

    注意

    当所有三个集合都是从相同的数据集创建时，不需要创建额外的训练/开发集合来测量数据不匹配。此外，请注意，考虑到上一章中解释的百分比并不是一成不变的，尝试不同的分割比率是可以的。尽管它们往往工作得很好，但在构建机器学习模型时，接受不同层次的实验是很重要的。

4.  使用`fit`方法在训练集(`X_train`和`Y_train`)上训练一个朴素贝叶斯模型。
5.  Finally, perform a prediction using the model that you trained previously, for a new instance with the following values for each feature: `39`, `6`, `13`, `4`, `0`, `2174`, `0`, `40`, `38`.

    对个人的预测应该等于零，这意味着个人的收入很可能少于或等于 50K。

    注意

    本章中的所有活动都使用同一个 Jupyter 笔记本，这样您就可以对同一数据集的不同模型进行比较。

    这项活动的解决方案可在第 236 页找到。

# 决策树算法

**决策树算法**基于类似树状结构的序列执行分类。它的工作原理是将数据集分成小的子集，作为开发决策树节点的指南。这些节点可以是决策节点，也可以是叶节点，前者代表一个问题或决策，后者代表做出的决策或最终结果。

## 决策树算法是如何工作的？

考虑到我们刚刚提到的，决策树根据决策节点中定义的参数不断地分割数据集。决策节点有分支，每个决策节点可以有两个或更多分支。分支代表定义数据分割方式的不同可能答案。

例如，考虑下面的表格，该表格根据一个人的年龄、最高教育程度和当前收入显示他是否有未偿还的学生贷款:

![Figure 4.6: Dataset for student loans
](img/B15781_04_06.jpg)

图 4.6:学生贷款数据集

下图显示了基于前面的数据构建的决策树的一种可能配置，其中浅色框表示决策节点，箭头表示决策节点的每个答案的分支，黑色框表示遵循以下顺序的实例的结果:

![Figure 4.7: Data represented in a decision tree
](img/B15781_04_07.jpg)

图 4.7:决策树中表示的数据

为了执行预测，一旦构建了决策树，模型就获取每个实例，并遵循与该实例的特征相匹配的序列，直到它到达一个叶子，即结果。根据这一点，分类过程从根节点(最上面的节点)开始，沿着描述实例的分支继续。这个过程继续进行，直到到达代表该实例的预测的叶节点。

例如，一个人*超过 40 岁*，收入*低于 15 万美元*，教育水平*本科*很可能没有学生贷款；因此，分配给它的类别标签将是*否*。

考虑到连续特征将在范围内处理，决策树可以处理定量和定性特征。此外，叶节点可以处理分类或连续类标签；对于分类类标签，进行分类，而对于连续类标签，要处理的任务是回归。

## 练习 4.02:应用决策树算法

在本练习中，我们将把决策树算法应用于生育数据集，目的是确定一个人的生育水平是否受其人口统计、环境条件和既往医疗状况的影响。按照以下步骤完成本练习:

1.  打开一个 Jupyter 笔记本来实现这个练习并导入`pandas`，以及来自 scikit-learn 的`tree`模块的`DecisionTreeClassifier`类:

    ```
    import pandas as pd from sklearn.tree import DecisionTreeClassifier
    ```

2.  加载您在*练习 4.01* 、*中下载的`fertility_Diagnosis`数据集，应用朴素贝叶斯算法*。考虑到数据集不包含标题行:

    ```
    data = pd.read_csv("fertility_Diagnosis.csv", header=None)
    ```

    ，确保将等于`None`的`header`参数添加到`read_csv`函数中
3.  将数据拆分为`X`和`Y`，考虑到类标签位于索引等于`9`的列下。使用以下代码:

    ```
    X = data.iloc[:,:9] Y = data.iloc[:,9]
    ```

4.  Instantiate the `DecisionTreeClassifier` class. Next, use the `fit` function to train the model using `X` and `Y`:

    ```
    model = DecisionTreeClassifier()
    model.fit(X, Y)
    ```

    再次显示运行上述代码片段的输出。此输出通过打印模型使用的每个超参数的值，总结了定义模型的条件，如下所示:

    ```
    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                           criterion='gini', max_depth=None,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0,
                           min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0,
                           presort='deprecated',
                           random_state=None, splitter='best')
    ```

    由于模型已经被实例化，而没有设置任何超参数，因此摘要将显示用于每个超参数的默认值。

5.  Finally, perform a prediction by using the model that you trained before, for the same instances that we used in *Exercise 4.01*, *Applying the Naïve Bayes Algorithm*: `−0.33`, `0.69`, `0`, `1`, `1`, `0`, `0.8`, `0`, `0.88`.

    请使用以下代码来完成此操作:

    ```
    pred = model.predict([[-0.33,0.69,0,1,1,0,0.8,0,0.88]])
    print(pred)
    ```

    预测的输出如下:

    ```
    ['N']
    ```

    同样，模型预测受试者的生育能力没有受到影响。

    注意

    要访问该特定部分的源代码，请参考 https://packt.live/3hDlvns 的。

    你也可以在[https://packt.live/3fsVw07](https://packt.live/3fsVw07)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

您已经成功地定型了决策树模型，并对新数据执行了预测。

## 活动 4.02:为我们的人口普查收入数据集训练决策树模型

你继续致力于建立一个能够预测一个人年收入的模型。使用预处理的人口普查收入数据集，您选择构建决策树模型:

1.  打开您在之前的练习中使用的 Jupyter 笔记本，从 scikit-learn 导入决策树算法。
2.  在 scikit-learn 的`DecisionTreeClassifier`类上使用`fit`方法训练模型。为了训练模型，使用来自先前活动的训练集数据(`X_train`和`Y_train`)。
3.  Finally, perform a prediction by using the model that you trained for a new instance with the following values for each feature: `39`, `6`, `13`, `4`, `0`, `2174`, `0`, `40`, `38`.

    对个人的预测应该等于零，这意味着个人的收入很可能少于或等于 50K。

    注意

    这项活动的解决方案可在第 237 页找到。

# 支持向量机算法

**支持向量机** ( **SVM** )算法是一个分类器，它找到有效地将观察结果分成它们的类别标签的超平面。它首先将每个实例定位到一个具有 *n* 维的数据空间中，其中 *n* 表示特征的数量。接下来，它跟踪一条假想的线，这条线清楚地将属于一个类标签的实例与属于其他类标签的实例分开。

支持向量指的是给定实例的坐标。据此，支持向量机是在数据空间中有效分离不同支持向量的边界。

对于二维数据空间，超平面是将数据空间分成两部分的线，每一部分代表一个类标签。

## SVM 算法是如何工作的？

下图显示了 SVM 模型的一个简单示例。三角形和圆形数据点都表示输入数据集中的实例，其中的形状定义了每个实例所属的类标签。虚线表示清楚地分隔数据点的超平面，这是基于数据点在数据空间中的位置定义的。这条线用于对看不见的数据进行分类，用正方形表示。这样，位于直线左侧的新实例将被分类为三角形，而位于右侧的实例将被分类为圆形。

要素的数量越多，数据空间的维度就越多，这使得可视化表示模型变得不可能:

![Figure 4.8: Graphical example of an SVM model
](img/B15781_04_08.jpg)

图 4.8:SVM 模型的图形示例

尽管该算法看起来非常简单，但其复杂性在绘制适当超平面的算法方法论中是显而易见的。这是因为该模型概括了数百个具有多种特征的观察结果。

为了选择正确的超平面，算法遵循以下规则，其中*规则 1* 比*规则 2* 更重要:

*   **Rule 1**: The hyperplane must maximize the correct classification of instances. This basically means that the best line is the one that effectively separates data points belonging to different class labels while keeping those that belong to the same one together.

    例如，在下图中，虽然两条线都能够将大多数实例划分到正确的类标签中，但是模型会选择线 A 作为比线 B 更好地划分类的线，线 B 无法对两个数据点进行分类:

    ![Figure 4.9: Sample of hyperplanes that explain Rule 1
    ](img/B15781_04_09.jpg)

图 4.9:解释规则 1 的超平面示例

*   **Rule 2**: The hyperplane must maximize its distance to the nearest data point of either of the class labels, which is also known as the **margin**. This rule helps the model become more robust, which means that the model is able to generalize the input data so that it works efficiently on unseen data. This rule is especially important in preventing new instances from being mislabeled.

    例如，通过查看下图，可以得出两个超平面都符合*规则 1* 的结论。然而，选择线 A，因为与线 B 到其最近的数据点的距离相比，线 A 最大化了其到两个类的最近的数据点的距离:

    ![Figure 4.10: Sample of hyperplanes that explain Rule 2
    ](img/B15781_04_10.jpg)

图 4.10:解释规则 2 的超平面示例

默认情况下，SVM 算法使用线性函数分割输入数据的数据点。但是，可以通过更改算法的内核类型来修改这种配置。例如，考虑下图:

注意

对于 scikit-learn 的 SVM 算法，内核指的是用于分割数据点的数学函数，可以是线性的、多项式的或 s 形的，等等。要了解有关该算法参数的更多信息，请访问以下 URL:[https://sci kit-learn . org/stable/modules/generated/sk learn . SVM . SVC . html # sk learn . SVM . SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)。

![Figure 4.11: Sample observations
](img/B15781_04_11.jpg)

图 4.11:样本观察

为了分离这些观察结果，模型必须画一个圆或另一个类似的形状。该算法通过使用内核(数学函数)来处理这一问题，内核可以向数据集引入额外的特征，以便将数据点的分布修改为允许线将它们分离的形式。有几个内核可以实现这一点，应该通过反复试验来选择一个内核，这样您就可以找到最能对可用数据进行分类的内核。

然而，scikit-learn 中 SVM 算法的默认内核是**径向基函数** ( **RBF** )内核。这主要是因为，根据几项研究，这个内核已经被证明对大多数数据问题非常有效。

## 练习 4.03:应用 SVM 算法

在本练习中，我们将对生育率数据集应用 SVM 算法。这个想法和前面的练习一样，是为了确定一个人的生育水平是否受到他们的人口统计、环境条件和以前的医疗条件的影响。按照以下步骤完成本练习:

1.  打开一个 Jupyter 笔记本来实现这个练习。从 scikit-learn 的`svm`模块

    ```
    import pandas as pd from sklearn.svm import SVC
    ```

    中导入熊猫和`SVC`类
2.  加载你在*练习 4.01* 、*中下载的`fertility_Diagnosis`数据集，应用朴素贝叶斯算法*。考虑到数据集不包含标题行:

    ```
    data = pd.read_csv("fertility_Diagnosis.csv", header=None)
    ```

    ，确保将`header = None`参数添加到`read_csv`函数中
3.  将数据拆分为`X`和`Y`，考虑到类标签在索引等于`9`的列下。使用下面的代码来做到这一点:

    ```
    X = data.iloc[:,:9] Y = data.iloc[:,9]
    ```

4.  Instantiate scikit-learn's `SVC` class and use the `fit` function to train the model using `X` and `Y`:

    ```
    model = SVC()
    model.fit(X, Y)
    ```

    同样，运行这段代码的输出代表了模型的概要，以及它的默认超参数，如下所示:

    ```
    SVC(C=1.0, break_ties=False, cache_size=200,
        class_weight=None, coef0=0.0,
        decision_function_shape='ovr', degree=3,
        gamma='scale', kernel='rbf', max_iter=-1,
        probability=False, random_state=None, shrinking=True,
        tol=0.001, verbose=False)
    ```

5.  Finally, perform a prediction using the model that you trained previously, for the same instances that we used in *Exercise 4.01*, *Applying the Naïve Bayes Algorithm*: −`0.33`, `0.69`, `0`, `1`, `1`, `0`, `0.8`, `0`, `0.88`.

    请使用以下代码来完成此操作:

    ```
    pred = model.predict([[-0.33,0.69,0,1,1,0,0.8,0,0.88]])
    print(pred)
    ```

    输出如下所示:

    ```
    ['N']
    ```

    同样，模型预测实例的类标签为`N`，这意味着受试者的生育能力没有受到影响。

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/2YyEMNX](https://packt.live/2YyEMNX)。

    你也可以在 https://packt.live/2Y3nIR2 在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

您已经成功地训练了一个 SVM 模型并执行了一次预测。

## 活动 4.03:为人口普查收入数据集训练一个 SVM 模型

继续您的任务，建立一个模型，能够预测一个人的年收入，最后一个算法，你要训练的是支持向量机。按照以下步骤实现此活动:

1.  打开您在之前的练习中使用的 Jupyter 笔记本，从 scikit-learn 导入 SVM 算法。
2.  Train the model using the `fit` method on the `SVC` class from scikit-learn. To train the model, use the training set data from the previous activity (`X_train` and `Y_train`).

    注意

    使用`fit`方法训练 SVC 类的过程可能需要一段时间。

3.  Finally, perform a prediction using the model that you trained previously, for a new instance with the following values for each feature: `39`, `6`, `13`, `4`, `0`, `2174`, `0`, `40`, `38`.

    对个人的预测应该等于零，也就是说，个人的收入很可能小于或等于 50K。

    注意

    这项活动的解决方案可在第 238 页找到。

# 错误分析

在前一章中，我们解释了错误分析的重要性。在本节中，我们将为之前活动中创建的所有三个模型计算不同的评估指标，以便进行比较。

出于学习目的，我们将使用准确度、精确度和召回指标来比较这些模型。通过这种方式，可以看出即使模型在某个指标方面可能更好，但在测量不同的指标时可能会更差，这有助于强调根据您希望实现的目标选择正确的指标来测量您的模型的重要性。

## 准确度、精密度和召回率

快速提醒一下，为了测量性能和执行错误分析，需要对不同的数据集(训练、验证和测试)使用`predict`方法。下面的代码片段提供了一种一次测量三组指标的简洁方法:

注意

完成本章的活动后，将执行以下步骤。这主要是因为本节中的步骤对应于本章活动的延续。

1.  首先，导入将要使用的三个指标:

    ```
    from sklearn.metrics import accuracy_score, \ precision_score, recall_score
    ```

2.  接下来，我们创建两个包含不同数据集的列表，这些数据集将在一个`for`循环中使用，以对所有模型的所有数据集执行性能计算:

    ```
    X_sets = [X_train, X_dev, X_test] Y_sets = [Y_train, Y_dev, Y_test]
    ```

3.  将创建一个字典，它将保存每个模型的每个数据集的每个评估指标的值:

    ```
    metrics = {"NB":{"Acc":[],"Pre":[],"Rec":[]},            "DT":{"Acc":[],"Pre":[],"Rec":[]},            "SVM":{"Acc":[],"Pre":[],"Rec":[]}}
    ```

4.  一个`for`循环用于遍历不同的数据集:

    ```
    for i in range(0,len(X_sets)):     pred_NB = model_NB.predict(X_sets[i])     metrics["NB"]["Acc"].append(accuracy_score(Y_sets[i], \                                 pred_NB))     metrics["NB"]["Pre"].append(precision_score(Y_sets[i], \                                 pred_NB))     metrics["NB"]["Rec"].append(recall_score(Y_sets[i], \                                 pred_NB))     pred_tree = model_tree.predict(X_sets[i])     metrics["DT"]["Acc"].append(accuracy_score(Y_sets[i], \                                 pred_tree))     metrics["DT"]["Pre"].append(precision_score(Y_sets[i], \                                 pred_tree))     metrics["DT"]["Rec"].append(recall_score(Y_sets[i], \                                 pred_tree))     pred_svm = model_svm.predict(X_sets[i])     metrics["SVM"]["Acc"].append(accuracy_score(Y_sets[i], \                                  pred_svm))     metrics["SVM"]["Pre"].append(precision_score(Y_sets[i], \                                  pred_svm))     metrics["SVM"]["Rec"].append(recall_score(Y_sets[i], \                                  pred_svm))
    ```

5.  Print the metrics, as follows:

    ```
    print(metrics)
    ```

    输出如下所示:

    ![Figure 4.12: Printing the metrics
    ](img/B15781_04_12.jpg)

图 4.12:打印指标

在`for`循环中，有三个代码块，分别对应于我们在前面的活动中创建的每个模型。每个代码块执行以下操作。

首先，做一个预测。预测是通过在模型上调用`predict`方法，输入一组数据来实现的。由于该操作发生在一个`for`循环中，因此将对所有数据集进行预测。

接下来，通过将实际数据与我们之前计算的预测进行比较，完成所有三个指标的计算。该计算将被追加到先前创建的字典中。

从前面的片段中，可以获得以下结果:

![Figure 4.13: Performance results of all three models
](img/B15781_04_13.jpg)

图 4.13:所有三种模型的性能结果

注意

通过打开名为`Error analysis`的文件，查看代码以获得这些结果，这些结果可以在本书的 GitHub 资源库中名为`Chapter04`的文件夹下找到。

最初，将在仅考虑来自准确性度量的值的同时，进行与选择最佳拟合模型相关的以及关于每个模型所遭受的条件的以下推断，假设贝叶斯误差接近 0(意味着模型可以达到接近 1 的最大成功率):

*   通过比较朴素贝叶斯和 SVM 模型的三个准确度分数，可以得出结论，对于所有三组数据，模型的行为方式几乎相同。这基本上意味着模型正在概括来自训练集的数据，这允许它们在看不见的数据上表现良好。尽管如此，模型的总体性能在 0.8 左右，这与最大成功率相差甚远。这意味着模型可能存在较大偏差。
*   此外，就训练集的准确性而言，决策树模型的性能更接近于最大成功率。然而，考虑到模型在验证集上的精度水平远低于其在训练集上的性能，该模型正遭受过度拟合的情况。据此，可以通过向训练集添加更多数据或微调模型的超参数来解决过度拟合问题，这将有助于提高验证和测试集的准确性水平。修剪树可以帮助过度拟合的模型。

考虑到这一点，研究人员现在拥有了选择模型所需的信息，并致力于改进结果以实现模型的最大可能性能。

接下来，出于学习目的，让我们比较决策树模型的所有指标的结果。尽管所有三个度量的值都证明了过度拟合的存在，但是对于精度和召回度量，可能观察到过度拟合的程度要大得多。此外，可以得出结论，该模型在由召回度量测量的训练集上的性能低得多，这意味着该模型在分类正面标签方面不太好。这意味着，如果案例研究的目的是最大化正面分类的数量，而不考虑负面标签的分类，则该模型还需要提高其在训练集上的性能。

注意

前面的比较表明，如果用不同的指标衡量，同一模型的性能会有所不同。有鉴于此，选择案例研究的相关性指标至关重要。

使用您从前面章节中获得的知识，继续探索上表中显示的结果。

# 总结

利用前几章的知识，我们从分析人口普查收入数据集开始这一章，目的是了解可用的数据并对预处理过程做出决策。解释了三种监督学习分类算法，即朴素贝叶斯算法、决策树算法和 SVM 算法，并将其应用于之前预处理的数据集，以创建推广到训练数据的模型。最后，我们通过计算不同数据集(训练、验证和测试)的准确度、精确度和召回率，比较了三个模型在人口普查收入数据集上的性能。

在下一章，我们将看看**人工神经网络**(**ann**)，它们的不同类型，以及它们的优缺点。我们也将使用人工神经网络来解决本章中讨论的相同的数据问题，并将其性能与其他监督学习算法进行比较。