<title>B18335_07_ePub</title>

# *第七章*:在线回归

在查看了前面章节中的在线异常检测和在线分类之后，还有一大类在线机器学习有待观察。**回归**是一系列受监督的机器学习模型，适用于目标变量为数字的用例。

在异常检测和分类中，您已经看到了如何构建模型来预测分类目标(是/否和虹膜种类)，但是您还没有看到如何处理数值目标。处理数字数据需要有不同的工作方法，无论是在模型训练和模型定义的更深层次，还是在我们使用度量标准时。

想象一下，作为一名天气预报员，试图预测明天的温度(摄氏度)。也许你期待一个晴天，你有一个模型，你用它来预测 25 摄氏度的温度。想象一下，如果第二天，你观察到天很冷，只有 18 度；你显然错了。

现在，想象一下你预测了 24 度。在一个分类用例中，你可能倾向于说 25 不是 24，所以结果是错误的。不过 24 的结果*比 18 的结果少错*。

在回归分析中，一个单一的预测可能或多或少是错误的。实际上，你很少是完全正确的。在分类上，你不是错就是对，所以这是不一样的。这引入了对新指标的需求和模型基准测试过程的变化。

在这一章中，你将首先得到回归模型的更深入的介绍，重点是 River 中的在线回归模型。之后，您将在回归模型基准上工作。

本章涵盖以下主题:

*   定义回归
*   回归的用例
*   River 中回归算法综述

# 技术要求

你可以在 GitHub 上找到这本书的所有代码，链接如下:[https://GitHub . com/packt publishing/Machine-Learning-for-Streaming-Data-with-Python](https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python)。如果您还不熟悉 Git 和 GitHub，下载笔记本和代码示例的最简单方法如下:

1.  转到存储库的链接。
2.  转到绿色的**代码**按钮。
3.  选择**下载 ZIP 文件**。

当您下载 ZIP 文件时，在您的本地环境中解压缩它，您将能够通过您喜欢的 Python 编辑器访问代码。

## Python 环境

要阅读本书，您可以下载存储库中的代码，并使用您喜欢的 Python 编辑器执行它。

如果你还不熟悉 Python 环境，我建议你去看看 Anaconda(【https://www.anaconda.com/products/individual】T2)，它带有 Jupyter Notebook 和 JupyterLab，这两个工具都非常适合执行笔记本。它还带有 Spyder 和 VS 代码，用于编辑脚本和程序。

如果你在你的机器上安装 Python 或相关程序有困难，你可以看看谷歌 Colab(【https://colab.research.google.com/】[)或 Kaggle 笔记本(](https://colab.research.google.com/)[【https://www.kaggle.com/code】](https://www.kaggle.com/code))，它们都允许你在网上免费运行 Python 代码，不需要任何设置。

# 定义回归

在这一章中，你会发现回归。回归是一种受监督的机器学习任务，其中构建了一个模型，该模型基于数字或分类自变量来预测或估计数字目标变量。

最简单的回归模型类型是 **线性回归**。让我们考虑一个如何使用线性回归进行回归的超级简单的例子。

假设我们有一个数据集，其中有 10 个人的观察结果。根据他们每周学习的小时数，我们必须估计他们的平均成绩(从 1 到 10)。当然，这是一个过于简单化的问题。

数据如下所示:

代码块 7-1

```
import pandas as pd
```

```
nb_hrs_studies = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
```

```
avg_grade = [5.5, 5.8, 6.8, 7.2, 7.4, 7.8, 8.2, 8.8, 9.3, 9.4]
```

```
data = pd.DataFrame({'nb_hrs_studies': nb_hrs_studies, 'avg_grade': avg_grade})
```

```
data
```

您将获得以下数据帧:

![Figure 7.1 – The dataset
](img/B18335_07_1.jpg)

图 7.1-数据集

让我们绘制数据，看看这如何变成一个回归问题:

代码块 7-2

```
import matplotlib.pyplot as plt
```

```
plt.scatter(data['nb_hrs_studies'], data['avg_grade'])
```

```
plt.xlabel('nb_hrs_studies')
```

```
plt.ylabel('avg_grades')
```

这会产生以下输出:

![Figure 7.2 – A scatter plot of the data
](img/B18335_07_2.jpg)

图 7.2–数据的散点图

现在，线性回归的目标是拟合最佳通过这些点的线(或超平面),并且能够预测任何`nb_hrs_studies`的估计`avg_grades`。其他回归模型各有其特定的方式来构造预测函数，但最终都有相同的目标:创建最佳拟合公式，以使用一个或多个自变量来预测数值目标变量。

在下一节中，您将发现一些可以使用回归的示例用例。

# 回归用例

回归的用例是巨大的:它是许多项目中非常常用的方法。不过，让我们看一些例子来更好地理解可以从回归模型中获益的不同类型的用例。

## 用例 1–预测

回归算法的一个非常常见的用例是预测。在预测中，目标是预测一个变量在一段时间内的未来值。这样的变量称为 **时间序列**。尽管时间序列建模有许多特定的方法，但回归模型也是获得良好的未来预测性能的有力竞争者。

在一些预测用例中，实时响应非常重要。股票交易就是一个例子，在股票交易中，股票价格的数据点以巨大的速度到达，预测必须立即进行调整，以使用股票交易的最佳信息。甚至自动股票交易算法也存在，它们需要快速反应，以尽可能从交易中获取最大利润。

关于这个主题的进一步阅读，您可以从查看以下链接开始:

*   [https://www . investopedia . com/articles/financial-theory/09/regression-analysis-basics-business . ASP](https://www.investopedia.com/articles/financial-theory/09/regression-analysis-basics-business.asp)
*   [https://www . mathworks . com/help/econ/time-series-regression-VII-forecasting . html](https://www.mathworks.com/help/econ/time-series-regression-vii-forecasting.html)

## 用例 2——预测制造过程中的缺陷产品数量

在实践中使用的实时和流式回归模型的第二个例子是预测性维护模型在制造业中的应用。例如，您可以使用生产线上每小时不合格产品数量的实时预测。这也是一个回归模型，因为结果是一个数字，而不是一个分类变量。

生产线可以将这种预测用于实时警报系统，例如，一旦预测达到了有缺陷产品的阈值。实时数据集成对此很重要，因为生产错误的产品是对资源的巨大浪费。

以下两个资源将帮助您了解有关此使用案例的更多信息:

*   [https://www . science direct . com/science/article/pii/s 2405896316308084](https://www.sciencedirect.com/science/article/pii/S2405896316308084)
*   [https://www . research gate . net/publication/315855789 _ Regression _ Models _ for _ Lean _ Production](https://www.researchgate.net/publication/315855789_Regression_Models_for_Lean_Production)

既然我们已经探索了一些回归的用例，让我们从回归的各种算法开始。

# 河流中回归算法概述

River 在线机器学习包中有大量的在线回归模型可用。

一些相关的例子如下:

*   `LinearRegression`
*   `HoeffdingAdaptiveTreeRegressor`
*   `SGTRegressor`
*   `SRPRegressor`

## 回归算法 1–线性回归

线性回归是最基本的回归模型之一。简单线性回归是通过数据点拟合直线的回归模型。下图说明了这一点:

![Figure 7.3 – A linear model in a scatter plot
](img/B18335_07_3.jpg)

图 7.3–散点图中的线性模型

这条橙色线是以下公式的结果:

![](img/Formula_07_001.jpg)

这里， *y* 代表`avg_grades`， *x* 代表`nb_hrs_studies`。在拟合模型时， *a* 和 *b* 系数是估计值。这个公式中的 *b* 系数称为截距。表示 *x* 等于`0`时 *y* 的值。*一个*系数代表直线的斜率。对于 *x* 中的每一个额外步骤， *a* 表示加到 *y* 上的数量。

这是线性回归的一个版本，但是还有一个版本叫做**多元线性回归**，其中有多个 *x* 变量。在这种情况下，模型并不代表一条线，而是代表一个超平面，其中为每个额外的 *x* 变量添加一个斜率系数。

### 河流中的线性回归

现在让我们继续使用 Python 中的 River ML 构建一个在线线性回归的示例:

1.  如果你还记得之前的例子，我们在`scikit-learn`中使用了一个叫做`make_classification`的函数。同样可以使用`make_regression`对回归问题进行:

代码块 7-3

```
from sklearn.datasets import make_regression
X,y = make_regression(n_samples=1000,n_features=5,n_informative=5,noise=100)
```

1.  为了更好地了解这个`make_regression`函数的结果，让我们检查一下这个数据集的`X`。您可以使用以下代码快速查看数据:

代码块 7-4

```
pd.DataFrame(X).describe()
```

`describe()`方法将输出一个包含变量描述性统计数据的数据框，如下所示:

![Figure 7.4 – Descriptive statistics
](img/B18335_07_4.jpg)

图 7.4-描述性统计

`X`数据中有五个列，有 1000 个观测值。

1.  现在，来看看`y`变量，也称为变量`target`，我们可以制作如下直方图:

代码块 7-5

```
pd.Series(y).hist()
```

生成的直方图如下图所示:

![Figure 7.5 – The resulting histogram
](img/B18335_07_5.jpg)

图 7.5–生成的直方图

这里可以进行更多探索性的数据分析，但这超出了本书的范围。

1.  现在让我们继续创建一个训练和测试集，以创建一个公平的模型验证方法。在下面的代码中，您可以看到如何从`scikit-learn`创建`train_test_split`函数来创建一个训练测试分割:

代码块 7-6

```
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
```

1.  您可以使用以下代码在 River 中创建线性回归:

代码块 7-7

```
!pip install river
from river.linear_model import LinearRegression
model = LinearRegression()
```

1.  然后，该模型必须适合训练数据。我们使用和你在书的前面看到的相同的循环。这个循环遍历各个数据点(`X`和`y`)，并按照 River 的要求将`X`值转换成一个字典。然后使用`learn_one`方法对模型进行逐点数据更新:

代码块 7-8

```
# fit the model
for x_i,y_i in zip(X_train,y_train):
    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}
    model.learn_one(x_json,y_i)
```

1.  一旦模型从训练数据中学习，就需要在测试集上对其进行评估。这可以通过循环测试数据并预测每个数据点的`X`值来完成。`y`值存储在一个列表中，用于对照测试数据集的实际`y`值进行评估:

代码块 7-9

```
# predict on the test set
import pandas as pd
preds = []
for x_i in X_test:
    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}
    preds.append(model.predict_one(x_json))
```

1.  我们现在可以为这个回归模型计算我们选择的度量，例如,`r2`分数。这可以使用以下代码来完成:

代码块 7-10

```
# compute accuracy
from sklearn.metrics import r2_score
r2_score(y_test, preds)
```

得到的结果是`0.478`。

在下一节中，让我们看看其他型号是否在这项任务上表现更好。

## 回归算法 2–hoeffdingadaptivetreegressor

我们将要讨论的第二个在线回归模型是一个更加具体的在线回归模型。与许多其他模型一样，`LinearRegression`模型是对基本离线模型的在线改编，而许多其他模型是专门为在线模型开发的。`HoeffdingAdaptiveTreeRegressor`就是其中之一。

**赫夫丁自适应树回归器** ( **HATR** )是基于**赫夫丁自适应树分类器** ( **HATC** )的回归模型。HATC 是一个基于树的模型，它使用**自适应窗口** ( **ADWIN** )方法来监控树的不同分支的性能。HATC 方法在新分支到期时用新分支替换这些分支。这是通过观察旧分支对新分支的更好表现来确定的。HATC 也有河。

HATR 回归版本基于 HATC 方法，并在每个决策节点使用 ADWIN 概念漂移检测器。这允许该方法检测底层数据中可能的变化，这被称为**漂移**。漂移检测将在下一章中详细讨论。

### 河流中的 HoeffdingAdaptiveTreeRegressor

我们将查看如下示例:

1.  让我们开始在之前的模型中使用的相同数据上拟合模型:

代码块 7-11

```
from river.tree import HoeffdingAdaptiveTreeRegressor
model = HoeffdingAdaptiveTreeRegressor(seed=42)
# fit the model
for x_i,y_i in zip(X_train,y_train):
    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}
    model.learn_one(x_json,y_i)
# predict on the test set
import pandas as pd
preds = []
for x_i in X_test:
    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}
    preds.append(model.predict_one(x_json))
# compute accuracy
from sklearn.metrics import r2_score
r2_score(y_test, preds)
```

1.  这个模型得到一个比线性回归`0.437`稍差的`r2`分数。让我们看看我们是否能做些什么来使它更好地工作。让我们写一个网格搜索，看看一些超参数是否有助于改进模型。

为此，让我们将模型写成一个函数，它接受超参数的值，并返回`r2`得分:

代码块 7-12

```
def evaluate_HATR(grace_period, leaf_prediction, model_selector_decay):
    # model pipeline
    model = (
        HoeffdingAdaptiveTreeRegressor(
            grace_period=grace_period,
            leaf_prediction=leaf_prediction,
            model_selector_decay=model_selector_decay,
            seed=42)
    )
    # fit the model
    for x_i,y_i in zip(X_train,y_train):
        x_json = {'val'+str(i): x for i,x in enumerate(x_i)}
        model.learn_one(x_json,y_i)
    # predict on the test set
    preds = []
    for x_i in X_test:
        x_json = {'val'+str(i): x for i,x in enumerate(x_i)}
        preds.append(model.predict_one(x_json))
    # compute accuracy
    return r2_score(y_test, preds)
```

1.  让我们如下指定要调整的超参数:

代码块 7-13

```
grace_periods=[0,5,10,]
leaf_predictions=['mean','adaptive']
model_selector_decays=[ 0.3, 0.8,  0.95]
```

1.  然后，我们循环通过数据，如下所示:

代码块 7-14

```
results = []
i = 0
for grace_period in grace_periods:
    for leaf_prediction in leaf_predictions:
        for model_selector_decay in model_selector_decays:
            print(i)
            i = i+1
            results.append([grace_period, leaf_prediction, model_selector_decay,evaluate_HATR(grace_period, leaf_prediction, model_selector_decay)])
```

1.  然后可以获得如下结果:

代码块 7-15

```
pd.DataFrame(results, columns=['grace_period', 'leaf_prediction', 'model_selector_decay', 'r2_score' ]).sort_values('r2_score', ascending=False)
```

获得的结果有点令人失望，因为没有一个测试值能够产生更好的结果。不幸的是，这是数据科学的一部分，因为不是所有的模型在每个用例上都工作得很好。

![Figure 7.6 – The resulting output of Code Block 7-15
](img/B18335_07_6.jpg)

图 7.6–代码块 7-15 的结果输出

让我们继续看下一个型号，看看它是否更合适。

## 回归算法 3–Sgt regressor

`SGTRegressor`是用于回归的随机梯度树。这是另一种基于决策树的模型，可以随着新数据的到来而学习。它是一个增量决策树，通过最小化损失函数来最小化均方误差。

### 河流中的 SGTRegressor

我们将使用下面的示例来验证这一点:

1.  让我们测试一下这个模型是否能够提高这个回归任务的性能:

代码块 7-16

```
from river.tree import SGTRegressor
# model pipeline
model = SGTRegressor()
# fit the model
for x_i,y_i in zip(X_train,y_train):
    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}
    model.learn_one(x_json,y_i)
# predict on the test set
preds = []
for x_i in X_test:
    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}
    preds.append(model.predict_one(x_json))
# compute accuracy
r2_score(y_test, preds)
```

1.  结果比以前的型号更糟糕，因为它是`0.07`。让我们再次看看是否可以使用超参数调谐对其进行优化:

代码块 7-17

```
from river.tree import SGTRegressor
def evaluate_SGT(delta, lambda_value, grace_period):
    # model pipeline 
    model = SGTRegressor(delta=delta,
                        lambda_value=lambda_value,
                        grace_period=grace_period,)
    # fit the model
    for x_i,y_i in zip(X_train,y_train):
        x_json = {'val'+str(i): x for i,x in enumerate(x_i)}
        model.learn_one(x_json,y_i)
    # predict on the test set
    preds = []
    for x_i in X_test:
        x_json = {'val'+str(i): x for i,x in enumerate(x_i)}
        preds.append(model.predict_one(x_json))
    # compute accuracy
    return r2_score(y_test, preds)
```

1.  对于这次试验，我们将优化的`grace_period`、`lambda_value`和`delta`超参数:

代码块 7-18

```
grace_periods=[0,10,25]
lambda_values=[0.5, 0.8, 1.]
deltas=[0.0001, 0.001, 0.01, 0.1]
```

1.  您可以使用下面的代码运行优化循环:

代码块 7-19

```
results = []
i = 0
for grace_period in grace_periods:
    for lambda_value in lambda_values:
        for delta in deltas:
            print(i)
            i = i+1
            result = evaluate_SGT(delta, lambda_value, grace_period)
            print(result)
            results.append([delta, lambda_value, grace_period,result])
```

1.  使用下面的代码行可以显示的最佳结果:

代码块 7-20

```
pd.DataFrame(results, columns=['delta', 'lambda_value', 'grace_period', 'r2_score' ]).sort_values('r2_score', ascending=False)
```

结果如下所示:

![Figure 7.7 – The resulting output of Code Block 7-20
](img/B18335_07_7.jpg)

图 7.7–代码块 7-20 的结果输出

结果是比未调教的`SGTRegressor`好，但比前两款差很多。模型可以进一步优化，但它似乎不是当前数据的最佳选择。

## 回归算法 4–SRP regressor

`SRPRegressor`或**流随机补丁回归器**，是一种在输入数据的子集上训练基本学习器的集成方法。这些子集被称为**小块**，既是特征的子集，也是观察的子集。这与上一章看到的**随机森林**的方法相同。

### 河流中的硫酸盐还原菌

我们将使用以下示例来验证这一点:

1.  在本例中，让我们使用线性回归作为基础学习者，因为与本章中测试的其他模型相比，该模型具有最佳性能:

代码块 7-21

```
from river.ensemble import SRPRegressor
# model pipeline 
base_model = LinearRegression()
model = SRPRegressor(
    model=base_model,
    n_models=3,
    seed=42
)
# fit the model
for x_i,y_i in zip(X_train,y_train):
    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}
    model.learn_one(x_json,y_i)
# predict on the test set
preds = []
for x_i in X_test:
    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}
    preds.append(model.predict_one(x_json))
# compute accuracy
r2_score(y_test, preds)
```

1.  得出的分数是`0.34`。让我们试着调整使用的模型数量，看看这个是否能提高的性能:

代码块 7-22

```
def evaluate_SRP(n_models):
    # model pipeline 
    base_model = LinearRegression()
    model = SRPRegressor(
        model=base_model,
        n_models=n_models,
        seed=42
    )
    # fit the model
    for x_i,y_i in zip(X_train,y_train):
        x_json = {'val'+str(i): x for i,x in enumerate(x_i)}
        model.learn_one(x_json,y_i)
    # predict on the test set
    preds = []
    for x_i in X_test:
        x_json = {'val'+str(i): x for i,x in enumerate(x_i)}
        preds.append(model.predict_one(x_json))
    # compute accuracy
    return r2_score(y_test, preds)
```

1.  您可以使用以下代码执行优化循环:

代码块 7-23

```
results = []
for n_models in range(1, 50):
    results.append([n_models, evaluate_SRP(n_models)])
```

1.  下面一行显示了`n_models`的每个值的结果:

代码块 7-24

```
pd.DataFrame(results,columns=['n_models', 'r2_score']).sort_values('r2_score', ascending=False)
```

结果是如下所示的:

![Figure 7.8 – The resulting output of Code Block 7-24
](img/B18335_07_8.jpg)

图 7.8–代码块 7-24 的结果输出

显然，12 款车型的结果已经找到了一个表现`0.457`的最佳点。相对于得分为`0.478`的简单`LinearRegression`模型，这是一个更差的结果。这表明`LinearRegression`模型在该数据集中测试的四个模型中得分最高。

当然，这个结果与`make_regression`函数背后的数据生成过程密切相关。如果`make_regression`函数添加任何东西，比如时间趋势，自适应模型可能会比简单的线性模型性能更好。

# 总结

在这一章中，你已经看到了回归建模的基础。您已经了解到分类和异常检测模型之间有一些相似之处，但也有一些基本的区别。

回归的主要区别是目标变量是数字，而它们在分类中是分类的。这不仅引入了度量标准的差异，还引入了模型定义和模型工作方式的差异。

您已经看到了几种传统的离线回归模型，以及它们对在线培训方式的适应性。你也看到了一些专门为在线训练和流而制作的在线回归模型。

在前面的章节中，您已经看到了如何使用训练测试集来实现建模基准。ML 领域不会停止发展，并且会定期发布更新更好的模型。这就要求从业者在评估模型的技能上要扎实。

掌握模型评估往往比了解最大的模型列表更重要。您需要知道大量的模型来开始建模，但是评估将允许您避免将错误的或过度拟合的模型推向生产。

尽管这通常适用于 ML，但下一章将介绍一类对此有着根本不同看法的模型。强化学习是在线 ML 的一个类别，其重点是模型更新。在线模型也有能力对进入系统的每一条数据进行学习，但强化学习更侧重于几乎自主的学习。这将是下一章的范围。

# 延伸阅读

*   *线性回归*:[https://river ml . XYZ/latest/API/linear-model/linear regression/](https://riverml.xyz/latest/api/linear-model/LinearRegression/)
*   *Make _ regression*:[https://sci kit-learn . org/stable/modules/generated/sk learn . datasets . Make _ regression . html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html)
*   *hoeffdingadaptivetreegressor*:[https://river ml . XYZ/latest/API/tree/hoeffdingadaptivetreegressor/](https://riverml.xyz/latest/api/tree/HoeffdingAdaptiveTreeRegressor/)
*   *hoeffdingadaptivetreecider*:[https://river ml . XYZ/latest/API/tree/hoeffdingadaptivetreecider/](https://riverml.xyz/latest/api/tree/HoeffdingAdaptiveTreeClassifier/)
*   *数据流和频繁模式的自适应学习和挖掘*:[https://dl.acm.org/doi/abs/10.1145/1656274.1656287](https://dl.acm.org/doi/abs/10.1145/1656274.1656287)
*   https://riverml.xyz/latest/api/tree/SGTRegressor/总统:[T21](https://riverml.xyz/latest/api/tree/SGTRegressor/)
*   https://riverml.xyz/latest/api/ensemble/SRPRegressor/