<title>B16721_07_Final_SK_ePub</title>

# *第七章*:了解 ML 模型

现在我们已经使用 H2O 软件建立了一些模型，生产前的下一步是了解模型是如何做决定的。这被不同地称为**机器学习可解释性** ( **MLI** )、**可解释的人工智能** ( **XAI** )、模型可解释性等等。所有这些术语的要点是，建立一个预测良好的模型是不够的。在完全信任之前部署任何模型都存在固有的风险。在这一章中，我们概述了 H2O 解释 ML 模型的一系列能力。

本章结束时，您将能够做到以下几点:

*   选择合适的模型度量来评估您的模型。
*   解释什么是 Shapley 值以及如何使用它们。
*   描述全局和局部可解释性的区别。
*   使用多种诊断方法来建立对模型的理解和信任。
*   使用全局和局部解释以及模型性能度量来从一组候选模型中选择最佳模型。
*   评估模型预测性能、评分速度和单个候选模型中满足的假设之间的权衡。

在本章中，我们将讨论以下主要话题:

*   选择模型性能指标
*   解释 H2O 制造的车型(全球和本地)
*   通过 H2O AutoDoc 自动化模型文档

# 选择模型性能指标

与任何模型最相关的问题是，*它的预测能力如何？*不考虑模型可能具有的任何其他积极属性，预测不佳的模型就是没有多大用处。如何最好地衡量预测性能取决于正在解决的具体问题和数据科学家可用的选择。H2O 为衡量模型性能提供了多种选择。

为了测量回归问题中预测模型的性能，H2O 提供了 R2、**均方误差** ( **MSE** )、**均方根误差** ( **RMSE** )、**均方根对数误差** ( **RMSLE** )和**平均绝对误差** ( **MAE** )作为度量。MSE 和 RMSE 是很好的默认选项，RMSE是我们的首选，因为度量标准用与预测相同的单位表示(而不是像 MSE 那样用平方单位)。所有基于平方误差的度量通常对异常值敏感。如果对异常值的健壮性是一个需求，那么 MAE 是一个更好的选择。最后，RMSLE 在欠预测比过预测更糟糕的特殊情况下是有用的。

对于分类模型，H2O 增加了基尼系数、绝对**马修斯相关系数** ( **MCC** )、F1、F0.5、F2、精确度、对数损失、**ROC 曲线下面积** ( **AUC** )、精确度-召回曲线下面积 ( **AUCPR** )和**Kolmogorov-Smirnov**(**根据我们的经验，AUC 是业务中最常用的指标。因为与业务合作伙伴和高管的沟通对数据科学家来说至关重要，所以我们建议在适合工作时使用众所周知的指标。在 AUC 的情况下，当数据相对平衡时，它在二进制分类模型方面做得很好。对于不平衡数据，AUCPR 是更好的选择。**

基于信息论的逻辑损失度量具有一些数学优势。特别是，如果您对类成员的预测概率本身感兴趣，而不仅仅是对预测的分类感兴趣，对数损失是一个更好的度量选择。关于这些评分选项的更多文档可以在[https://docs . H2O . ai/H2O/latest-stable/H2O-docs/performance-and-prediction . html](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/performance-and-prediction.html)找到。

在 AutoML 中为分类问题创建的排行榜包括 AUC、Logloss、AUCPR、平均每类误差、RMSE 和 MSE 作为性能指标。在 [*第五章*](B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082) 、*高级建模-第一部分*中创建的`check` AutoML 对象的排行榜如图*图 7.1* 所示为例:

![Figure 7.1 – An AutoML leaderboard for the check object 
](img/B16721_07_01.jpg)

图 7.1–支票对象的 AutoML 排行榜

除了预测性能的之外，模型性能的其他指标在企业环境中也很重要。其中两个默认包含在 AutoML 排行榜中的是拟合一个模型所需的时间量(`training_time_ms`)和预测一行数据所需的时间量(`predict_time_per_row_ms`)。

在*图 7.1* 中，根据 AUC 和 Logloss 的最佳模型是所有模型的堆叠集合(顶行中的模型)。这个模型也是最慢的，比任何单个模型慢一个数量级。特别是对于流或实时应用，不能足够快地评分的模型可能自动被取消候选资格，而不管其预测性能如何。

为了理解和评估我们的 ML 模型，我们接下来处理模型可解释性。

# 解释 H2O 制造的车型

根据我们的测试数据测量的模型性能指标可以告诉我们一个模型预测得有多好，预测得有多快。正如在引言一章中提到的，知道一个模型预测得很好并不是将其投入生产的充分理由。单独的性能指标不能提供任何关于*为什么*模型预测是这样的洞察。如果我们不理解为什么模型预测得很好，我们就没有希望能够预测到会使模型不能很好工作的条件。在将模型推广到生产之前，解释模型推理的能力是关键的一步。这个过程可以描述为获得对模型的信任。

可解释性通常分为全局组件和局部组件。全局可解释性描述了模型如何适用于整个群体。获得对模型的信任主要是决定它如何在全球范围内工作的功能。局部解释在单独的行上操作。他们提出了一些问题，比如个人预测是如何产生的。`h2o.explain`和`h2o.explain_row`方法分别为全局和局部解释捆绑了一组可解释函数和可视化。

我们从简单介绍 Shapley 值开始这一部分，Shapley 值是模型可解释性的基础方法之一，第一次遇到时可能会感到困惑。我们用`h2o.explain`覆盖单个模型的全局解释，用`h2o.explain_row`覆盖局部解释。然后我们使用`h2o.explain`来解决 AutoML 的全局解释，我们用它来证明可解释性在模型选择中的作用。我们使用在第 5 章 、*高级模型构建-第 1 部分*中开发的两个模型来说明这些方法的输出。第一个，`gbm`，是使用默认值和 H2O `check`构建的个人基线模型。选择这些模型仅作为示例，承认原始基线模型是通过多个特征工程和模型优化步骤改进的。

## 沙普利值的简单介绍

Shapley 值已经成为 ML 可解释性的一个重要部分，作为一种将每个特征的贡献归因于整体或个体预测的手段。Shapley 值在数学上是优雅的，非常适合归因的任务。在本节中，我们将介绍 Shapley 值:它们的来源、计算以及如何使用它们进行解释。

2012 年诺贝尔经济学奖得主劳埃德·沙普利(Lloyd Shapley，1923-2016)在 1953 年推导出沙普利值，作为博弈论中一个特定问题的解决方案。假设一群一起工作的玩家得到了一份奖品。奖金应该如何在玩家之间公平分配？

沙普利从定义公平的数学公理开始:**对称**(贡献相同金额的玩家获得相同的支出)**哑元**(没有贡献的玩家什么也得不到)**可加性**(如果游戏可以分成可加性部分，那么就可以分解支出)。Shapley 值是满足这些公理的唯一数学解。简而言之，沙普利价值法根据玩家的边际贡献按比例支付报酬。

接下来，我们演示几个简单场景的 Shapley 值的计算。

### 沙普利计算图解–两个玩家

为了说明 Shapley 值的计算，考虑以下简单的例子。两个音乐家，约翰和保罗，单独表演可以分别赚 4 和 3。约翰和保罗一起玩赚了 10 英镑。他们应该如何划分 10？

为了计算他们的边际贡献，考虑这些玩家可以被排序的方式的数量。对于两个玩家来说，只有两种独特的顺序:约翰在玩，然后保罗加入，或者保罗在玩，然后约翰加入。这在*图 7.2* 中进行了说明:

![Figure 7.2 – Unique player sequences for John and Paul
](img/B16721_07_02.jpg)

图 7.2-约翰和保罗的独特球员序列

作为唯一玩家序列的公式允许我们计算每个玩家的 Shapley 值。我们在*图 7.3* 中说明了约翰的沙普利值的计算:

![Figure 7.3 – Sequence values for John
](img/B16721_07_03.jpg)

图 7.3-约翰的序列值

约翰是第一个出现在序列 1 中的玩家，因此沙普利的贡献只是边际价值 *v(J) = 4* 。在第二个序列中，约翰在保罗之后加入。约翰的边际价值是约翰和保罗的共同价值， *v(JP)* ，减去保罗的边际价值， *v(P)* 。换句话说，*10–3 = 7*。John 的 Shapley 值是每个序列的平均值: *S(J) = 11/2 = 5.5* 。因此，约翰应该得到 10 英镑中的 5.50 英镑。

Paul 的 Shapley 值以类似的方式计算(显然，也可以通过减法计算)。顺序计算如*图 7.4* 所示:

![Figure 7.4 – Sequence values for Paul
](img/B16721_07_04.jpg)

图 7.4-Paul 的序列值

在*图 7.4* 中的第一个序列中，保罗在约翰之后加入，所以序列值为 joint， *v(JP) = 10* ，减去约翰的边际， *v(J) = 4* 。第二个序列正好是保罗的边际值: *v(P)=3* 。Paul 的 Shapley 值为 *S(P) = 9/2 = 4.5* 。

这些计算很简单，对两个玩家都有意义。让我们看看当我们添加第三个玩家时会发生什么。

### 沙普利计算图解——三名玩家

假设第三个音乐家乔治加入了约翰和保罗。乔治自己挣 2 英镑，和约翰一起演出挣 7 英镑，和保罗一起演出挣 9 英镑，三个人一起演出挣 20 英镑。为清晰起见，我们在*图 7.5* 中总结了收益:

![Figure 7.5 – Earnings for John, Paul, and George
](img/B16721_07_05.jpg)

图 7.5-约翰、保罗和乔治的收入

因为有三个玩家，所以有 3 个！=约翰、保罗和乔治可以到达的 6 个独特序列。图 7.6 总结了这种三人游戏场景中 John 的 Shapley 值的计算:

![Figure 7.6 – Arrival sequences and values for calculating the Shapley value for John
](img/B16721_07_06.jpg)

图 7.6-用于计算 John Shapley 值的到达顺序和值

在*图 7.6* 中，序列 1 和 2 很简单:John 是第一个玩家，所以只需要 *v(J)* 值。在序列 3 和序列 5 中，约翰是第二个玩家。序列值的计算方法是，取 John 和第一个玩家的联合值，然后减去该玩家的边际值。序列 4 和序列 6 是相同的:约翰是最后一个玩家。他的边际贡献是取三方互动， *v(JPG)* ，减去保罗和乔治的联合价值， *v(PG)* 计算出来的。沙普利值为 *S(J) = 42/6 = 7* 。

我们可以继续，用同样的方法找到保罗和乔治的沙普利值。

### 计算 N 个玩家的 Shapley 值

正如你所看到的，随着玩家数量的增加，Shapley 值的计算会很快变得难以承受。Shapley 序列的计算依赖于知道主效应的值以及从双向到双向的所有相互作用，如图*图 7.5* 所示。另外还有 *N！*待解序列。随着玩家数量的增加，计算任务急剧增加。

在预测模型的上下文中，每个特征是一个玩家，而预测是共享的奖励。我们可以使用 Shapley 值来描述每个特征对最终预测的影响。有些模型有几十个、几百个甚至更多的特征，在现实世界中计算 Shapley 值是很重要的。幸运的是，现代计算和计算某些模型家族的 Shapley 值的数学捷径的结合使得 Shapley 计算站得住脚。

无论是在我们展示的简单例子中，还是在大型复杂 ML 模型中，Shapley 值的解释都是相同的。

接下来我们将注意力转向单一模型的整体解释。

## 单一型号的整体说明

我们使用 [*第 5 章*](B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082) 、*高级模型构建-第 1 部分*中构建的基线 GBM 模型来说明 e 单个模型的解释。我们将该型号标记为`gbm`，并在*图 5.5* 至*图 5.10* 中记录其性能。

全局解释的基本命令如下:

```
model_object.explain(test)
```

这里，`test`是模型评估中使用的维持测试数据集。其他可选参数包括:

*   `top_n_features`:整数，表示在基于列的方法如`5`中使用多少列。
*   `columns`:在基于列的方法中使用的列名向量，作为`top_n_features`的替代。
*   `include_explanations`或`exclude_explanations`:分别为`include`或`exclude`方法，如`confusion_matrix`、`varimp`、`shap_summary`或`pdp`。

对于单一分类模型，如`gbm`，该命令将按照重要性顺序显示前五个变量的混淆矩阵、变量重要性图、SHAP 汇总图和部分相关性图。

我们使用带有`gbm.explain(test)`命令的`gbm`模型来演示这一点，并依次讨论每个显示。

### 混乱矩阵

第一个输出结果是混淆矩阵，如图*图 7.7* 所示:

![Figure 7.7 – Confusion matrix for the GBM baseline model
](img/B16721_07_07.jpg)

图 7.7-GBM 基线模型的混淆矩阵

`explain`方法的一个很好的特性是为每个显示提供简单的摘要描述:`gbm`显示真阴性(17616)、假阳性(2087)、假阴性(1716)和真阳性(2057)，以及假阳性率(10.59%)和假阴性率(45.48%)。

### 可变重要性图

来自`explain`方法的第二个可视化是变量重要性图，如图*图 7.8* 所示:

![Figure 7.8 – Variable importance plot for the GBM baseline model
](img/B16721_07_08.jpg)

图 7.8-GBM 基线模型的可变重要性图

注意*图 7.8* 中的变量重要性图与我们使用`varimp_plot`命令手动创建的*图 5.10* 中显示的图相同。在这里包含它是使用`explain`方法的好处之一。

### SHAP 摘要情节

`explain`的第三个可视化输出是 SHAP 汇总图。**基于 Shapley 值的 SHAP** ，为提供了黑箱模型的信息视角。GBM 基线模型的 SHAP 汇总图如*图 7.9* 所示:

![Figure 7.9 – SHAP summary plot for the GBM baseline model
](img/B16721_07_09.jpg)

图 7.9–GBM 基线模型的 SHAP 汇总图

让我们更详细地解释一下*图 7.9* 中的 SHAP 汇总情节。在这个信息丰富的情节中，发生了很多事情:

*   在左侧，我们根据 Shapley 值按照要素重要性递减的顺序列出了要素(数据列)。(注意 Shapley 特征重要性等级不一定与图 7.8 中的特征重要性相同。)
*   在右侧，我们有一个归一化特征值范围，从 **0.0** 到**1.0**(H2O 输出的蓝色到红色)。换句话说，对于每个特征，我们按颜色对原始数据值进行编码:低原始值为蓝色，中间值为紫色，高原始值为红色(在此图中显示为不同的灰色阴影)。
*   每个观测值的水平位置由其 SHAP 值决定。SHAP 值衡量每个要素对预测的贡献。较低的 SHAP 值与较低的预测相关联，而较高的值与较高的预测相关联。

有了这个的初步了解，我们可以做出如下观察:

*   右侧为红色值、左侧为蓝色值的要素与响应正相关。由于我们正在对不良贷款的概率进行建模，因此诸如更长期限(`term`)或更高循环利用率(`revol_util`)等特征与贷款违约正相关。(循环信用利用率实质上是客户的信用卡逐月余额。)
*   左侧为红色值、右侧为蓝色值的要素与响应呈负相关。因此，举例来说，较高的年收入(`annual_inc`)与贷款违约呈负相关。

这些来自 SHAP 概要图的模型观察具有直观的意义。你可能会认为信用卡余额较高或年收入较低的人拖欠贷款的可能性较大。

注意，我们可以使用`gbm.shap_summary_plot(test)`命令得到相同的图。

### 部分相关图

`explain`输出的第四个可视化是一组部分依赖图。显示的具体曲线取决于`top_n_features`或`columns`可选参数。默认情况下，前五个功能按照变量重要性递减的顺序显示。*图 7.10* 显示了地址状态的部分依赖图:

![Figure 7.10 – Partial dependence plot for address state
](img/B16721_07_10.jpg)

图 7.10–地址状态的部分相关图

*图 7.11* 显示了循环利用变量的部分相关图:

![Figure 7.11 – Partial dependence plot for revolving utilization
](img/B16721_07_11.jpg)

图 7.11-循环利用率的部分相关图

由`explain`输出的部分相关图包括由平均响应及其可变性(阴影区域包围的线)覆盖的样本大小的表示(从图形底部开始的阴影区域)。在分类变量的情况下，平均响应是一个带条的点，表示可变性，如图*图 7.10* 。在数值变量的情况下，平均响应是一条带有较浅阴影的黑线，表示可变性，如图*图 7.11* 所示。

请注意，我们可以使用以下方法为任何单独的列创建一个部分相关性图:

```
gbm.pd_plot(test, column='revol_util')
```

### 全球个体条件期望(ICE)图

ICE 图将在后面的*单个模型的局部解释*部分介绍。然而，为了完整起见，我们在这里包括了全球版本的 ICE 图。注意该图不是由`explain`输出的。`gbm.ice_plot(test, column='revol_util')`命令返回如图*图 7.12* 所示的全球冰图:

![Figure 7.12 – Global ICE plot for revolving utilization
](img/B16721_07_12.jpg)

图 7.12-循环利用的全球 ICE 图

变量的全局 ICE 图是该变量的部分相关图的扩展。部分相关图显示了平均响应与特定变量的值之间的关系。阴影，如图*图 7.11* 所示，表示部分依赖线的可变性。全球冰图通过使用多条线来代表人口，放大了这一点。(在分类变量的情况下，线用点代替，阴影用条代替。)

如*图 7.12* 所示，全球 ICE 图包括最小值(第 0 个百分位数)、十分位数(第 10 个百分位数到第 90 个百分位数乘以 10 秒)、最大值(第 100 个百分位数)和部分相关性本身的线条。这在视觉上比单独的部分依赖更准确地描绘了总体。平行于部分相关线的百分位数对应于部分相关是其良好代表的人口部分。通常情况下，总体的最小值和最大值的行为可能与部分相关线描述的平均行为有很大不同。在*图 7.12* 中，有三条线与其他线不同:最小值、最大值和第 10 百分位。

接下来我们把注意力转向单一模型的局部解释。

## 单一型号的局部说明

`h2o.explain_row`方法允许数据科学家调查模型的本地解释。虽然全局解释用于理解模型如何表示总体，但是局部解释使我们能够在每行的基础上询问模型。当行代表客户时，这在业务中可能特别重要，正如我们的 Lending Club 分析中的情况。

当使用预测模型做出直接影响客户的决策时(例如，不批准贷款申请或提高客户的保险费率)，全局解释不足以满足业务、法律或监管要求。这就是地方解释的关键所在。

`explain_row`方法为指定的`row_index`值返回这些本地解释。`gbm.explain_row(test, row_index=10)`命令根据不同的重要性为各列提供一个 SHAP 解释图和多个 ICE 图。与部分相关图一样，可以选择性地提供`top_n_features`或`columns`参数。

得到的 SHAP 解释图如图 7.13 所示:

![Figure 7.13 – SHAP explanation for index = 10
](img/B16721_07_13.jpg)

图 7.13–SHAP 对指数= 10 的解释

SHAP 解释显示了每个变量对基于 Shapley 值的整体预测的贡献。对于*图 7.13* 中显示的客户，正的 SHAP 值可以认为是增加了贷款违约的概率，而负的 SHAP 值是降低了违约的概率。对于该客户，78.5%的循环利用率是预测概率的最大正贡献因素。最大的负贡献因素是年收入 9 万，这比其他任何变量都更能降低贷款违约的概率。SHAP 解释可用于提供**原因代码**，以便帮助解释型号。原因代码也可用作直接与客户共享信息的基础，例如，在适用于某些金融和保险相关监管模型的不利行动代码中。

我们接下来访问一些从`explain_row`方法输出的 ICE 图。

### 冰图为当地的解释

ICE 图是部分相关图的单个或每行对应图。正如特性的部分相关图在改变特性值时显示目标变量的平均响应，ICE 图在改变单行特性值时测量目标变量响应。考虑作为`gbm.explain_row`调用结果的*图 7.14* 中显示的地址状态的 ICE 图:

![Figure 7.14 – ICE plot for address state
](img/B16721_07_14.jpg)

图 7.14–地址状态的 ICE 图

图 7.14 中*的垂直黑色虚线代表相关行的实际响应。在这种情况下，状态是 **NJ** (新泽西)，响应大约为 0.10。如果这一行的状态是 **VA** (弗吉尼亚)，那么响应会更低(大约 0.07)。如果这一行的州名改为 **NV** (内华达州)，响应会更高，大约为 0.16。*

考虑下一个*图 7.15* ，贷款期限的 ICE 图:

![Figure 7.15 – ICE plot for the loan term
](img/B16721_07_15.jpg)

图 7.15-贷款期限的 ICE 图

*图 7.13* 中的 SHAP 解释值，期限为 36 个月，是第二大负面因素(它降低贷款违约的概率仅次于年收入，是最大的)。根据*图 7.15* ，60 个月的贷款期限会导致违约概率略高于 0.35，显著高于 36 个月期限的违约概率约为 0.10。虽然 SHAP 解释和 ICE 图测量的是两种不同的东西，但它们的解释可以共同用来理解特定预测的行为。

我们考虑的最后一个 ICE 图是针对循环利用率的，这是一个数字特征，而不是分类特征。该图如图 7.16 所示:

![Figure 7.16 – ICE plot for revolving utilization
](img/B16721_07_16.jpg)

图 7.16-循环利用的 ICE 图

根据图 7.13 中*的 SHAP 解释，循环利用是最重要的积极因素(增加贷款违约的可能性)。*图 7.16* 中的 ICE 图显示了响应与循环利用率值之间的关系。如果`revol_util`是 50%,贷款违约的概率会降低到大约 0.08。在 20%的情况下，违约概率约为 0.05。如果该客户被拒绝贷款，循环利用的高价值将是一个可以辩护的理由。相应 ICE 图的结果可用于通知客户他们可以采取哪些步骤来获得贷款资格。*

## 多个模型的全局解释

在决定将哪个模型提升到生产中时，例如，从 AutoML 运行中，数据科学家可以完全依赖预测模型指标。这可能意味着简单地推广具有最佳 AUC 值的模型。然而，有很多信息可以用来帮助这个决定，预测能力只是多个标准中的一个。

H2O 的全局和局部解释功能提供了额外的信息，这些信息对于结合预测属性评估模型非常有用。我们使用第 5 章 、*高级模型构建-第 1 部分*中的`check` AutoML 对象进行演示。

为多个模型启动全局解释的代码简单如下:

```
check.explain(test)
```

这产生了可变重要性热图、模型相关性热图和多模型部分相关性图。我们将按顺序逐一查看。

### 可变重要性热图

可变重要性热图通过添加颜色作为与变量(作为行)和模型(作为列)一起查看的维度，直观地组合了多个模型的可变重要性图。`check.explain`产生的变量重要性热图如图*图 7.17* 所示:

![Figure 7.17 – Variable importance heatmap for an AutoML object
](img/B16721_07_17.jpg)

图 7.17-AutoML 对象的可变重要性热图

可变重要性值被编码为从低值的蓝色(冷)到高值的红色(热)的连续颜色。最终的图形在视觉上是有意义的。在*图 7.17* 中，垂直条带对应每个型号，水平条带对应单个特征。相似的垂直带表示模型使用其特征的方式之间的高度对应。例如，`XGBoost_1`和`XGBoost_2`型号(最后两列)显示相似的模式。

您还可以看到类似颜色的水平条带，用于变量，如`delinq_2yrs`、`verification_status`，或者在较小程度上用于变量`annual_inc`。这表明所有的候选模型都以同等的重要性对待这些变量。最后一行中的`term`变量是最引人注目的，在不同的模型中是不同的。这些模型对其绝对重要性的看法并不一致。然而，你必须注意不要对此做过多的解读。注意对于`term`，十个模型中的六个*相对*重要性是相同的(除了蓝色方块:`DRF_1`、`GBM_4`、`XGBoost_2`和`XGBoost_1`)。对于这六个模型，`term`是最重要的特征，尽管它的确切值变化很大。

直接创建该显示的代码如下:

```
check.varimp_heatmap()
```

接下来，让我们考虑模型关联热图。

### 模型关联热图

变量重要性热图允许我们根据模型如何查看和使用其组成变量来比较多个模型。模型相关性热图解决了一个不同的问题:*这些不同模型的预测相关性如何？*为了回答这个问题，我们求助于*图 7.18* 中的模型相关性热图:

![Figure 7.18 – Model correlation heatmap for an AutoML object
](img/B16721_07_18.jpg)

图 7.18–AutoML 对象的模型关联热图

沿着图 7.18 的*对角线的最暗的块显示了模型与其自身之间的完美关联。顺序变浅的阴影表示模型之间的相关性降低。您可能如何使用此展示来确定将哪个型号提升到生产中？*

这就是业务或监管约束发挥作用的地方。在我们的例子中，`StackedEnsemble_AllModels`在 AUC 方面具有最好的模型性能。假设我们不被允许将一个集合模型推广到产品中，不管是什么原因。与我们的最佳模型关联度最高的单个模型包括`XGBoost_3`、`GBM_5`和`GLM_1`。然后，这些可以成为提升到生产中的候选者，最终的决定基于附加的标准(可能是测试集上的 AUC 值)。

如果这些附加标准之一是本机可解释性，那么这个 AutoML 对象的`GLM_1`是唯一的选择。请注意，可解释的模型在模型关联热图中用红色字体表示。

我们可以使用以下内容直接创建此显示:

```
check.model_correlation_heatmap(test)
```

让我们在下一小节继续介绍多模型的部分相关图。

### 多模型部分相关图

用于多个模型的`explain`方法的第三个输出是部分相关性图的延伸。对于分类变量，对应于不同模型的图符号和颜色显示在单独的图上。*图 7.19* 是使用`term`变量的一个例子:

![Figure 7.19 – Multiple model partial dependence plot for a loan term
](img/B16721_07_19.jpg)

图 7.19-贷款期限的多模型部分相关图

对于数字变量，多个模型在同一个部分相关图上用不同颜色的线表示。*图 7.20* 是使用`revol_util`变量的一个例子:

![Figure 7.20 – Multiple model partial dependence plot for revolving utilization
](img/B16721_07_20.jpg)

图 7.20-循环利用的多模型部分相关图

在*图 7.19* 和*图 7.20* 中，竞争模型产生了非常相似的结果。情况并不总是这样。例如，*图 7.21* 显示了年收入的多模型部分相关图:

![Figure 7.21 – Multiple model partial dependence plot for annual income
](img/B16721_07_21.jpg)

图 7.21-年收入的多模型部分相关图

尽管图 7.21 中的大多数模型对于低收入人群来说是相似的，但随着收入的增加，它们会出现相当大的差异。这部分是由于年收入分布尾部的样本量非常小。数据科学家也可能基于不现实或不合理的尾部行为决定取消某些模型的资格。例如，根据我们的经验，贷款违约风险随着年收入的增加而增加是没有意义的。在最坏的情况下，我们预计收入和违约之间的关系不会超过某一点。随着收入的增加，我们更有可能期待贷款违约率单调下降。基于这种推理，我们将从考虑中移除顶部两条线(`DRF_1`和`GBM_1`)的模型。

与其他`explain`方法一样，我们可以使用以下命令直接创建这个图:

```
check.pd_multi_plot(test, column='annual_inc')
```

我们接下来访问模型文档。

# 自动化模型文档(H2O AutoDoc)

数据科学团队在企业环境中扮演的重要角色之一是记录投入生产的模型的历史、属性和性能。至少，模型文档应该是数据科学团队最佳实践的一部分。更常见的是，在企业环境中，全面的模型文档或白皮书被强制要求满足内部和外部控制以及法规或遵从性要求。

一般来说，模型文档应该足够全面，以允许重新创建被记录的模型。这需要确定所有数据源，包括培训和测试数据特征、指定硬件系统组件、记录软件版本、建模代码、软件设置和种子、采用的建模假设、考虑的替代模型、性能指标和适当的诊断，以及基于业务或监管条件的任何其他必要内容。这个过程虽然至关重要，但非常耗时，而且可能很乏味。

**H2O AutoDoc** 是一款商业软件产品，可为 H2O-3 和 scikit-learn 中构建的模型自动创建全面的文档。类似的功能已经存在于 H2O.ai 的**无人驾驶 AI** 中，这是一款商业产品，它将自动特征工程与增强的 AutoML 相结合，以构建和部署监督学习模型。AutoDoc 已经成功地用于记录生产中的模型。我们在此简要介绍使用 AutoDoc 自动创建文档:

1.  创建了模型对象后，我们将`Config`和`render_autodoc`模块导入 Python:

    ```
    from h2o_autodoc import Config from h2o_autodoc import render_autodoc
    ```

2.  接下来，我们将指定输出文件的路径:

    ```
    config = Config(output_path = "autodoc_report.docx")
    ```

3.  然后，我们将通过传递配置信息和模型对象来呈现报告:

    ```
    doc_path = render_autodoc(h2o=h2o, config=config,                           model=gbm)
    ```

4.  一旦报告被创建，报告的位置可以用下面的内容来表示:

    ```
    print(doc_path)
    ```

*图 7.22* 显示了 H2O AutoDoc 在 Microsoft Word 中创建的 44 页报告的目录:

![Figure 7.22 – Table of contents for model documentation created by H2O AutoDoc
](img/B16721_07_22.jpg)

图 7.22-H2O AutoDoc 创建的模型文档目录

以一致的方式用最少的人工工作量生成完整的文档的优势是不言而喻的。输出为微软 Word 文档或 markdown 格式，报告可以单独编辑和进一步定制。报告模板也很容易编辑，允许数据科学团队针对不同用途使用不同的报告结构:例如，内部白皮书或监管审查报告。AutoDoc 功能一直是企业版 H2O 软件最受欢迎的功能之一。

# 总结

在本章中，我们回顾了多个模型性能指标，并学习了如何选择一个来评估模型的预测性能。我们通过一些简单的例子介绍了 Shapley 值，以进一步理解它们在预测模型评估中的目的和用途。在 H2O，我们使用`explain`和`explain_row`命令为单一模型创建全局和局部解释。我们学习了如何解释由此产生的诊断和可视化，以获得对模型的信任。对于 AutoML 对象和其他模型列表，我们生成了全局和局部解释，并了解了如何使用它们和模型性能度量来剔除不合适的候选模型。综上所述，我们现在可以评估模型性能、评分速度和解释之间的权衡，以确定将哪个模型投入生产。最后，我们讨论了模型文档的重要性，并展示了 H2O AutoDoc 如何为任何在 H2O 制造的模型自动生成详细的文档。

在下一章中，我们将把我们在 H2O 学到的所有关于建立和评估模型的知识整合在一起，创建一个部署就绪的模型，用于预测 Lending Club 数据中的不良贷款。